[
  {
    "objectID": "talks/smarter_data_people/index.html",
    "href": "talks/smarter_data_people/index.html",
    "title": "Smarter Data People",
    "section": "",
    "text": "Podcast Episode - Smarter Data People - Building Data Capability"
  },
  {
    "objectID": "talks/smarter_data_people/index.html#abstract",
    "href": "talks/smarter_data_people/index.html#abstract",
    "title": "Smarter Data People",
    "section": "",
    "text": "Podcast Episode - Smarter Data People - Building Data Capability"
  },
  {
    "objectID": "talks/smarter_data_people/index.html#links",
    "href": "talks/smarter_data_people/index.html#links",
    "title": "Smarter Data People",
    "section": "Links",
    "text": "Links\nURL"
  },
  {
    "objectID": "talks/WOMBAT_2019/index.html",
    "href": "talks/WOMBAT_2019/index.html",
    "title": "Pearls and pitfalls of time series analysis using Google Analytics data",
    "section": "",
    "text": "Google Analytics is a popular service that tracks and reports website traffic. Using R to query, clean and model Google Analytics data can add significant value, but also significant complexity. From API’s to BigQuery, in this talk I will provide on overview of Google’s unique approach to structuring web data, an overview of the R landscape and share some pearls and pitfalls of using Google Analytics data at scale for data analysis projects."
  },
  {
    "objectID": "talks/WOMBAT_2019/index.html#abstract",
    "href": "talks/WOMBAT_2019/index.html#abstract",
    "title": "Pearls and pitfalls of time series analysis using Google Analytics data",
    "section": "",
    "text": "Google Analytics is a popular service that tracks and reports website traffic. Using R to query, clean and model Google Analytics data can add significant value, but also significant complexity. From API’s to BigQuery, in this talk I will provide on overview of Google’s unique approach to structuring web data, an overview of the R landscape and share some pearls and pitfalls of using Google Analytics data at scale for data analysis projects."
  },
  {
    "objectID": "talks/WOMBAT_2019/index.html#links",
    "href": "talks/WOMBAT_2019/index.html#links",
    "title": "Pearls and pitfalls of time series analysis using Google Analytics data",
    "section": "Links",
    "text": "Links\nSlides"
  },
  {
    "objectID": "talks/keeping-up-with-data/index.html",
    "href": "talks/keeping-up-with-data/index.html",
    "title": "Keeping Up with Data - Podcast S2E1",
    "section": "",
    "text": "This is Keeping Up With Data, the podcast that keeps data enthusiasts up to speed with what is happening in the world of data. We bring in the leading minds from the data industry to talk all things career, news, embarrassing stories, failures and successes."
  },
  {
    "objectID": "talks/keeping-up-with-data/index.html#abstract",
    "href": "talks/keeping-up-with-data/index.html#abstract",
    "title": "Keeping Up with Data - Podcast S2E1",
    "section": "",
    "text": "This is Keeping Up With Data, the podcast that keeps data enthusiasts up to speed with what is happening in the world of data. We bring in the leading minds from the data industry to talk all things career, news, embarrassing stories, failures and successes."
  },
  {
    "objectID": "talks/keeping-up-with-data/index.html#links",
    "href": "talks/keeping-up-with-data/index.html#links",
    "title": "Keeping Up with Data - Podcast S2E1",
    "section": "Links",
    "text": "Links\nURL\nVideo"
  },
  {
    "objectID": "talks/smart-r-academics/index.html",
    "href": "talks/smart-r-academics/index.html",
    "title": "R for Research - Essential Tools for Academics",
    "section": "",
    "text": "R for Research - Essential Tools for Academics"
  },
  {
    "objectID": "talks/smart-r-academics/index.html#abstract",
    "href": "talks/smart-r-academics/index.html#abstract",
    "title": "R for Research - Essential Tools for Academics",
    "section": "",
    "text": "R for Research - Essential Tools for Academics"
  },
  {
    "objectID": "talks/smart-r-academics/index.html#links",
    "href": "talks/smart-r-academics/index.html#links",
    "title": "R for Research - Essential Tools for Academics",
    "section": "Links",
    "text": "Links\nSlides"
  },
  {
    "objectID": "talks/smart-outlier/index.html",
    "href": "talks/smart-outlier/index.html",
    "title": "Unsupervised outlier detection systems for e-commerce",
    "section": "",
    "text": "For large e-commerce businesses, monitoring the rate at which users convert on their websites is essential. Given conversion rate data exhibits certain unique characteristics such as sub-daily time series, multiple seasonality, presence of trend and strictly non-negative values, detecting outliers by business users is non-trivial. We have explored a range of time series outlier detection methods to test and build a system for automatic outlier detection for e-commerce businesses."
  },
  {
    "objectID": "talks/smart-outlier/index.html#abstract",
    "href": "talks/smart-outlier/index.html#abstract",
    "title": "Unsupervised outlier detection systems for e-commerce",
    "section": "",
    "text": "For large e-commerce businesses, monitoring the rate at which users convert on their websites is essential. Given conversion rate data exhibits certain unique characteristics such as sub-daily time series, multiple seasonality, presence of trend and strictly non-negative values, detecting outliers by business users is non-trivial. We have explored a range of time series outlier detection methods to test and build a system for automatic outlier detection for e-commerce businesses."
  },
  {
    "objectID": "talks/smart-outlier/index.html#links",
    "href": "talks/smart-outlier/index.html#links",
    "title": "Unsupervised outlier detection systems for e-commerce",
    "section": "Links",
    "text": "Links\nRecorded Video"
  },
  {
    "objectID": "talks/rstudioconf_2021/index.html",
    "href": "talks/rstudioconf_2021/index.html",
    "title": "How reproducible am I? A retrospective on a year of commercial data science projects in R",
    "section": "",
    "text": "Reproducibility is a critical aspect in science to enable trust & communication. In R, many tools exist to bring in the best practices of reproducibility into the hands of data scientists. However, outside of a research setting, how does reproducibility hold up in commercial data science projects? In this talk I take an honest retrospective of my own commercial R projects in the last year. I look at the various types of analyses completed, and which workflows were selected and why. Through this process we can learn how workflow choices may help in the short term but hinder in the long term. More importantly what can be done strike the balance between progress and perfection when doing data science in the wild?"
  },
  {
    "objectID": "talks/rstudioconf_2021/index.html#abstract",
    "href": "talks/rstudioconf_2021/index.html#abstract",
    "title": "How reproducible am I? A retrospective on a year of commercial data science projects in R",
    "section": "",
    "text": "Reproducibility is a critical aspect in science to enable trust & communication. In R, many tools exist to bring in the best practices of reproducibility into the hands of data scientists. However, outside of a research setting, how does reproducibility hold up in commercial data science projects? In this talk I take an honest retrospective of my own commercial R projects in the last year. I look at the various types of analyses completed, and which workflows were selected and why. Through this process we can learn how workflow choices may help in the short term but hinder in the long term. More importantly what can be done strike the balance between progress and perfection when doing data science in the wild?"
  },
  {
    "objectID": "talks/rstudioconf_2021/index.html#links",
    "href": "talks/rstudioconf_2021/index.html#links",
    "title": "How reproducible am I? A retrospective on a year of commercial data science projects in R",
    "section": "Links",
    "text": "Links\nVideo\nslides"
  },
  {
    "objectID": "talks/siligong2022/index.html",
    "href": "talks/siligong2022/index.html",
    "title": "Building and deploying a Bluey episode recommendation system in R",
    "section": "",
    "text": "Does your child explain Bluey episodes to you but you have no idea what episode they are talking about? This talk will show how to build and deploy a basic data science backed web application in R"
  },
  {
    "objectID": "talks/siligong2022/index.html#abstract",
    "href": "talks/siligong2022/index.html#abstract",
    "title": "Building and deploying a Bluey episode recommendation system in R",
    "section": "",
    "text": "Does your child explain Bluey episodes to you but you have no idea what episode they are talking about? This talk will show how to build and deploy a basic data science backed web application in R"
  },
  {
    "objectID": "talks/siligong2022/index.html#links",
    "href": "talks/siligong2022/index.html#links",
    "title": "Building and deploying a Bluey episode recommendation system in R",
    "section": "Links",
    "text": "Links\nGithub\nSlides"
  },
  {
    "objectID": "talks/SSA_early_career/index.html",
    "href": "talks/SSA_early_career/index.html",
    "title": "Statistics Early Career Talk",
    "section": "",
    "text": "The New South Wales branch of the Statistical Society of Australia warmly invites all undergraduate, postgraduate and early career statisticians and data scientists to attend our annual event for Early Career and Student Statisticians on Wednesday 1st December 2021 at 6pm. The event will take place at The Occidental (43 York St, Sydney NSW 2000), near Wynyard Station."
  },
  {
    "objectID": "talks/SSA_early_career/index.html#abstract",
    "href": "talks/SSA_early_career/index.html#abstract",
    "title": "Statistics Early Career Talk",
    "section": "",
    "text": "The New South Wales branch of the Statistical Society of Australia warmly invites all undergraduate, postgraduate and early career statisticians and data scientists to attend our annual event for Early Career and Student Statisticians on Wednesday 1st December 2021 at 6pm. The event will take place at The Occidental (43 York St, Sydney NSW 2000), near Wynyard Station."
  },
  {
    "objectID": "talks/SSA_early_career/index.html#links",
    "href": "talks/SSA_early_career/index.html#links",
    "title": "Statistics Early Career Talk",
    "section": "Links",
    "text": "Links\nSlides"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nVenue\n\n\n\n\n\n\n01 Dec 2023\n\n\nDeploying your model code into production with Microsoft Azure\n\n\nStatistical Society of Australia - Tutorial\n\n\n\n\n24 Nov 2022\n\n\nBuilding and deploying a Bluey episode recommendation system in R\n\n\nSiligong.Data\n\n\n\n\n09 Feb 2022\n\n\nKeeping Up with Data - Podcast S2E1\n\n\nPodcast Episode\n\n\n\n\n01 Dec 2021\n\n\nStatistics Early Career Talk\n\n\nSSA NSW - Early Career and Student Statisticians Event 2021\n\n\n\n\n21 Jan 2021\n\n\nHow reproducible am I? A retrospective on a year of commercial data science projects in R\n\n\nrstudio::global 2021\n\n\n\n\n23 Mar 2020\n\n\nSmarter Data People\n\n\nPodcast Episode\n\n\n\n\n29 Nov 2019\n\n\nPearls and pitfalls of time series analysis using Google Analytics data\n\n\nMonash University Business Analytics Team- WOMBAT 2019 Workshop\n\n\n\n\n21 May 2019\n\n\nR for Research - Essential Tools for Academics\n\n\nUniversity of Wollongong - SMART Infrastructure Facility\n\n\n\n\n16 Oct 2018\n\n\nUnsupervised outlier detection systems for e-commerce\n\n\nUniversity of Wollongong SMART Infrastructure Facility\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "consulting/data-viz/index.html",
    "href": "consulting/data-viz/index.html",
    "title": "Data Visualisations",
    "section": "",
    "text": "These data visualizations are all scripted and made 100% in R.\nFor the full collection go to the Github Repo"
  },
  {
    "objectID": "consulting/data-viz/index.html#entry-for-pacific-dataviz-competition",
    "href": "consulting/data-viz/index.html#entry-for-pacific-dataviz-competition",
    "title": "Data Visualisations",
    "section": "Entry for Pacific DataViz Competition",
    "text": "Entry for Pacific DataViz Competition"
  },
  {
    "objectID": "consulting/data-viz/index.html#trees-of-san-francisco---tidy-tuesday-project",
    "href": "consulting/data-viz/index.html#trees-of-san-francisco---tidy-tuesday-project",
    "title": "Data Visualisations",
    "section": "Trees of San Francisco - Tidy Tuesday Project",
    "text": "Trees of San Francisco - Tidy Tuesday Project"
  },
  {
    "objectID": "consulting/data-viz/index.html#simpsons-guest-stars---tidy-tuesday-project",
    "href": "consulting/data-viz/index.html#simpsons-guest-stars---tidy-tuesday-project",
    "title": "Data Visualisations",
    "section": "Simpsons Guest Stars - Tidy Tuesday Project",
    "text": "Simpsons Guest Stars - Tidy Tuesday Project"
  },
  {
    "objectID": "consulting/data-viz/index.html#town-map-made-from-open-street-map-osm-data",
    "href": "consulting/data-viz/index.html#town-map-made-from-open-street-map-osm-data",
    "title": "Data Visualisations",
    "section": "Town Map Made From Open Street Map (OSM) Data",
    "text": "Town Map Made From Open Street Map (OSM) Data"
  },
  {
    "objectID": "consulting/data-viz/index.html#australian-meterorite-strikes---tidy-tuesday-project",
    "href": "consulting/data-viz/index.html#australian-meterorite-strikes---tidy-tuesday-project",
    "title": "Data Visualisations",
    "section": "Australian Meterorite Strikes - Tidy Tuesday Project",
    "text": "Australian Meterorite Strikes - Tidy Tuesday Project"
  },
  {
    "objectID": "consulting/fire/index.html",
    "href": "consulting/fire/index.html",
    "title": "An Empirical Modelling and Simulation Framework for Fire Events Initiated by Vegetation and Electricity Network Interactions",
    "section": "",
    "text": "Electrical infrastructure is one of the major causes of bushfire in Australia alongside arson and lightning strikes. The two main causes of electrical-infrastructure-initiated fires are asset failure and powerline vegetation interactions. In this paper, we focus on powerline–vegetation interactions that are caused by vegetation falling onto or blowing onto electrical infrastructure. Currently, there is very limited understanding of both the spatio-temporal variability of these events and their causative factors. Bridging this knowledge gap provides an opportunity for electricity utility companies to optimally allocate vegetation management resources and to understand the risk profile presented by vegetation fall-in initiated fires, thereby improving both operational planning and strategic resource allocation. To bridge this knowledge gap, we developed a statistical rare-event modelling and simulation framework based on Endeavour Energy’s fire start and incident records from the last 10 years. The modelling framework consists of nested, rare-event-corrected, conditional probability models for vegetation events and consequent ignition events that provide an overall model for vegetation-initiated ignitions. Model performance was tested on an out-of-time test set to determine the predictive utility of the models. Predictive performance was reasonable with test set AUC values of 0.79 and 0.66 for the vegetation event and ignition event models, respectively. The modelling indicates that wind speed and vegetation features are strongly associated with vegetation events, and that Forest Fire Danger Index (FFDI) and soil type are strongly associated with ignition events. The framework can be used by energy utilities to optimize resource allocation and prepare future networks for climate change.\nLink: https://www.mdpi.com/2571-6255/6/2/61"
  },
  {
    "objectID": "consulting/bluey/index.html",
    "href": "consulting/bluey/index.html",
    "title": "What was thay Bluey Episode?",
    "section": "",
    "text": "Does your child explain Bluey episodes to you but you have no idea what episode they are talking about and can’t handle flicking through all 131 of them?\nProblem solved.\nThis website lets you type in the vague descriptions of a small child and it will return a mathematically ranked list of closest matching Bluey episodes.\nShiny App\nSlides\nGithub"
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Data Science & Statistical Consulting",
    "section": "",
    "text": "I am open to discussing short or long term consulting and contract opportunities in a range of fields. If you are interesting in hiring me get in touch here."
  },
  {
    "objectID": "consulting/index.html#some-of-my-work",
    "href": "consulting/index.html#some-of-my-work",
    "title": "Data Science & Statistical Consulting",
    "section": "Some of my work",
    "text": "Some of my work"
  },
  {
    "objectID": "contact/index.html",
    "href": "contact/index.html",
    "title": "Contact Me",
    "section": "",
    "text": "Want to get in touch? Drop me an email or book in a chat.\ndean@wavedatalabs.com.au\nBook a meeting"
  },
  {
    "objectID": "contact/index.html#location",
    "href": "contact/index.html#location",
    "title": "Contact Me",
    "section": "Location",
    "text": "Location\n Ground Floor, Enterprise 1, Innovation Campus, Squires Way, North Wollongong NSW 2500 Australia"
  },
  {
    "objectID": "contact/index.html#other-platforms",
    "href": "contact/index.html#other-platforms",
    "title": "Contact Me",
    "section": "Other platforms",
    "text": "Other platforms\n LinkedIn \n Github \n Twitter \n Mastodon"
  },
  {
    "objectID": "posts/2022-09-25-three-stick-statistics/index.html",
    "href": "posts/2022-09-25-three-stick-statistics/index.html",
    "title": "Three Stick Statistics",
    "section": "",
    "text": "When I was a teenager I was very popular among my peers, which is why I spent every second Saturday playing in a retiree’s social golf club. For each event we would travel via air-conditioned coach to a local public golf course and play some sort of themed game. The most exciting event was the three stick event. This game requires players to pick just three golf clubs out of their bag (plus a putter) to play their entire round (you can take up to 14 clubs on a normal round).\nSo which three clubs do you pick?\nThis was easy for me as I could only reliably hit two clubs: the five iron and the seven iron. I usually included a driver so as not be mocked by the septuagenarians at the par-5 tees.\nI also have an embarrassingly crap collection of golf clubs as many have been sent to an early retirement in water features.\nHowever, many friends who now play golf have a bewildering selection of exotic clubs from hybrid driving irons to 60-degree lob-wedges.\nIt seems like in there is always some new technology or method to master in data science broadly. This is interesting, but also exhausting and fills me with existential dread that I am becoming irrelevant.\nIt got me thinking, if you had to play statistics (or data science) for the rest of your career with just three algorithms/models, what would they be?"
  },
  {
    "objectID": "posts/2022-09-25-three-stick-statistics/index.html#my-three-sticks",
    "href": "posts/2022-09-25-three-stick-statistics/index.html#my-three-sticks",
    "title": "Three Stick Statistics",
    "section": "My three sticks:",
    "text": "My three sticks:\n\nRandom Forest: Uncontroversial ensemble method that can quickly get a performance ceiling with minimal fuss. It lacks some interpretability, but I’ll keep it in my bag for when I want to squeeze as much AUC as I can in a prediction setting.\nGeneralised Additive Model (GAM): You could argue this is quite broad, but that’s okay. I’ve had good luck with GAMs as they provide a solid statistical approach that I am comfortable with, along with the flexibility of using smooths for non-linear terms that seems to be a key feature in the problems I have to solve. If I had to only pick one club this would probably be it.\nk-medoids clustering: Clustering methods are a great third club to have in your bag. They solve a different problem to the modelling tasks above and provide a good hedge for those unsupervised tasks that come along fairly regularly. While everyone knows k-means, I enjoy the increased interpretation of k-medoids as the cluster centers are always one of the input data points. It also allows for more generalised distance methods (such as Gower’s distance) which I have found useful for mixed data types.\n\nWe can include lm() as the default putter you get for free.\nWhich three clubs would you pick in your bag?"
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "",
    "text": "This feed contains a list of current incidents from the NSW RFS, and includes location data and Major Fire Update summary information where available. Click through from the feed to the NSW RFS website for full details of the update.\nGeoJSON is a lightweight data standard that has emerged to support the sharing of information with location or geospatial data. It is widely supported by modern applications and mobile devices.\nSee here: https://www.rfs.nsw.gov.au/news-and-media/stay-up-to-date/feeds for attribution and guidelines. Please read these important guidelines before using this data."
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#current-incidents-feed-geojson",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#current-incidents-feed-geojson",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "",
    "text": "This feed contains a list of current incidents from the NSW RFS, and includes location data and Major Fire Update summary information where available. Click through from the feed to the NSW RFS website for full details of the update.\nGeoJSON is a lightweight data standard that has emerged to support the sharing of information with location or geospatial data. It is widely supported by modern applications and mobile devices.\nSee here: https://www.rfs.nsw.gov.au/news-and-media/stay-up-to-date/feeds for attribution and guidelines. Please read these important guidelines before using this data."
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#code",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#code",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "Code",
    "text": "Code\nLoad packages\n\nlibrary(sf)\nlibrary(mapview)\nlibrary(tidyverse)\n\nPull incidents\n\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\"\nfires &lt;- st_read(url)\n\nReading layer `majorIncidents' from data source \n  `http://www.rfs.nsw.gov.au/feeds/majorIncidents.json' using driver `GeoJSON'\nSimple feature collection with 22 features and 7 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 141.9267 ymin: -37.06285 xmax: 153.0005 ymax: -29.11298\nGeodetic CRS:  WGS 84\n\n\nOptional step to get points only\n\n# points only\nfire_pt &lt;- fires %&gt;% \n  st_cast(\"POINT\") \n\nOptional Step to get Polygons only. Note the hack to aply a zero distance buffer.\n\n#' Polygons only\nfire_poly &lt;- fires %&gt;% \n  st_buffer(dist = 0) %&gt;% \n  st_union(by_feature = TRUE)\n\nMapping data interactively\n\nmapview(fire_poly, layer.name = \"RFS Current Incident Polygons\", zcol = \"category\") +\n  mapview(fire_pt, layer.name = \"RFS Current Incident Locations\", zcol = \"category\")\n\n\n\n\nscreenshot of leafet interactive map of fire incidents"
  },
  {
    "objectID": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "href": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "title": "Building your own R Data Science Lab in the browser",
    "section": "",
    "text": "Most R users probably just run RStudio Desktop from Posit on their local computers. This involves manually installing R, RStudio and all other packages.\nHowever it is often the case that users are operating in a restricted computing environment, such as in a corporate or government setting. Alternatively you may wish to create a custom development environment to test or replicate some other specific setup. This is a good case to move away from locally managed software to containerization, such as Docker.\nI have set up a Github repository that sets up a local data science development environment in the browser.\nIt builds a docker container including:\n\nUbuntu 20.04 LTS\nR version 4.2\nRStudio Server 2022.02.3+492\nAll tidyverse packages and devtools\ntex & publishing-related package\n\nThe image builds on the rocker/verse image from Rocker Project.\nSome other enhanced configuration options are included in the Dockerfile, such as preloading you RStudio preferences to get the same look and feel you have locally, the option to install other CRAN packages & mounting local volumes to persist your work locally.\nGo here for Step by step instructions:\nhttps://github.com/deanmarchiori/ds-env-setup"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "",
    "text": "We have a manufacturing process in the day job that is subject to sample auditing.\nThere are \\(N\\) widgets produced and we need to audit \\(n\\) of them. Some sort of rejection threshold is needed on that sample to decide if the whole batch of widgets has met a specified quality level.\nTypically, a binomial distribution would be appropriate for measuring the probability of \\(k\\) successes (in this case defects found) in \\(n\\) independent trials with probability \\(p\\).\n\\[\nPr(X=k) = {n \\choose k} p^k(1-p)^{n-k}\n\\]\nThe word independent is doing a lot of work here as it implies that we are sampling with replacement in order to maintain a fixed probability parameter \\(p\\).\nIn cases where you are taking draws from a population without replacement (such as when you do destructive inspections on a widget) the underlying population changes with each draw and so does the probability \\(p\\).\nIn this case, modelling the process using a hypergeometric distribution may be a more appropriate choice.\n\\[\nPr(X=k) = \\frac{{K \\choose k}{N-K \\choose n-k}}{{N \\choose n}}\n\\]\nIt similarly describes the probability of \\(k\\) successes in \\(n\\) draws without replacement. However, instead of specifying a parameter \\(p\\), we provide the population size \\(N\\), which contains \\(K\\) success states in the population."
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Example",
    "text": "Example\nLet’s say we have 2000 widgets manufactured and we want to sample 50 (ignore why 50, that is a whole separate question). We have an assumed quality level of 10% defective units (which we define as ‘success’ for complicated reasons).\nQ: Based on a sample of 50 widgets how many defective units would be considered unlikely (95% CI) to occur randomly given our assumed quality level, and therefore result in us rejecting the entire batch?\nWe can compare the binomial probability mass function with the hypergeometric and observe they are essentially the same.\n\nlibrary(tidyverse)\n\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\nHowever, if we had a smaller population of say 100 or 70 widgets, how would this compare?\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       hypergeom_100 = dhyper(x, m = 10, n = 90, k = 50),\n       hypergeom_070 = dhyper(x, m = 7, n = 63, k = 50)\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\nWe can see these curves are markedly different. And indeed the 95% confidence intervals obtained are narrower for the hypergeometric case.\n\nqbinom(p = c(0.025, 0.975), size = 50, prob = 0.1)\n\n[1] 1 9\n\nqhyper(p = c(0.025, 0.975), m = 10, n = 90, k = 50)\n\n[1] 2 8\n\n\nWe can see from a random draw of 1 million samples from each PMF that they both have the same expected values, but the variance is smaller in the hypergeometric case.\n\nX &lt;- rbinom(n = 1e6, size = 50, prob = 0.1)\nY &lt;- rhyper(nn = 1e6, m = 10, n = 90, k = 50)\n\nmean(X)\n\n[1] 5.003297\n\nvar(X)\n\n[1] 4.503315\n\nmean(Y)\n\n[1] 5.000162\n\nvar(Y)\n\n[1] 2.27195"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Does it matter which one you use?",
    "text": "Does it matter which one you use?\nAs a consequence of removing samples in each draw we influence the probability of a subsequent success. If our \\(N\\) and \\(K\\) is very large relative to our sample \\(n\\) this wont make much of an impact, but it can be impactful for smaller populations, or relatively larger samples.\nFrom our example above, failing to use a hypergeometric distribution to model this process for smaller populations will result in wider, more conservative acceptance regions which can increase consumer risk in a manufacturing process.\nTypical guidance on when to use each distribution is given in manufacturing standards such as AS 1199.1-2003: Sampling Procedures for Inspection by Attributes and typically involves how you structure your sampling scheme."
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "title": "Running Shiny in a Docker container",
    "section": "",
    "text": "Basic minimal example for running shiny in Docker. It is assumed you have Docker installed."
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "title": "Running Shiny in a Docker container",
    "section": "Dockerfile",
    "text": "Dockerfile\nThis should be adapted as required.\n# Using rocker/rver::version, update version as appropriate\nFROM rocker/r-ver:3.5.0\n\n# install dependencies\nRUN apt-get update && apt-get install -y \\\n    sudo \\\n    gdebi-core \\\n    pandoc \\\n    pandoc-citeproc \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\  \n    libxml2-dev \\  \n    libssl-dev \\  \n    wget\n\n\n# Download and install shiny server\nRUN wget --no-verbose https://download3.rstudio.org/ubuntu-14.04/x86_64/VERSION -O \"version.txt\" && \\\n    VERSION=$(cat version.txt)  && \\\n    wget --no-verbose \"https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-$VERSION-amd64.deb\" -O ss-latest.deb && \\\n    gdebi -n ss-latest.deb && \\\n    rm -f version.txt ss-latest.deb && \\\n    . /etc/environment && \\\n    R -e \"install.packages(c('shiny', 'rmarkdown'), repos='$MRAN')\" && \\\n    cp -R /usr/local/lib/R/site-library/shiny/examples/* /srv/shiny-server/\n\n# Copy configuration files into the Docker image\nCOPY shiny-server.conf  /etc/shiny-server/shiny-server.conf\nCOPY shiny-server.sh /usr/bin/shiny-server.sh\n\n# Copy shiny app to Docker image\nCOPY /myapp /srv/shiny-server/myapp\n\n# Expose desired port\nEXPOSE 80\n\nCMD [\"/usr/bin/shiny-server.sh\"]"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "title": "Running Shiny in a Docker container",
    "section": "List Images",
    "text": "List Images\ndocker images"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "title": "Running Shiny in a Docker container",
    "section": "List All Containers",
    "text": "List All Containers\ndocker ps -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "title": "Running Shiny in a Docker container",
    "section": "Remove Containers",
    "text": "Remove Containers\nFor individual containers add the container ID\n$ docker rm\nTo remove all exited containers :\n$ docker rm $(docker ps -a -q -f status=exited)"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "title": "Running Shiny in a Docker container",
    "section": "System Prune",
    "text": "System Prune\nRemove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.\ndocker system prune -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Save as tar-archive",
    "text": "Save as tar-archive\ndocker save -o ~/myapp.tar myapp"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Load and Run Archive",
    "text": "Load and Run Archive\ndocker load -i myapp.tar\ndocker run myapp"
  },
  {
    "objectID": "posts/2022-02-03-what-was-that-bluey-episode/index.html",
    "href": "posts/2022-02-03-what-was-that-bluey-episode/index.html",
    "title": "What was that Bluey Episode?",
    "section": "",
    "text": "Does your child explain Bluey episodes to you but you have no idea what episode they are talking about and can’t handle flicking through all 131 of them?\nProblem solved.\nThis website lets you type in the vague descriptions of a small child and it will return a mathematically ranked list of closest matching Bluey episodes.\nhttps://deanmarchiori.shinyapps.io/blueysearch/"
  },
  {
    "objectID": "posts/2022-02-03-what-was-that-bluey-episode/index.html#development-notes",
    "href": "posts/2022-02-03-what-was-that-bluey-episode/index.html#development-notes",
    "title": "What was that Bluey Episode?",
    "section": "Development notes",
    "text": "Development notes\nThe website is a Shiny app (deployed to shinyapps.io), which contains all 130 episode titles, descriptions and thumbnails.\nAll of the episode descriptions were tokenized into ‘terms’ using the {tidytext} R package and formed into a Document-Term-Matrix. Rather than use the typical term counts, a binary indicator was used if a term appears in an episode. This was preferred as the user’s text input is unlikely to really mimic the detail of an episode description, which threw the similarity measure out a bit.\nOnce a user inputs text, the Shiny app dynamically forms a new term vector and compares it to the all-episode’s matrix using cosine distance. The episodes are then ranked based on smallest cosine distance and displayed to the users.\nFor the source code visit: https://github.com/deanmarchiori/bluey-search"
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html",
    "title": "Optimal Stopping Problems",
    "section": "",
    "text": "The August 2022 edition of Significance Magazine posts a challenge:\nThe questions are:\nThis is a well known problem in Optimal Stopping theory. When you look at the first few knights you have no information about how they rank relative to all 100. But by the time you have assessed most of the 100 knights you have information but you have no agency. The knights have all moved on and you either miss out or have to settle for whoever is left.\nThis problem comes up every time I look for a parking spot on a busy day. Do I take the first spot I find a long way from the beach? I could try to get a closer spot but may not find one and could miss out entirely. Most likely the earlier spots are now taken.\nWhen is the optimal time to stop looking and take the leap?\nThe strategy here is:\nWe assume we look at least one knight before stopping our search and assume that if we fail to find a superior knight after our looking period - we miss out! We obviously wont settle for less. (I guess I could force this to accept the last one in line - not a big deal)."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#simulation-in-r",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#simulation-in-r",
    "title": "Optimal Stopping Problems",
    "section": "Simulation in R",
    "text": "Simulation in R\nWe can simulate this problem by sampling 100 random knights and assigning them a random ranking of 1-100 (assume 100 is best).\n\nset.seed(111)\nsample(100)\n\n  [1]  78  84  83  47  25  59  69  35  72  26  49  45  74   8 100  96  24  48\n [19]  95   7  21  15   1   9  63  40  85  93  71  52  28  38  88  61  92  30\n [37]   5  53  37   6  36  41  70  42  18  27  29  23  32  89  86  57  16  90\n [55]  39   4  68  55  99  98  79  43  54  97  65  50  94  44  10  91  56  80\n [73]  19  73  33  11  12  67  81  62  76  60  77  34   2  20  13  51  14  64\n [91]  75  87  66  17  22   3  31  46  82  58\n\n\nNext we can look at the cumulative maximum ranking. This will track the score of the best knight seen so far. We also run, say, 10,000 simulations of this problem to analyse.\n\nlibrary(tidyverse)\n\nset.seed(111)\nn &lt;- 100\nsims &lt;- 1e4\n\nknight_sims &lt;- expand_grid(sim = 1:sims) |&gt; \n  mutate(rankings = map(sim, ~sample(1:n))) |&gt; \n  unnest(rankings) |&gt; \n  group_by(sim) |&gt; \n  mutate(stop_after_days = 1:n,\n         max_rank = cummax(rankings))\n\nknight_sims\n\n# A tibble: 1,000,000 × 4\n# Groups:   sim [10,000]\n     sim rankings stop_after_days max_rank\n   &lt;int&gt;    &lt;int&gt;           &lt;int&gt;    &lt;int&gt;\n 1     1       78               1       78\n 2     1       84               2       84\n 3     1       83               3       84\n 4     1       47               4       84\n 5     1       25               5       84\n 6     1       59               6       84\n 7     1       69               7       84\n 8     1       35               8       84\n 9     1       72               9       84\n10     1       26              10       84\n# … with 999,990 more rows\n\n\nWe can write a function that can map over each day and determine what rank knight we would end up with if we stopped searching after x days and committed to the next best knight.\n\noptimal_stop &lt;- function(samp, i) {\n  if (i == 0) {\n    samp[1]\n  }\n  \n  if (i &gt; length(samp) | i &lt; 0) {\n    stop(\"Invalid choice\")\n  }\n  \n  if (any(samp &gt; cummax(samp)[i]) == FALSE) {\n    # if you want to default to the last option\n    #samp[length(samp)]\n    \n    # if you would rather miss out if no superior choice\n    NA\n  } else{\n    idx &lt;- which.max(samp &gt; cummax(samp)[i])\n    samp[idx]\n  }\n}\n\nWe want to know how often we end up with the optimal (top ranked) knight depending on which day we stop looking."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#results",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#results",
    "title": "Optimal Stopping Problems",
    "section": "Results",
    "text": "Results\n\nresults &lt;- knight_sims |&gt;  \n  mutate(result = map_int(stop_after_days, ~optimal_stop(rankings, .x))) |&gt; \n  mutate(is_optimal = result == max(rankings, na.rm = TRUE)) |&gt; \n  group_by(stop_after_days) |&gt; \n  summarise(optimal_stops = sum(is_optimal, na.rm = TRUE),\n            n = n(),\n            optimal_stop_pct = optimal_stops / n,\n            var = sqrt(optimal_stop_pct * (1 - optimal_stop_pct) / n))\n\nresults\n\n# A tibble: 100 × 5\n   stop_after_days optimal_stops     n optimal_stop_pct     var\n             &lt;int&gt;         &lt;int&gt; &lt;int&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1               1           533 10000           0.0533 0.00225\n 2               2           846 10000           0.0846 0.00278\n 3               3          1116 10000           0.112  0.00315\n 4               4          1352 10000           0.135  0.00342\n 5               5          1572 10000           0.157  0.00364\n 6               6          1771 10000           0.177  0.00382\n 7               7          1941 10000           0.194  0.00396\n 8               8          2098 10000           0.210  0.00407\n 9               9          2225 10000           0.222  0.00416\n10              10          2360 10000           0.236  0.00425\n# … with 90 more rows\n\n\nWe can plot the success proportion of bagging the most optimal knight for each stopping day.\n\nggplot(results, aes(stop_after_days, optimal_stop_pct)) +\n  geom_line() +\n  geom_ribbon(aes(stop_after_days, ymax = optimal_stop_pct + 1.96 * var, \n                  ymin = optimal_stop_pct - 1.96 * var), \n              alpha = 0.22) +\n  theme_bw() +\n  geom_vline(xintercept = 37, lty = 2, col = 'red') +\n  geom_hline(yintercept = 0.368, lty = 2, col = 'blue') +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"What is the probability of picking the best knight?\",\n       subtitle = \"Based on stopping searching at x days and choosing the next best knight\",\n       x = \"Look for this many days\",\n       y = \"Probability of optimal choice\")\n\n\n\n\nHere is the number of knights we should observe without picking any\n\nwhich.max(results$optimal_stop_pct)\n\n[1] 37\n\n\nAnd here is the probability of choosing the best knight with this strategy\n\nmax(results$optimal_stop_pct)\n\n[1] 0.3755\n\n\nSo we should let the first 37 knights pass, just like Merlin said. If we do this, we will give ourselves roughly 37% probability of picking the most optimal knight, which is the best we can do."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#mathematical-derivation",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#mathematical-derivation",
    "title": "Optimal Stopping Problems",
    "section": "Mathematical Derivation",
    "text": "Mathematical Derivation\nSo why 37 days and 37% probability?\nIf we assume we reject the first \\(r-1\\) knights and then choose the next best knight we get to probability\n\\[\nP(r) = \\sum_{j = r}^{n}P(\\text{jth knight is best and you select it})\n\\]\n\\[\nP(r) = \\sum_{j = r}^{n}(\\frac{1}{n})(\\frac{r-1}{j-1}) = (\\frac{r-1}{n})\\sum_{j=r}^{n}\\frac{1}{j-1}\n\\]\nWe want to know which \\(r\\) maximises the probability for large \\(n\\). As \\(n\\) approaches infinity we can let \\(x\\) be the limit of \\(r/n\\) and use \\(t\\) for \\(j/n\\) and \\(dt\\) for \\(1/n\\) we get the integral\n\\[\nP(x) = x \\int_{x}^{1}(\\frac{1}{t})dt = -x\\log(x)\n\\]\nIf we take the derivative with respect to x and set the equation to zero we can find the value of \\(x\\) which maximises this probability:\n\\[\n\\text{optimal x} = \\frac{1}{e} = 0.3678...\n\\]\nand gives the optimal stopping number (for any large n) of \\(\\frac{n}{e} \\approx 37\\)\n\n1/exp(1)\n\n[1] 0.3678794\n\n\n\n100/exp(1)\n\n[1] 36.78794"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "",
    "text": "A recent project with repeated measures data involved fitting a random intercept term, and eventually making predictions for new groups not in the training sample. Importantly there was a need for individual predictions rather than population mean level predictions. Now, you obviously cannot include the random effect for a level that is not in your data, so the idea was to make a population level prediction with an adequate prediction interval that reflected the variation from both the fixed and random effects. This is complicated.\nIn the help page for lme4::predict.merMod() is the following note:\nThere are some useful resources out there but it took a while to track down, so this post may serve as a good reference in the future.\nLet’s go through an example using the famous sleepstudy data showing the average reaction time per day (in milliseconds) for subjects in a sleep deprivation study.\nlibrary(lme4)\nlibrary(tidyverse)\n\ndata(\"sleepstudy\")"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Model",
    "text": "Linear Model\nWe would like to model the relationship between Reaction and Days\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  theme_bw()\n\n\n\n\nFitting a basic linear model:\n\nfit_lm &lt;- lm(Reaction ~ Days, data = sleepstudy)\n\nsummary(fit_lm)\n\n\nCall:\nlm(formula = Reaction ~ Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,    Adjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n\n\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw()\n\n\n\n\nBut this ignores the fact these data are not independent. We have multiple observation per subject. Some look like a good fit, others not.\n\nggplot(sleepstudy, aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Mixed Effects Model",
    "text": "Linear Mixed Effects Model\nLet’s add a random intercept term for Subject. For simplicity we will leave out any other random effects.\n\nfit &lt;- lme4::lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)\n\nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nNew fitted lines can be drawn, showing the adjusted intercept for each subject (original regression line kept for reference).\n\nsleepstudy |&gt; \n  mutate(pred = predict(fit, re.form = NULL)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], col = \"grey\") +\n  geom_line(aes(Days, pred), show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Bootstrapped Prediction Intervals (observed data)",
    "text": "Bootstrapped Prediction Intervals (observed data)\nLet’s try and generate prediction intervals using lme4::bootMer() as suggested.\nFirst on the in-sample data.\n\n# predict function for bootstrapping\npredfn &lt;- function(.) {\n  predict(., newdata=new, re.form=NULL)\n}\n\n# summarise output of bootstrapping\nsumBoot &lt;- function(merBoot) {\n  return(\n    data.frame(fit = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.5, na.rm=TRUE))),\n               lwr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.025, na.rm=TRUE))),\n               upr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.975, na.rm=TRUE)))\n    )\n  )\n}\n\n# 'new' data\nnew &lt;- sleepstudy\n\nNotes:\n\nIn the predict() function we specify re.form=NULL which identifies which random effects to condition on. Here NULL includes all random effects. Obviously here you can compute individual predictions assuming you feed it with the correct grouping level in your data.\nIn the lme4::bootMer() function we set use.u=TRUE. This conditions on the random effects and only provides uncertainly estimates for the i.i.d. errors resulting from the fixed effects of the model.\n\n\nIf use.u is TRUE and type==“parametric”, only the i.i.d. errors are resampled, with the values of u staying fixed at their estimated values.\n\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=TRUE, type=\"parametric\")\n\n\nnew |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject, fill = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Dealing with unobserved data",
    "text": "Dealing with unobserved data\nHowever, this gets complicated if we want to make predictions for new subjects.\nWe can no longer condition on the random effects, as the new subject level will not have a fitted random intercept value. Instead we need to effectively make a population level prediction (i.e. set the random effect to zero.). This makes sense as we don’t know what the random effect ought to be for a given, unobserved subject.\nBut we don’t want the prediction interval to just cover the uncertainty in the population level estimate. If we are interested in individual predictions, how can we incorporate the uncertainly of the random effects in the prediction intervals?\nLets generate a new, unobserved subject.\n\nnew_subject &lt;- tibble(\n  Days = 0:9,\n  Subject = factor(\"999\")\n  )\n\nWe provide a new predict function that doesn’t condition on the random effects by using re.form = ~0. This lets us input and obtain predictions for new subjects.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\n\nnew_subject |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nHowever using predict just results in a completely deterministic prediction as shown above.\nAn alternative approach is to use lme4::simulate() which will simulate responses for subjects non-deterministically using the fitted model object.\nBelow we can see a comparison on both approaches.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\nsfun &lt;- function(.) {\n    simulate(., newdata=new_subject, re.form=NULL, allow.new.levels=TRUE)[[1]]\n}\n\n\nnew_subject |&gt; \n  bind_cols(simulated = sfun(fit)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  pivot_longer(cols = c(3, 4), names_to = \"type\", values_to = \"val\") |&gt; \n  ggplot(aes(Days, val, col = type)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nWe can use this simulate() function in our bootstrapping to resample responses from the fitted model (rather than resampling deterministic population predictions).\nThis time we set use.u=FALSE to provide uncertainly estimates from both the model errors and the random effects.\n\nIf use.u is FALSE and type is “parametric”, each simulation generates new values of both the “spherical” random effects uu and the i.i.d. errors , using rnorm() with parameters corresponding to the fitted model x.\n\n\nboot &lt;- lme4::bootMer(fit, sfun, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\nSo while we don’t have a conditional mode of the random effect (because its a new subject) we can derive a bootstrapped estimate of the prediction interval by resampling the random effects and model errors on simulated data values."
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Aside",
    "text": "Aside\nFor comparison, here is what the same prediction interval would look like if we just used an unconditional population prediction. While the overall gist is the same, despite also resampling both the random effects and the i.i.d. errors, the interval is narrower as it is resampling just the deterministic population predictions of the model.\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "References",
    "text": "References\nMost of the material and code is taken from a variety of sources below. In particular the lme4 github issue. Also, the merTools package has a nice vignette comparing these methods with their own solution.\nhttps://tmalsburg.github.io/predict-vs-simulate.html https://github.com/lme4/lme4/issues/388 https://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-confidence-or-prediction-intervals-on-predictions"
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "",
    "text": "In a previous post I linked to a project that makes it easy to deploy and extend an existing Rocker Project Docker image to quickly set up a fully featured RStudio Server environment locally on your machine.\nHere I’ll cover some options to deploy this environment to the cloud so you can access it anywhere."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 1: Deploy to a Virtual Machine",
    "text": "Option 1: Deploy to a Virtual Machine\nA common pattern is to create a Virtual Machine (VM) with a cloud service provider (such as AWS, Azure, GCP) and run your code there. I’ll cover an example using Microsoft Azure.\n\nDeploy a VM with an Ubuntu operating system. Go ahead and choose the compute power you need.\n\n\n\nConfigure a custom network rule to allow traffic on port 8787 for RStudio\n\n\n3. Log into your new VM terminal using SSH\n\n\nInstall Docker Engine by following these steps\nClone and Deploy the docker container from Step 2 in my guide."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 2: Deploy using Azure App Service",
    "text": "Option 2: Deploy using Azure App Service\nThe above is fine, but arguably if you are setting up a VM from scratch for development purposes I’m not sure what benefit there is from using a docker container. You may as well just directly install what you want and consider the VM a ‘container’.\nHowever, if you plan to make this available to other users in your organisation, or to adapt this guide for Shiny App development you may be interested in other features such as TLS/SSL security, scale up, advanced networking, continuous integration, continuous deployment, staging/production deployment slots etc. This represents a shift from development sandpit to ‘web app’. For this case, Azure App Service may be a lower hassle option. This is Microsoft’s enterprise grade, web app deployment managed service.\nIn the Virtual Machine model you are setting up compute infrastructure, deploying and running containers directly - then fiddling with the infrastructure layer for everything else. In App Service you deploy your custom docker container (here containing RStudio Server) to Azure Container Registry (kind of like DockerHub). Azure App Services then builds and serves your app from there - without you having to stand up and manage an Infra layer directly.\n\n\nCreate Azure Container Registry (ACR) (or some other Docker repository) using this help guide\nRun and test your container locally\nDeploy your local container to ACR using this help guide\nCreate a new web app in Azure App Services using this help guide\nConfiguration:\n\n\nI didn’t have to fiddle with ports, presumably it reads the exposed ports in the docker file and does this magically.\n\nFor custom environment variables like the RStudio Server password, I had to manually add this in the config section.\n\n\nand it worked just fine:"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html",
    "title": "Man vs Machine Learning",
    "section": "",
    "text": "Over the past few years I have been doing more and more work in Microsoft Azure and the Azure Machine Learning Studio. One feature of the Azure ML studio is an automated machine learning feature. This is essentially a no-code UI that ‘empowers professional and nonprofessional data scientists to build machine learning models rapidly’ (emphasis mine).\nWhile many (including me) have leveled a fair amount of criticism towards such solutions, I thought it would be worth seeing what the fuss was about.\nCould I go head-to-head on the same predictive modelling challenge and compete with the might of Microsoft’s AutoML solution? Even worse, would I enjoy it? Even more worse, could I win??"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "title": "Man vs Machine Learning",
    "section": "Method 1: Azure AutoML",
    "text": "Method 1: Azure AutoML\nThe process to set up a new AutoML job was very easy and assumes you are working under somewhat sanitized conditions (which I was in this case).\n\nOnce you kick it off, it chugged away for an hour and 33 minutes. To my horror, I realized it takes the ‘kitchen sink’ approach and fits a suite of 41 (!) different machine learning models at the training data. Hyperparameter tuning is done by constructing a validation set using K-Fold cross validation.\nVideo\nThe best performing model is then selected and then predictions are run on the test set. It’s a little concerning that Test set evaluation is only in ‘Preview’ mode. It was also very confusing to dig out the results on the test set. Most of the metrics prominently displayed are overly confident in-sample accuracy results.\nThe winning model in my case was a ‘Voting Ensemble’ of three models\n\nMaxAbsScaler, ExtremeRandomTrees\nStandardScalerWrapper, XGBoostRegressor\n\nStandardScalerWrapper, LightGBM\n\nOverall the process was very easy and user friendly. It look a long time to train, but I didn’t have to think about anything - at all (which is usually time consuming) so overall it was a quick solution. I trained the model on a Standard_DS11_v2 (2 cores, 14 GB RAM, 28 GB disk) compute instance which costs $0.2 per hour. So it cost money, but not much.\nPerformance evaluation to follow below…"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "title": "Man vs Machine Learning",
    "section": "Method 2: Manual Time Series Model in R",
    "text": "Method 2: Manual Time Series Model in R\nThe process for doing this myself involved much more thought and brain-effort. Here are some notes.\nThe data set is quite complicated as its sub-daily and has (probably) three seasonal periods (daily, weekly, yearly). There was also maybe some trend and outliers to deal with. The data set also contained covariates such as Temperature and Holiday indicators.\nDue to the seasonal complexity many traditional statistical methods were not appropriate like straight ARIMA (autoregressive integrated moving average) and ETS (exponential smoothing). While STL (Seasonal and Trend decomposition using Loess) can handle multiple seasonal periods I wanted a method to handle the covariates (like Temperature and Holidays). My next step was to think of Time Series Linear Regression models. However, accounting for yearly seasonality with 30min data meant fitting 17,520 (2 * 24 * 365) parameters just for this seasonal period. Which seemed excessive.\nFor longer, multiple-seasonal periods, using Fourier terms can be a good idea. Here a smaller number of terms in a fourier series can be estimated to approximate a more complex function. This type of Dynamic Harmonic Regression2 can also handle exogenous covariates and we can even fit the model with ARIMA errors to account for the short term dynamics of time series data.\nIn fact, this very approach was outlined in the excellent Forecasting: Principles and Practice3 using this very same example data set. I decided to borrow (steal) the ideas of creating a piece-wise linear trend for temperature. I also went a bit crazy with encoding specific holiday dummy variables and some other tweaks.\nOverall I found this method slow to fit, and not overly performant. I decided next to try fitting a Prophet4 model. Prophet is an open-source automated algorithm for time series forecasting developed by Facebook. It uses a Bayesian framework to fit complex, non-linear, piece-wise regression models. For complex time series data, it provides a decent, fast framework including exogenous variables, holiday and seasonal effects. I didn’t do any principled hyperparameter tuning, but I did fiddle around with the model a bit."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nSo who won?\nThe AutoML platform did :( , but only just. Below is the comparison of RMSE and MAPE. The AutoML is red, my predictions are in blue. I stuffed up over Christmas a bit, which admittedly is a tricky hold-out month for testing.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n213\n\n\nAzure AutoML\nMAPE\n3.56\n\n\nMe\nRMSE\n274\n\n\nMe\nMAPE\n4.96"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#discussion",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#discussion",
    "title": "Man vs Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nSo overall it was pretty close, but in terms of pure predictive performance, the AutoML platform did pip me at the post. Admittedly, the solution I arrived at was probably more of an ML solution than a ‘classical’ time series method given it is still an automated algorithm. If I had more time and patience I probably could have pursued a more complex regression model. In fact in Forecasting: Principles and Practice, the authors also cite the performance of a straight Dynamic Harmonic Regression is limited, however they go on to propose other innovative approaches56, including splitting the problem into separate models for each 30min period and using regression splines to better capture exogenous effects. So it can be done, but not without a huge amount of effort."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#automl-solution",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#automl-solution",
    "title": "Man vs Machine Learning",
    "section": "AutoML solution",
    "text": "AutoML solution\nThe AutoML platform again used a Voting Ensemble, churned out in 43 minutes, but this time using:\n\nProphetModel (it must have copied me from last round ;))\n\nExponential Smoothing"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#my-solution",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#my-solution",
    "title": "Man vs Machine Learning",
    "section": "My solution",
    "text": "My solution\nGiven the multiplicative process here, I modeled the log transformed data. (I did try a more generalized Box-Cox transformation, but got better performance with a straight natural log transform). I tried an ARIMA model, using model selection via the Hyndman-Khandakar algorithm8, which resulted in a ARIMA(2,0,1)(1,1,2)[12] w/ drift&gt;."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results-1",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results-1",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nYay! I won this round. Quite easily.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n2.43\n\n\nAzure AutoML\nMAPE\n9.22\n\n\nMe\nRMSE\n1.63\n\n\nMe\nMAPE\n7.23"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#references",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#references",
    "title": "Man vs Machine Learning",
    "section": "References",
    "text": "References\nO’Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‘tsibble’. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#other",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#other",
    "title": "Man vs Machine Learning",
    "section": "Other",
    "text": "Other\nThanks to the Tidyverts team https://tidyverts.org/. The new an improved time series stack in R makes all this so easy.\nNote: None of this was super-rigorous, and I certainly tilted the board in my favour here and there. It was just fun and a chance to play around with a tool that I have previously avoided for no real reason."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#footnotes",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#footnotes",
    "title": "Man vs Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Australian Energy Market Operator; tsibbledata R package↩︎\nYoung, P. C., Pedregal, D. J., & Tych, W. (1999). Dynamic harmonic regression. Journal of Forecasting, 18, 369–394. https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-131X(199911)18:6%3C369::AID-FOR748%3E3.0.CO;2-K↩︎\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05.↩︎\nTaylor SJ, Letham B. 2017. Forecasting at scale. PeerJ Preprints 5:e3190v2 https://doi.org/10.7287/peerj.preprints.3190v2↩︎\nFan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-parametric additive model. IEEE Transactions on Power Systems, 27(1), 134–141. https://ieeexplore.ieee.org/document/5985500↩︎\nHyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak electricity demand. IEEE Transactions on Power Systems, 25(2), 1142–1153. https://ieeexplore.ieee.org/document/5345698↩︎\nSource: Medicare Australia; tsibbledata R package↩︎\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(1), 1–22. https://doi.org/10.18637/jss.v027.i03↩︎"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Is the share market just random noise?\n\n\nModelling market prices using a random walk model\n\n\n\n\nR\n\n\nMarkets\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nGeolocating Sydney’s weirdest property\n\n\nUsing Open Street Map and R to geolocate an image\n\n\n\n\nR\n\n\nOSM\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nMan vs Machine Learning\n\n\nI went head-to-head with Microsoft’s AutoML platform in a predictive modelling challenge.\n\n\n\n\nR\n\n\nAzure\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nDeploy Your Own R Data Science Lab in the Cloud\n\n\n\n\n\n\n\nR\n\n\nazure\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nBuilding your own R Data Science Lab in the browser\n\n\n\n\n\n\n\nR\n\n\ndocker\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nBeware of Boundaries in Binominal Proportion Confidence Intervals\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nWhen should you be using the Hypergeometric distribution in practice?\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nPrediction Intervals for Linear Mixed Effects Models\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nOptimal Stopping Problems\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nThree Stick Statistics\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nWhat was that Bluey Episode?\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nRunning Shiny in a Docker container\n\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n  \n\n\n\n\nMapping NSW Fire Incidents in R\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2021\n\n\nDean Marchiori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "href": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "title": "Beware of Boundaries in Binominal Proportion Confidence Intervals",
    "section": "",
    "text": "Binomial proportion confidence intervals are often employed when attempting to perform tests for significance, or sample size calculations around sample measurements resulting from a Bernoulli process.\nThe typical choice when calculating binomial proportion confidence intervals is the asymptotic, or normally approximated ‘Wald’ interval where success probability is measured by:\n\\[\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nIn many settings, such as marketing analytics or manufacturing processes the sample proportion is close to 0 or 1. Evaluating asymptotic confidence intervals near these boundary conditions will lead to underestimation of the error, and in some cases producing an interval outside \\([0, 1]\\).\nFortunately other methods exist, such as Wilson’s score interval, exact methods and Bayesian approaches. The recommendation here is to examine the probability coverage and explore alternative methods for sample size and CI calculation, especially when the parameter is near the boundary conditions, or in cases of very small n. \n\nlibrary(binom)\nlibrary(tidyverse)\n\nn &lt;- 50\np &lt;- c(0.01, 0.5, 0.99)\n\n\nx &lt;- purrr::map_df(p, .f =  ~binom.confint(x = n * .x, n = n, methods = 'all'))\n\n\nggplot(x, aes(colour = factor(x))) +\n  geom_point(aes(mean, method), show.legend = F) +\n  geom_errorbarh(aes(xmin = lower, xmax = upper, y = method), show.legend = F) +\n  geom_vline(xintercept =  c(0, 1), lty = 2, col = \"grey\") +\n  facet_wrap(~(x*2/100)) +\n  theme_bw() +\n  labs(title = \"A variety of binomial confidence interval methods for p = 0.01, 0.5 & 0.99\",\n       subtitle = \"Note unusual behaviour near 0.01 and 0.99\")\n\n\n\n\n\ncov &lt;- purrr::map_df(p, ~binom.coverage(.x, n, conf.level = 0.95, method = \"all\"))\n\n\nggplot(cov, aes(colour = factor(p))) +\n  geom_point(aes(coverage, method), show.legend = F) +\n  geom_vline(xintercept =  0.95, lty = 2) +\n  facet_wrap(~(p)) +\n  theme_bw() +\n  labs(title = \"Probability coverage for a variety of binomial confidence interval methods\",\n       subtitle = \"Reference line at 0.95 coverage\")\n\n\n\n\nA good discussion is contained in:\nWallis, Sean A. (2013). “Binomial confidence intervals and contingency tests: mathematical fundamentals and the evaluation of alternative methods” (PDF). Journal of Quantitative Linguistics. 20 (3): 178–208. doi:10.1080/09296174.2013.799918. S2CID 16741749.\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"
  },
  {
    "objectID": "index.html#some-of-my-work",
    "href": "index.html#some-of-my-work",
    "title": "Dean Marchiori",
    "section": "Some of my work",
    "text": "Some of my work\nYou can check out my writing in the blog or you can watch some of my recorded talks.\nIf you want to see how I code, my open source analysis and software projects are hosted on github."
  },
  {
    "objectID": "index.html#licence-and-accreditation",
    "href": "index.html#licence-and-accreditation",
    "title": "Dean Marchiori",
    "section": "Licence and Accreditation",
    "text": "Licence and Accreditation\nAccredited Statistician - Statistical Society of Australia\n(Lic 53250221, Exp 12/25)\n\nAzure Certified Data Science Associate - Microsoft"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html",
    "href": "posts/2023-09-17-geolocating/index.html",
    "title": "Geolocating Sydney’s weirdest property",
    "section": "",
    "text": "A defiant Aussie family has refused to sell their farm-land property despite the entire neighborhood being converted into a new housing estate.\nIs this real? Where is it? Could I geolocate it using just OSINT1 techniques?… Yeah of course.\nI have a loose theory that no matter who or where you are, there is probably sufficient data for a sufficiently motivated person to find you."
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#footnotes",
    "href": "posts/2023-09-17-geolocating/index.html#footnotes",
    "title": "Geolocating Sydney’s weirdest property",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOpen Source Intelligence↩︎\nhttps://x.com/historyinmemes/status/1671673688683413504?s=20↩︎"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#the-challenge",
    "href": "posts/2023-09-17-geolocating/index.html#the-challenge",
    "title": "Geolocating Sydney’s weirdest property",
    "section": "The Challenge",
    "text": "The Challenge\nI saw this pop up on Twitter2 and other tabloid sites a while ago and thought it would be fun to try and geolocate it from the Twitter post alone.\n\n\nA family in Australia has remained defiant in selling their nearly 5-acre property in the last few years as developers have been forced to build around them. Most recently, they declined a whopping $50 million offer for their home. Slap bang in the middle of a new-build… pic.twitter.com/pULUqpe1em\n\n— Historic Vids (@historyinmemes) June 22, 2023\n\n\nThe main clue we get is:\n\nAbout 40 minutes from Sydney’s central core, the property offers panoramic views of the Blue Mountains.\n\n(Actually the tweet and some articles do mention the suburb, but let’s ignore that so we don’t spoil the fun)"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#the-method",
    "href": "posts/2023-09-17-geolocating/index.html#the-method",
    "title": "Geolocating Sydney’s weirdest property",
    "section": "The Method",
    "text": "The Method\n\nIf the property is 40 minutes from Sydney and has views of the Blue Mountains, its likely to be somewhere West of the city.\n\n\n\nWe can use the {osrm} package in R to construct drive time isochrones. This leverages the Open Source Routing Machine, based on Open Street Map data to calculate polygons that represent a given drive time from a set of coordinates. If we set this as 35-45 min drive time from the center of Sydney, we should get a ‘ring’ around Sydney containing the property location.\n\n\n\n\n\n\n\n\n\nBy looking at the final frame we can see some distinguishing features in the image:\n\n\nThere is a roundabout nearby\n\nThere is a ‘Secondary Road’ (not a primary or trunk road, but more significant than roads found in villages etc)\nThere are several ‘dead-end’ roads. Defined as cul-de-sacs that are not turning-circle roundabouts, but have no other exit point.\n\n\nWe can now construct a query to the OpenStreetMap Overpass API which stores features about the metadata of the street network. Let’s look for roundabouts on secondary roads within 500m of a no-exit road.\n[out:json];\n\n(  \nway[junction=roundabout][highway=secondary]\n   ({{bbox}});\n  ) -&gt; .roundabout;\n\n(\nnode[noexit=yes]\n ({{bbox}});\n) -&gt; .culdesac;\n\n(\n way.roundabout(around.culdesac:500);\n);\n\nout body;\n&gt;;\nout skel qt;\n\nThe above query was run over all of Sydney, and exported as a geojson file, which I then intersected with our drive-time ring above.\n\n\n\n\n\n\n\n\n\n\nThis creates a shortlist of 101 candidate roundabouts. We now need to manually inspect each one to match it to the tweet. As these are unsorted, the average search time to find our roundabout of interest will be \\(n/2\\) which means I have have to manually check 50 images on average. To streamline this, I wrote a function to loop through all the candidate roundabouts, and automatically export a satellite image at roughly the level of zoom that would help me identify the right frame.\n\nVideo"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#found-it",
    "href": "posts/2023-09-17-geolocating/index.html#found-it",
    "title": "Geolocating Sydney’s weirdest property",
    "section": "Found it!",
    "text": "Found it!\nAnd found it. Don’t think I need to share the exact address, not that it’s a secret or anything."
  },
  {
    "objectID": "talks/SSA-tutorial/index.html",
    "href": "talks/SSA-tutorial/index.html",
    "title": "Deploying your model code into production with Microsoft Azure",
    "section": "",
    "text": "Objectives:\nTo educate attendees on processes and tools to deploy local python or R modelling code into high-quality, robust, scalable production code.\nTo provide attendees with a practical overview of Microsoft Azure Machine Learning Studio\nTarget Audience:\nData Science practitioners who write python or R code to build predictive models, and are interested in industry best practice for converting experimental code into reliable ML services for their users.\nAgenda and Schedule:\nWelcome and Introduction\n1 hour – Session 1: Intro to Azure Machine Learning Studio\n15 min – Break\n1 hour – Session 2: Model Training and Inference\n15 min – Break\n45 min – Session 3: Deployment and Monitoring\n15 min – Conclusion and Next Steps"
  },
  {
    "objectID": "talks/SSA-tutorial/index.html#abstract",
    "href": "talks/SSA-tutorial/index.html#abstract",
    "title": "Deploying your model code into production with Microsoft Azure",
    "section": "",
    "text": "Objectives:\nTo educate attendees on processes and tools to deploy local python or R modelling code into high-quality, robust, scalable production code.\nTo provide attendees with a practical overview of Microsoft Azure Machine Learning Studio\nTarget Audience:\nData Science practitioners who write python or R code to build predictive models, and are interested in industry best practice for converting experimental code into reliable ML services for their users.\nAgenda and Schedule:\nWelcome and Introduction\n1 hour – Session 1: Intro to Azure Machine Learning Studio\n15 min – Break\n1 hour – Session 2: Model Training and Inference\n15 min – Break\n45 min – Session 3: Deployment and Monitoring\n15 min – Conclusion and Next Steps"
  },
  {
    "objectID": "talks/SSA-tutorial/index.html#links",
    "href": "talks/SSA-tutorial/index.html#links",
    "title": "Deploying your model code into production with Microsoft Azure",
    "section": "Links",
    "text": "Links\nEvent\nSlides"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html",
    "href": "posts/2024-01-04-randomshares/index.html",
    "title": "Is the share market just random noise?",
    "section": "",
    "text": "Earlier in my career, in the afternath of the GFC I worked as trader for an online share trading platform. Everyday I would get blasted with a firehose of emotion from investors. People day-trading on their accounts, needing help placing exotic trades or dealing during volatile markets.\nIt’s easy to get swept up in the excitement. Sweating on every tick of the market. Listening to the live market news and updates, trying to time it just right.\nI had a holiday booked with my partner, so I took a couple of weeks off and didn’t look at the markets. When I returned, it was like I never left. Everyone was still on the same hamster wheel. The market had moved, but it was kind of the same.\nHaving formal mathematics training lent me some perspective. It reminded me of the famous fractal pattern popularised by Benoit Mandelbrot where certain geometric shapes look similar to themselves as you continually zoom in (a process known as self-similarity1).\nIt got me thinking, was this all just random noise. Was this all just a giant circle-jerk based on psychology, sales targets and the need to fill column inches?\nI recently completed Michael Kemp’s excellent book The Ulysses Contract: How to never worry about the share market again.2\nKemp breaks down common myths around ‘authority’ in financial markets and the tendancy for humans to fall prey to cognitive biases and essentially forgetting history when it comes to the markets. I agreed with nearly every word in the book. Although admitedly, I did blush at how my attitudes differed when I was a freshly minted finance graduate.\nAuthor and interesting person Nassim Nicholas Taleb also writes about this in his 2001 classic Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets.3. Despite his writing style being extrememly abrasive, the underlying points are compelling. Are we all blinded by randomness? Commentating, analysing and taking credit for what is essentially random noise happening to us?"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#the-test",
    "href": "posts/2024-01-04-randomshares/index.html#the-test",
    "title": "Is the share market just random noise?",
    "section": "The Test",
    "text": "The Test\nI have simulated 1000 values of a random walk (AR(1)) model and placed this next to 1000 recent observations of a randomly selected Australian stock’s daily closing price.\nCan you tell which is which?"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#the-test-1",
    "href": "posts/2024-01-04-randomshares/index.html#the-test-1",
    "title": "Geo",
    "section": "The test",
    "text": "The test\n\nI will simulate 1000 values of a random walk (AR(1)) model, and place this next to 1000 observations of a random Australian stock’s daily closing price. Can you tell which is which?"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#footnotes",
    "href": "posts/2024-01-04-randomshares/index.html#footnotes",
    "title": "Is the share market just random noise?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Self-similarity↩︎\nhttps://www.goodreads.com/en/book/show/88826560↩︎\nhttps://www.goodreads.com/book/show/38315.Fooled_by_Randomness↩︎\nhttps://otexts.com/fpp3/stationarity.html#random-walk-model↩︎\nhttps://en.wikipedia.org/wiki/Random_walk_hypothesis↩︎"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#a-random-market",
    "href": "posts/2024-01-04-randomshares/index.html#a-random-market",
    "title": "Is the share market just random noise?",
    "section": "A random market?",
    "text": "A random market?\nIt got me thinking, could the average punter tell the difference between the stock market and random noise?\nTo test this, I simulated a type of data called a ‘random walk’. A random walk is a type of mathematical, time-series model. It describes a process of sequential observations (say, daily share prices). Each day’s share price is simulated by taking yesterday’s price and just adding randomly generated ‘noise’. It doesn’t try to emulate the shape or style of share market. It’s really dumb. It just takes the price one day and either adds or subtracts a small random amount (of normally distributed noise).\nThe funny thing about this model is it often contains4:\n\nlong periods of apparent trends up or down\nsudden and unpredictable changes in direction.\n\nSound like the share market anyone??\nThis can be written as:\n\\[\ny_t = y_{t-1} + e_t\n\\] Really, this is just a special case of a class of time-series models called ARIMA models. In particular it is an Autoregressive model, that is, a model that is formed from linear combination of previous values.\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t\n\\] Where we set just one term \\(\\phi_1 = 1\\), \\(c = 0\\) and specify \\(e_t \\sim N(0, s)\\)"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#so-are-the-markets-actually-random",
    "href": "posts/2024-01-04-randomshares/index.html#so-are-the-markets-actually-random",
    "title": "Is the share market just random noise?",
    "section": "So are the markets actually random?",
    "text": "So are the markets actually random?\nI don’t really know. In the short term, it does look random. In the long term, the Australian share market has continued to exhibit long term growth. This doesn’t mean its dynamics aren’t driven my a random walk, after all these models can incorporate drift.\nThere is even a formal theory in finance called the Random Walk hypothesis5 in which some financial heavyweights have argued that stock prices evolve based on a random walk process and are thus - unpredictable.\nRandomness is very tricky - the implication here isn’t that the markets are meaningless or the underlying dynamics are random or that you cant make money.\nRather, we can think of stock prices being modelled well by a random walk process. So what you can do about a process that is modelled well by a random walk?\nYou can start by calling bullshit on attempts to explain or predict short term price fluctuations. Even if they sound confident and wear a suit."
  }
]