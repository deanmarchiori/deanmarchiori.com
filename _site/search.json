[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dean Marchiori",
    "section": "",
    "text": "I‚Äôm a specialist independent data science consultant helping science & technology teams innovate faster with data and maths. Not hype. I work with clients on complex problems across applied mathematics, statistical modelling, data analysis & machine learning.\nIf you are interested in hiring me for a project get in touch: dean@wavedatalabs.com.au\n \n  \n   \n  \n    \n     Contact Me\n  \n  \n    \n     Book a meeting\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Bluesky\n  \n  \n    \n     Download CV"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Dean Marchiori",
    "section": "Bio",
    "text": "Bio\nDean Marchiori is a statistician and founder of Wave Data Labs an independent data science consulting practice. Dean‚Äôs career started in equities and derivatives trading in the aftermath of the GFC before moving on to establish and lead high performance data science teams. His areas of specialisation are in time series modelling & anomaly detection, geospatial analytics, unsupervised machine learning and the R programming language. He is currently co-chair of the Statistical Society of Australia‚Äôs section for statistical computing and visualisation and chair of the SSA working group for data science accreditation. Dean obtained his Bachelor of Science in Mathematics with University Medal from Charles Sturt University. He also holds a Master of Applied Finance degree, and a Master of Applied Statistics from Macquarie University where he was awarded the Julian Leslie Prize and served as a sessional teaching academic in the School of Mathematical and Physical Sciences. Dean holds the Accredited Statistician (AStat) qualification from the Statistical Society of Australia and was named one of the top 10 analytics professionals in Australia by the Institute of Analytics Professionals of Australia (IAPA)."
  },
  {
    "objectID": "index.html#consulting",
    "href": "index.html#consulting",
    "title": "Dean Marchiori",
    "section": "Consulting",
    "text": "Consulting\nIf you are interested in my work you can read some case studies, or see what my clients have to say about working with me on my consulting page. Below are some recent projects delivered.\n\nüïµ Anomaly detection and cluster analysis for financial crime detection: I built an automated system to detect suspicious companies and complex shell relationships.\nüî• Australian Bushfire Risk Analysis & Mapping: This interactive analysis was the first of its kind to highlight bushfire planning risk for the entire Australian mainland and recieved nationwide media coverage.\n\nü¶ü Data Systems and Analytics for Vector-Borne Disease Threat Reduction: I partnered with EcoHealth Alliance on a major scientific research project focused on two WHO priority zoonoses, Rift Valley fever virus and Crimean-Congo hemorrhagic fever virus.\nüë∂ Analysis and mapping of access to Assisted Reproductive Technologies: Did you know over half a million Australians live more than 240km from an Assisted reproductive technology clinic and would likely have to sleep nearby in order to visit."
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "Dean Marchiori",
    "section": "Blog",
    "text": "Blog\nI occassionally write blog posts about data and statistics. Some of my greatest hits are below:\n\nü•ä Man vs Machine Learning: I went head-to-head with Microsoft‚Äôs AutoML platform in a predictive modelling challenge - and won! (sort of)\n\nüè† How to book an Airbnb with Math: Choosing an Airbnb using mathematical analysis of user review data.\n\nüåè Geolocating Sydney‚Äôs weirdest property: Using Open Street Map and R to geolocate a tweet of Sydney‚Äôs strangest house.\n\nüîß Integrating R with Modern Tech Stacks: A Practical Guide to Using R and Docker for Integrating Statistical Analysis into Modern Tech Stacks"
  },
  {
    "objectID": "index.html#open-source",
    "href": "index.html#open-source",
    "title": "Dean Marchiori",
    "section": "Open Source",
    "text": "Open Source\nIf you would like to see how I code, all of my public projects are hosted on Github\n\n‚öôÔ∏è End-to-End MLOps in R A full example project of end-to-end MLOps model deployment in R\nüìò Data Science Workflows in R: An introduction to deploying production quality R code\n‚òÄÔ∏è {weatherOz}: An API Client for Australian Weather and Climate Data Resources [R Package, co-author]\n\nüê∂ bluey-search: Does your child explain Bluey episodes to you but you have no idea what episode they are talking about and can‚Äôt handle flicking through all 130 of them? Problem solved. [shiny app, author]\nüìã analysis-flow: Data Analysis Workflows & Reproducibility Learning Resources [resource list]"
  },
  {
    "objectID": "index.html#publications",
    "href": "index.html#publications",
    "title": "Dean Marchiori",
    "section": "Publications",
    "text": "Publications\nTo view a verified list of my scientific publications see ORCiD\n\nPires et al., (2024). weatherOz: An API Client for Australian Weather and Climate Data Resources in R. Journal of Open Source Software, 9(98), 6717, https://doi.org/10.21105/joss.06717\n\nWilson R, Wickramasuriya R, Marchiori D. An Empirical Modelling and Simulation Framework for Fire Events Initiated by Vegetation and Electricity Network Interactions. Fire. 2023; 6(2):61. https://doi.org/10.3390/fire6020061\n\nMarchiori, D. (2022). Meta-Analysis of Clinical Phenotype and Patient Survival in Neurodevelopmental Disorder with Microcephaly, Arthrogryposis, and Structural Brain Anomalies Due to Bi-allelic Loss of Function Variants in SMPD4. medRxiv, 2022-10. https://doi.org/10.1101/2022.10.08.22280875"
  },
  {
    "objectID": "index.html#talks",
    "href": "index.html#talks",
    "title": "Dean Marchiori",
    "section": "Talks",
    "text": "Talks\nBelow are some invited talks, workshops and other events of note:\n\n2024-10-22: I was invited to deliver a half-day workshop for WOMBAT2024 the Workshop Organised by the Monash Business Analytics Team (WOMBAT). The title was Turning R code into production quality software. [Resources]\n\n2024-10-21: I co-organised the first OceaniaR Hackathon day in Melbourne, the event had a focus on promoting projects and participation from R users in the Oceania region including Australia, New Zealand and the Pacific Islands. [repo]\n\n2023-12-01: I gave a half-day workshop for the Statistical Society of Australia on Deploying your model code into production with Microsoft Azure [Resources]\n\n2022-11-08: I gave a talk called BlueySearch: how to build and deploy a basic data science backed web app in R at the Siligong Data Tech Meetup in Wollongong. [slides]\n\n2021-01-21: I gave a lightning talk at rstudio::global conference called How reproducible am I? A retrospective on a year of commercial data science projects in R [video]\n\n2021-12-01: I presented at the Statistical Society of Australia‚Äôs Early Career and Student Statisticians Career Event 2021 in Sydney.\n2021-12-01: I was interviewed on the Keeping Up With Data podcast. [video]\n\n2019-11-28: I gave an invited talk titled Pearls and pitfalls of time series analysis using Google Analytics data at WOMBAT2019 workshop organised by the Monash Business Analytics Team, at the State Library Victoria. [slides]\n2018-10-16: I Presented a seminar titled Unsupervised outlier detection systems for e-commerce as part of the SMART Seminar Series at the University of Wollongong. [video]\n\nFeatured interviews and talks:"
  },
  {
    "objectID": "consulting/2024-01-01-service-3/index.html",
    "href": "consulting/2024-01-01-service-3/index.html",
    "title": "Youth mental health geospatial hotspot analysis",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nThe hot spot methods involved using a geospatial analytics technique for uncovering local areas of spatial association.\nFull source code can be viewed on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-3/index.html#project",
    "href": "consulting/2024-01-01-service-3/index.html#project",
    "title": "Youth mental health geospatial hotspot analysis",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nThe hot spot methods involved using a geospatial analytics technique for uncovering local areas of spatial association.\nFull source code can be viewed on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-3/index.html#outcomes",
    "href": "consulting/2024-01-01-service-3/index.html#outcomes",
    "title": "Youth mental health geospatial hotspot analysis",
    "section": "Outcomes",
    "text": "Outcomes\nThis analysis of census data reveals youth mental health hot spots: areas with higher mental health diagnosis rates in young people than neighbouring areas.\nTo reference this analysis see here"
  },
  {
    "objectID": "consulting/2024-10-11-eco/index.html",
    "href": "consulting/2024-10-11-eco/index.html",
    "title": "Data Systems and Analytics for Vector-Borne Disease Threat Reduction",
    "section": "",
    "text": "Earlier this year I was fortunate enough to work with EcoHealth Alliance on a major project focused on two WHO priority zoonoses, Rift Valley fever virus and Crimean-Congo hemorrhagic fever virus. Over a 10 year period, research teams sampled over 800 people and livestock, sampled over 300 rodents, and conducted ecological site characterizations at 150 sites in each Tanzania and South Africa.\nTo integrate data from disparate datasets we developed a novel R pipeline to standardize data cleaning, quality control and integration following reproducible science principals. This was one of the first projects of this scale that I have seen using so many tools that align to current industry best practice:\n\nVersion controlled code in Github with strict code review\nFull CI/CD automation with Github Actions\n\n{targets} as a pipeline tool\n\nR package to store and document all functions\n\n{renv} for package dependency management\n\nProper encrypted environment files for all credentials\n\nA common deterrent I hear from beginners around using tools to aid reproducibility in R code often center around the barriers to entry. They are seen as adding complexity and overhead into the process. Which they do (on small projects), but as projects become more and more complex, the benefits quickly outweigh the costs.\nAn overview of this project was given as a poster at the recent 8th World One Health Congress in Cape Town"
  },
  {
    "objectID": "consulting/2024-01-01-service-4/index.html",
    "href": "consulting/2024-01-01-service-4/index.html",
    "title": "Analysis and mapping of access to Assisted Reproductive Technologies",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nFull source code available on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-4/index.html#project",
    "href": "consulting/2024-01-01-service-4/index.html#project",
    "title": "Analysis and mapping of access to Assisted Reproductive Technologies",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nFull source code available on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-4/index.html#outcomes",
    "href": "consulting/2024-01-01-service-4/index.html#outcomes",
    "title": "Analysis and mapping of access to Assisted Reproductive Technologies",
    "section": "Outcomes",
    "text": "Outcomes\nOver half a million Australians live more than 240km from an Assisted reproductive technology clinic and would likely have to sleep nearby in order to visit.\nTo reference this analysis see here"
  },
  {
    "objectID": "consulting/2024-01-01-service-2/index.html",
    "href": "consulting/2024-01-01-service-2/index.html",
    "title": "Anomaly detection and cluster analysis for financial crime detection",
    "section": "",
    "text": "Financial crime has become increasingly complex. Criminals are leveraging intricate methods to conceal illicit activities, making it vital for organizations to stay ahead by using advanced analytical techniques. One such method involves using anomaly detection and cluster analysis on corporate registry data to identify suspicious behavior that could indicate financial crime."
  },
  {
    "objectID": "consulting/2024-01-01-service-2/index.html#footnotes",
    "href": "consulting/2024-01-01-service-2/index.html#footnotes",
    "title": "Anomaly detection and cluster analysis for financial crime detection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSpecific details of the client relationship, methods and code are confidential and cannot be released.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html",
    "href": "posts/2023-06-26-failure/index.html",
    "title": "Why your data science projects are failing",
    "section": "",
    "text": "The most undervalued skill in delivering value with data science teams is picking projects that are likely to succeed. There is no shortcut - it takes years of hard earned experience.\nA number that seems to be floating around is 80% of data science projects will FAIL. Ouch.\nMany of these types of numbers are ‚Äòpredictions‚Äô from consultancies who stand to benefit from making big claims.\nhttps://blogs.gartner.com/andrew_white/2019/01/03/our-top-data-and-analytics-predicts-for-2019/\nCited reasons to fix this include:\nThese are all lovely ideas, but moving the lever on these are often impossible or impractical."
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html#so-what-are-some-easy-things-you-can-you-do",
    "href": "posts/2023-06-26-failure/index.html#so-what-are-some-easy-things-you-can-you-do",
    "title": "Why your data science projects are failing",
    "section": "So what are some easy things you can you do?",
    "text": "So what are some easy things you can you do?\n\nChange your mindset (and how you run projects)\n\nData analytics is an exploratory and scientific endeavour that isn‚Äôt supposed to succeed every time. Just like not all lab experiments yield positive results. Instead of lamenting failures, develop a mindset of innovation and agile working where new ideas are prototyped and investment in R&D promoted but capped and balanced.\n\nPick better projects\n\nA question I get all the time, is how to get started with data science projects in an established business. Often there is a disconnect between those doing the work and those deciding what to do. The most undervalued skill in delivering value with data science teams is picking projects that are likely to succeed. There is no shortcut - it takes years of hard earned experience and it requires a balance of hands-on technical skills, with commercial awareness."
  },
  {
    "objectID": "posts/2023-06-26-failure/index.html#how-can-we-help",
    "href": "posts/2023-06-26-failure/index.html#how-can-we-help",
    "title": "Why your data science projects are failing",
    "section": "How can we help?",
    "text": "How can we help?\nWe have a dedicated program for businesses looking to get started or deepen their data analytics journey. We can help change your attitude and pick better projects.\nBook in a Demo"
  },
  {
    "objectID": "posts/2025-05-21-find-the-shooter/index.html",
    "href": "posts/2025-05-21-find-the-shooter/index.html",
    "title": "Bayesian numerical estimation of fat-tailed distributions",
    "section": "",
    "text": "A palace in the middle of a fictional city is protected by a large wall. During the night, in protest, a citizen randomly opens fire at the palace walls with a machine gun from a fixed but hidden vantage point, riddling the long, straight palace walls with bullet holes. You are called in the morning to the Emperors office to analyse the bullet holes in the wall and determine where the shooter was firing from.\nHere is a rudimentary diagram\nWe can take any one bullet hole and reframe it mathematically:\nFrom the diagram we express the unknown firing angle \\(\\theta\\) for some shot, \\(k\\). The shooter is in position \\((\\alpha, \\beta)\\). We can express the following:\n\\[\ntan (\\theta_k) = \\frac{x_k - \\alpha}{\\beta}\n\\]\nGiving the location along the wall \\(x_k\\) as:\n\\[\nx_k = \\beta tan(\\theta_k) + \\alpha\n\\]\nIf we assume the bullets are sprayed uniformly (and angrily) across an angle of \\(-\\pi/2\\) to \\(\\pi/2\\) (-90¬∞ to 90¬∞) we can simulate the bullet strike locations (\\(x_k\\)) for a known shooter location (\\(\\alpha\\), \\(\\beta\\)).\nLet‚Äôs set the shooter location to 50m right of centre and 70m away from the wall. We will do this to generate the simulated bullet holes. When solving this, we won‚Äôt use these values directly, but rather we will try to recover them using a few analysis techniques.\n# Shooter Location in meters from centre of wall\nalpha &lt;- 50\nbeta &lt;- 70\n\n# Number of shots\nN &lt;- 1000\n\n# simulated firing angles\ntheta &lt;- runif(n = N, min = -pi / 2, max = pi / 2)\n\n# Hole locations in wall\nx_k &lt;- beta * tan(theta) + alpha\ndata.frame(x = x_k) |&gt;\n    ggplot(aes(x)) +\n    geom_histogram() +\n    scale_x_continuous(limits = c(-500, 500)) +\n    geom_vline(xintercept = alpha, lty = 2, col = \"red\") +\n    theme_bw()\nFrom above we assumed the shooting angle is uniformly distributed (rather than aimed at a specific narrow target). So the probability of a given firing angle is:\n\\[\np(\\theta) = \\frac{1}{\\pi}\n\\]\nIn order to find the probability density function for our bullet strikes \\(p(x)\\) we can use a rule to change the variable in a probability density function:\n\\[\np(x) = p(\\theta)\\lvert\\frac{d\\theta}{dx}\\rvert\n\\]\nFor the last part of this equation, we can take the derivative of both sides of \\(\\beta tan(\\theta) = x - \\alpha\\) (see above) with respect to \\(x\\) giving:\n\\[\n\\beta sec^2(\\theta) \\times \\frac{d\\theta}{dx} = 1\n\\]\nDoing some rearranging and using some trigonometric identities we get:\n\\[\np(x | \\alpha, \\beta) = \\frac{1}{\\pi \\beta [1 + (\\frac{x-\\alpha}{\\beta})^2]}\n\\]\nwhich happens to be the PDF of the Cauchy distribution.\nThe parameters for the Cauchy (location and scale parameters) correspond to our alpha and beta, which describe the horizontal and vertical coordinates of our shooter respectively.\nWe can visually overlay this on our simulated sample above to verify.\ndata.frame(x = x_k) |&gt;\n    ggplot() +\n    geom_histogram(aes(x, y = ..density..)) +\n    scale_x_continuous(limits = c(-500, 500)) +\n    geom_function(fun = dcauchy, args = list(location = alpha, scale = beta), color = \"blue\", size = 1) +\n    geom_vline(xintercept = alpha, lty = 2, col = \"red\") +\n    theme_bw()\nThis is a problem as the Cauchy distribution, while stable, it has undefined mean and variance. The above histogram looks well behaved, but I have truncated the x-axis. It‚Äôs actually extremely heavy tailed. Below is the same data in all its glory.\ndata.frame(x = x_k) |&gt;\n    ggplot() +\n    geom_histogram(aes(x, y = ..density..)) +\n    # scale_x_continuous(limits = c(-500, 500)) +\n    geom_vline(xintercept = alpha, lty = 2, col = \"red\") +\n    theme_bw()\nThe implication here is that we cannot rely on the sample mean from a cauchy distribution to converge under the CLT to the true mean, so it‚Äôs unrelibale from an inference perspective. The existence of the fat-tails will prevent this convergence as seen below in the simulation. We can compare this, for reference, to the rapid convergence of samples from a standard normal distribution to its true mean.\nThere are a few sensible heuristics to overcome this such as relying on the mode/median estimates. Another way that is tricky but possible is to use maximum likelihood estimation.\nI won‚Äôt do this directly, but you can easily plot the log-likelihood of the distribution over a grid of parameter values and get a good idea of what‚Äôs going on.\n\\[\n\\hat{\\ell}(x_{1}, \\dotsc, x_{n} \\mid \\alpha, \\beta) = -n \\log(\\beta \\pi) - \\sum_{i=1}^{n} \\log\\left(1 + \\left(\\frac{x_{i} - \\alpha}{\\beta}\\right)^{2}\\right)\n\\]\n# Log-likelihood function\nloglik &lt;- function(x_k, alpha, beta) {\n    -length(x_k) * log(beta * pi) - sum(log(1 + ((x_k - alpha) / beta)^2))\n}\n\n# Define grid of alpha and beta values\nalphas &lt;- seq(1, 100)\nbetas &lt;- seq(1, 100)\n\n# Compute over grid of parameters\ngrid &lt;- outer(alphas, betas, Vectorize(function(alpha, beta) loglik(x_k, alpha, beta)))\nnormalised_grid &lt;- matrix(exp(grid - max(grid)), nrow = length(alphas), ncol = length(betas))\n\n# plot\npersp3d(alphas,\n    betas,\n    normalised_grid,\n    shade = 0.5, col = \"light green\",\n    lit = TRUE, xlab = \"Alpha\", ylab = \"Beta\", zlab = \"Log-Likelihood\"\n)\nA fun way to try and solve this is using Bayesian inference. It would be hard (maybe not possible) to solve it analytically, but we can do this numerically using Hamiltonian Monte Carlo (or a close variant of this) via the stan language in R.\nBelow is the basic model. I can set an uninformative, uniform prior essentially over a 100m x 100m grid outside the palace gate.\ndata\n{\n    int&lt;lower = 0&gt; N;\n    vector[N] x_k;\n}\nparameters\n{\n    real mu;\n    real&lt;lower = 0&gt; sigma;\n}\nmodel\n{\n    // Priors\n    mu ~ uniform(0, 100);\n    sigma ~ uniform(0, 100);\n\n    // Likelihood  \n    x_k ~ cauchy(mu, sigma);\n}\n# Simulated data\ndata_list &lt;- list(N = N, x_k = x_k)\n\n# Compile\nmodel &lt;- cmdstan_model(here::here(\"posts/2025-05-21-find-the-shooter/model.stan\"))\n\nfit &lt;- model$sample(\n    data = data_list,\n    seed = 123,\n    chains = 4,\n    parallel_chains = 4,\n    iter_sampling = 1000,\n    iter_warmup = 500\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 1500 [  6%]  (Warmup) \nChain 1 Iteration:  200 / 1500 [ 13%]  (Warmup) \nChain 1 Iteration:  300 / 1500 [ 20%]  (Warmup) \nChain 1 Iteration:  400 / 1500 [ 26%]  (Warmup) \nChain 1 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 1 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 1 Iteration:  600 / 1500 [ 40%]  (Sampling) \nChain 1 Iteration:  700 / 1500 [ 46%]  (Sampling) \nChain 1 Iteration:  800 / 1500 [ 53%]  (Sampling) \nChain 1 Iteration:  900 / 1500 [ 60%]  (Sampling) \nChain 1 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 1 Iteration: 1100 / 1500 [ 73%]  (Sampling) \nChain 1 Iteration: 1200 / 1500 [ 80%]  (Sampling) \nChain 1 Iteration: 1300 / 1500 [ 86%]  (Sampling) \nChain 1 Iteration: 1400 / 1500 [ 93%]  (Sampling) \nChain 1 Iteration: 1500 / 1500 [100%]  (Sampling) \n\n\nChain 2 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 1500 [  6%]  (Warmup) \nChain 2 Iteration:  200 / 1500 [ 13%]  (Warmup) \nChain 2 Iteration:  300 / 1500 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 1500 [ 26%]  (Warmup) \nChain 2 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 2 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 2 Iteration:  600 / 1500 [ 40%]  (Sampling) \nChain 2 Iteration:  700 / 1500 [ 46%]  (Sampling) \nChain 2 Iteration:  800 / 1500 [ 53%]  (Sampling) \nChain 2 Iteration:  900 / 1500 [ 60%]  (Sampling) \nChain 2 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 2 Iteration: 1100 / 1500 [ 73%]  (Sampling) \nChain 2 Iteration: 1200 / 1500 [ 80%]  (Sampling) \nChain 2 Iteration: 1300 / 1500 [ 86%]  (Sampling) \nChain 2 Iteration: 1400 / 1500 [ 93%]  (Sampling) \nChain 2 Iteration: 1500 / 1500 [100%]  (Sampling) \n\n\nChain 3 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 1500 [  6%]  (Warmup) \nChain 3 Iteration:  200 / 1500 [ 13%]  (Warmup) \nChain 3 Iteration:  300 / 1500 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 1500 [ 26%]  (Warmup) \nChain 3 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 3 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 3 Iteration:  600 / 1500 [ 40%]  (Sampling) \nChain 3 Iteration:  700 / 1500 [ 46%]  (Sampling) \nChain 3 Iteration:  800 / 1500 [ 53%]  (Sampling) \nChain 3 Iteration:  900 / 1500 [ 60%]  (Sampling) \nChain 3 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 3 Iteration: 1100 / 1500 [ 73%]  (Sampling) \nChain 3 Iteration: 1200 / 1500 [ 80%]  (Sampling) \nChain 3 Iteration: 1300 / 1500 [ 86%]  (Sampling) \nChain 3 Iteration: 1400 / 1500 [ 93%]  (Sampling) \nChain 3 Iteration: 1500 / 1500 [100%]  (Sampling) \n\n\nChain 4 Iteration:    1 / 1500 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 1500 [  6%]  (Warmup) \nChain 4 Iteration:  200 / 1500 [ 13%]  (Warmup) \nChain 4 Iteration:  300 / 1500 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 1500 [ 26%]  (Warmup) \nChain 4 Iteration:  500 / 1500 [ 33%]  (Warmup) \nChain 4 Iteration:  501 / 1500 [ 33%]  (Sampling) \nChain 4 Iteration:  600 / 1500 [ 40%]  (Sampling) \nChain 4 Iteration:  700 / 1500 [ 46%]  (Sampling) \nChain 4 Iteration:  800 / 1500 [ 53%]  (Sampling) \nChain 4 Iteration:  900 / 1500 [ 60%]  (Sampling) \nChain 4 Iteration: 1000 / 1500 [ 66%]  (Sampling) \nChain 4 Iteration: 1100 / 1500 [ 73%]  (Sampling) \nChain 4 Iteration: 1200 / 1500 [ 80%]  (Sampling) \nChain 4 Iteration: 1300 / 1500 [ 86%]  (Sampling) \nChain 4 Iteration: 1400 / 1500 [ 93%]  (Sampling) \nChain 4 Iteration: 1500 / 1500 [100%]  (Sampling) \n\n\nChain 1 finished in 0.1 seconds.\nChain 2 finished in 0.1 seconds.\nChain 3 finished in 0.1 seconds.\nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.2 seconds.\nfit$summary() |&gt;\n    knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlp__\n-5615.07\n-5614.76\n1.02\n0.74\n-5617.11\n-5614.08\n1\n1821.00\n2260.39\n\n\nmu\n49.22\n49.27\n3.14\n3.16\n44.06\n54.38\n1\n3256.49\n2405.38\n\n\nsigma\n69.43\n69.34\n3.19\n3.21\n64.37\n75.00\n1\n3140.82\n2484.86\nWe can see it has recovered the parameters mu and sigma. Which are used in stan to represent the location and scale parameters, which we have called \\(\\alpha\\) and \\(\\beta\\) representing the x and y axis locations of our shooter.\nBelow we can see an animation of the convergence of our sampling based on how many data points we used in the estimation."
  },
  {
    "objectID": "posts/2025-05-21-find-the-shooter/index.html#final-thoughts",
    "href": "posts/2025-05-21-find-the-shooter/index.html#final-thoughts",
    "title": "Bayesian numerical estimation of fat-tailed distributions",
    "section": "Final thoughts",
    "text": "Final thoughts\nSo a couple of things, firstly I have used 1000 data points to help generate my posterior samples. Is it reasonable that in this toy example the shooter would have unloaded 1000 rounds? No.¬†It would still work with fewer data, but a key point here is how slow these estimates converge. I have picked a high number to easily demonstrate my point, but you can refer to the animation above to see how unstable it is with few data points. This is a reality in dealing with heavy-tailed distributions like the Cauchy. Looking at the same criticism from another perspective, the data generating process will produce outliers. How likely is it that our wall is thousands of meters long? Would we even detect the shots? Again, probably no. So you might ask why not just truncate the tails and approximate this process using a Gaussian? It would certainly give you easier to obtain estimates of the parameters and fewer headaches. The point here is it would work‚Ä¶until it didn‚Äôt. In fields like financial markets, using gaussian assumptions for fat tailed processes will likely make you look smart in the short term, but when it goes wrong it will go catastrophically wrong. Taleb discusses this at length in this book Statistical Consequences of Fat Tails.\nNote: This is an adaptation of a well known problem called ‚ÄòGull‚Äôs lighthouse problem‚Äô, first mentioned in the paper by Gull ‚ÄúBayesian inductive inference and maximum entropy‚Äù1. There are loads of blog posts solving this, I haven‚Äôt done anything novel except enjoy replicating this by hand to learn a bit more. The mathematical derivation I have used comes from ‚ÄúData Analysis: A Bayesian Tutorial‚Äù by Sivia and Skilling.2\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2025-05-21-find-the-shooter/index.html#footnotes",
    "href": "posts/2025-05-21-find-the-shooter/index.html#footnotes",
    "title": "Bayesian numerical estimation of fat-tailed distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGull, Stephen F. ‚ÄúBayesian inductive inference and maximum entropy.‚Äù Maximum-entropy and bayesian methods in science and engineering: Foundations. Dordrecht: Springer Netherlands, 1988. 53-74.‚Ü©Ô∏é\nSivia, D., & Skilling, J. (2006). Data analysis: a Bayesian tutorial. OUP Oxford.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-06-18-yolo/index.html",
    "href": "posts/2025-06-18-yolo/index.html",
    "title": "Real-time Computer Vision AI",
    "section": "",
    "text": "This post just serves as a basic notes write up of getting started with YOLO for object detection.\nYOLO (You Only Look Once)1 is a real-time computer vision object detection system that uses a single neural network to predict bounding boxes and class probabilities directly from full images in one evaluation.\nThere are many versions of YOLO from v1 up to (at time of writing) v11. I have chosen to use YOLOv52 which is maintained by Ultralytics. The v5 model is popular as it‚Äôs highly performant, very easy to use and is built on the popular PyTorch framework."
  },
  {
    "objectID": "posts/2025-06-18-yolo/index.html#setup",
    "href": "posts/2025-06-18-yolo/index.html#setup",
    "title": "Real-time Computer Vision AI",
    "section": "Setup",
    "text": "Setup\nI am using a laptop with the following specs:\nMemory: 96GB\nProcessor: Intel Core i9-14900HX x 32\nGraphics: NVIDIA GeForce RTX 4050 GPU\nOS: Ubuntu 24.04.2 LTS\n\nDependencies\n\nPython &gt;= 3.80 Get Started\nPyTorch &gt;= 1.8 Get Started\n\n\n\nInstall\n\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt"
  },
  {
    "objectID": "posts/2025-06-18-yolo/index.html#prediction",
    "href": "posts/2025-06-18-yolo/index.html#prediction",
    "title": "Real-time Computer Vision AI",
    "section": "Prediction",
    "text": "Prediction\nHere we will demonstrate prediction (inference) only using a pre-trained model.\nThe model in this example is yolov5su.pt which is a pre-trained PyTorch model for Object Detection tasks. This was trained on the COCO (Common Objects in Context) dataset. COCO is a large-scale object detection, segmentation, and captioning dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models.\n\nImage Object Detection\nHere is an example of object detection on a static jpg image.\n\nfrom ultralytics import YOLO\n\n# load pre-trained model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# download sample image or provide file path to your own\nimg = \"https://ultralytics.com/images/zidane.jpg\" \n\n# Perform inference\nresults = model(img, save=True)\n\n\n\n\nVideo\nYOLOv5 can also perform inference on video files. Here is a short clip I shot on my phone.\n\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLOv5 model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# Define path to video file\nsource = \"input/street.mp4\"\n\n# Run inference on the source\nresults = model(source, save=True) \n\nVideo\n\n\nWebcam streaming\nA subtle variation is to use your webcam as a source and perform live object-detection. Under default settings this will buffer the entire stream into RAM then write it to an .avi file. This can be adjusted to purely streaming the current framer in a live viewer window, which is fun.\n\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# Open the video file. Not the path as '0' to indicate the webcam source\ncap = cv2.VideoCapture(0)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO inference on the frame\n        results = model(frame)\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO Inference\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n\nVideo\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2025-06-18-yolo/index.html#footnotes",
    "href": "posts/2025-06-18-yolo/index.html#footnotes",
    "title": "Real-time Computer Vision AI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou Only Look Once: Unified, Real-Time Object Detection‚Ü©Ô∏é\nJocher, G. (2020). Ultralytics YOLOv5 (Version 7.0) [Computer software]. AGPL-3.0 License. https://github.com/ultralytics/yolov5. https://doi.org/10.5281/zenodo.3908559‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html",
    "href": "posts/2023-07-12-three-questions/index.html",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "",
    "text": "If you have hired a data scientist or run a team of data people, you may not be an expert yourself (that‚Äôs why you hired an expert, right?). So how do you know you are receiving a quality predictive model and not some BS that was thrown carelessly at a black-box machine learning model.\nThe consequences of poorly built statistical models are not trivial. In 2015, Amazon realized its ‚ÄòAI recruiting tool‚Äô didn‚Äôt like women1. In the US, facial recognition models used by police were found to have biases that more commonly misidentified underrepresented communities.2\nSo what can you do about it?"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#three-questions-you-can-ask-your-data-scientist",
    "href": "posts/2023-07-12-three-questions/index.html#three-questions-you-can-ask-your-data-scientist",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "Three questions you can ask your data scientist",
    "text": "Three questions you can ask your data scientist\n\n1. How could the model be unfair?\n\nWhat training data was used, and how was this collected?\nCould certain sub-populations be over or under-represented in this data?\nHave customer details been anonymised?\n\nAre sensitive features used in the model training (race, religion, gender, political preferences).\n\nAlgorithmic unfairness and biases are already huge issues which are only getting worse. Further reading on this topic is covered by Cathy O‚ÄôNeil‚Äôs excellent book Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy3.\n\n\n2. What assumptions does the model make, and how have these been tested?\nAll statistical models have assumptions that are made in order for the math to work out. Models are intended to be simplified representations of reality, deliberately so statisticians can exploit properties like the Central Limit Theorem to help make inferences. For example the assumptions behind a simple linear regression include:\n\nThe response variable can be expressed as a linear combination of the predictors.\n\nThe variance of the error terms is homoscedastic (has constant variance).\n\nThe errors in the response are independent.\n\nIn many cases these can be checked using standard diagnostics tests and plots. However in most cases it requires more in depth domain knowledge and context.\n\n\n3. How accurate is the model?\nEver been told a model is 99% accurate? I‚Äôd be very worried if you had. Be skeptical of very high performance.\nxkcd.com highlight this well with their ‚ÄòIs it Christmas?‚Äô predictive model.\n\n\nOn what data has the model been tested? Is it a completely independent test set? Was there any leakage from the training data? Was the feature engineering done before or after the training/test split (hint: it usually needs to be after).\nIf your model is a binary classification model, you should know what the ‚Äònull model‚Äô is and whether it outperforms this.\nAccuracy is just one measure and is the proportion of correct classifications (both positive and negative class) but you may have different tolerance for misclassifications of each class. For example, you might be predicting the presence of a disease from a test. If you make a false positive, will the risk of side-effects or cost outweigh the risk the making a false negative and having the patient get sicker or die? It‚Äôs tricky.\nIn addition to accuracy ask for the following metrics, along with an explanation of what they mean.\n\nSensitivity / Recall\nSpecificity\nPrecision / Positive Predictive Value\nNegative Predictive Value\nROC AUC\n\nAsk for a confusion matrix. All of the above measures (except ROC) can be calculated from the confusion matrix. Despite its name, it will be instantly clear where the model is working well versus not.\nAsk what cut-off or threshold was used to make the predicted classifications. Many classification models return conditional class probabilities, which need to be converted into labels such as (Yes/No, Cancer/Not Cancer, Churn/Not Churn). A default value is to use a 0.5 probability for the crisp cutoff, but it‚Äôs subjective and depends on the desired trade off in sensitivity / specificity as well as other complicated factors like training class imbalance."
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#what-now",
    "href": "posts/2023-07-12-three-questions/index.html#what-now",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "What now?",
    "text": "What now?\nIf you don‚Äôt get good answers to these questions, you should probably give me a call.\nHave I missed something? Let me know!"
  },
  {
    "objectID": "posts/2023-07-12-three-questions/index.html#footnotes",
    "href": "posts/2023-07-12-three-questions/index.html#footnotes",
    "title": "Three Questions to Ask Your Data Scientist",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G‚Ü©Ô∏é\nhttps://www.nytimes.com/2019/07/08/us/detroit-facial-recognition-cameras.html‚Ü©Ô∏é\nhttps://www.goodreads.com/book/show/28186015-weapons-of-math-destruction‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html",
    "href": "posts/2023-09-11-enterprise-ml/index.html",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "",
    "text": "Are you about to scale up your data analytics team to do more AI/ML work? Here are 5 things you need to know up front to make your life easier.\nThese are tips focused on enterprise-level organisations wanting to implement Microsoft‚Äôs Azure Machine Learning studio. But I‚Äôm sure the content translates well to other settings."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#people---process---platform-pick-any-three",
    "href": "posts/2023-09-11-enterprise-ml/index.html#people---process---platform-pick-any-three",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "1) People - Process - Platform (pick any three)",
    "text": "1) People - Process - Platform (pick any three)\nEffective analytics and machine learning requires coordination of People, Process & Platform. But do you need all three?\nYes. But you don‚Äôt need all three to get started.\nMost Data Scientists will not care about and will not identify as owning MLOps. They want to lead experimentation and innovation, not building production-grade ML pipelines. You will either have to:\n\nFind an internal champion to lead this and train others.\nHire an ML engineer if you have enough work to keep them busy (A recent ML Engineer job ad I saw was around $650k, so bear that in mind).\n\nWork with a specialist partner / consultant to fill this gap and support your teams doing what they are good at.\n\nConverting experimental analytics POCs into robust MLOps-style pipelines is not trivial. It takes real, behind the scenes work which is kind of thankless. But sometimes you need to eat your vegetables and do this. Ensuring you have a well crafted process for governing how, when and why MLOps should exist will help secure the time, funding and social capital you need."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#dont-overlook-security",
    "href": "posts/2023-09-11-enterprise-ml/index.html#dont-overlook-security",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "2) Don‚Äôt overlook security",
    "text": "2) Don‚Äôt overlook security\nWant to know where you are going to face challenges and delays implementing an AI/Ml platform?\nWonder no more: Network Security.\nAzure ML Studio (and any ML platform) will require serious planning around network architecture and security, and for good reason. It‚Äôs not as scary as it sounds, but it does require expertise. You need to have strong representation on your project from a decision maker and a ‚Äòdoer‚Äô in network security. If these people aren‚Äôt inside the tent with you, your project will grind to a halt."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#think-about-dev-work-not-just-mlops",
    "href": "posts/2023-09-11-enterprise-ml/index.html#think-about-dev-work-not-just-mlops",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "3) Think about dev work, not just MLOps",
    "text": "3) Think about dev work, not just MLOps\nIf we are being honest, not many organisations are ready for scalable, reproducible, high performance ML deployments. But almost all orgs are building toward this in the next 5 years. So what‚Äôs most useful in the here and now?\n\nEasy to access online notebooks with various kernels\nClick of a button scalable compute, without fussing about with the infra layer\nAutoML tools to find performance ceilings and explore the solution space of new problems\n\nWant to get your data scientist on board? Tell them they wont have to run jobs on their laptops overnight anymore."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#its-python-heavy-sorry-r-users",
    "href": "posts/2023-09-11-enterprise-ml/index.html#its-python-heavy-sorry-r-users",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "4) It‚Äôs Python heavy (sorry R users)",
    "text": "4) It‚Äôs Python heavy (sorry R users)\nThere a lot to complain about if you are an R user. Azure ML studio is decidedly a Python oriented tool. While admittedly, most high-level AI development work is in Python, in reality a lot of so called ‚ÄúML‚Äù projects are classical statistical models anyway. There are nice tools within R ecosystem for putting R in prod. However if you work in an enterprise setting, you may need to deploy your models on the chosen enterprise analytics platform. This is not ideal for R users, but hopefully in a future post I can show you how to have the best of both worlds."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#its-not-an-etl-environment",
    "href": "posts/2023-09-11-enterprise-ml/index.html#its-not-an-etl-environment",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "5) Its not an ETL environment",
    "text": "5) Its not an ETL environment\nLike any tool its only good for what it‚Äôs good for. When we think of true end-to-end machine learning projects there are steps in that process that are better fit in other tools. A common suspect here is the data import and processing. It‚Äôs nice to coordinate ML deployments with your data engineering teams to ensure you are using the right tools for the right jobs."
  },
  {
    "objectID": "posts/2023-09-11-enterprise-ml/index.html#want-to-learn-more",
    "href": "posts/2023-09-11-enterprise-ml/index.html#want-to-learn-more",
    "title": "Deploying Enterprise Scale AI & Machine Learning Infrastucture",
    "section": "Want to learn more?",
    "text": "Want to learn more?\nIf you are wanting to take the next steps in setting up people, process and platforms for high performance machine learning, get in touch for a chat."
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "",
    "text": "A recent project with repeated measures data involved fitting a random intercept term, and eventually making predictions for new groups not in the training sample. Importantly there was a need for individual predictions rather than population mean level predictions. Now, you obviously cannot include the random effect for a level that is not in your data, so the idea was to make a population level prediction with an adequate prediction interval that reflected the variation from both the fixed and random effects. This is complicated.\nIn the help page for lme4::predict.merMod() is the following note:\nThere are some useful resources out there but it took a while to track down, so this post may serve as a good reference in the future.\nLet‚Äôs go through an example using the famous sleepstudy data showing the average reaction time per day (in milliseconds) for subjects in a sleep deprivation study.\nlibrary(lme4)\nlibrary(tidyverse)\n\ndata(\"sleepstudy\")"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Model",
    "text": "Linear Model\nWe would like to model the relationship between Reaction and Days\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  theme_bw()\n\n\n\n\n\n\n\n\nFitting a basic linear model:\n\nfit_lm &lt;- lm(Reaction ~ Days, data = sleepstudy)\n\nsummary(fit_lm)\n\n\nCall:\nlm(formula = Reaction ~ Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  251.405      6.610  38.033  &lt; 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,    Adjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n\n\n\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw()\n\n\n\n\n\n\n\n\nBut this ignores the fact these data are not independent. We have multiple observation per subject. Some look like a good fit, others not.\n\nggplot(sleepstudy, aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#linear-mixed-effects-model",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Linear Mixed Effects Model",
    "text": "Linear Mixed Effects Model\nLet‚Äôs add a random intercept term for Subject. For simplicity we will leave out any other random effects.\n\nfit &lt;- lme4::lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)\n\nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n\n\nNew fitted lines can be drawn, showing the adjusted intercept for each subject (original regression line kept for reference).\n\nsleepstudy |&gt; \n  mutate(pred = predict(fit, re.form = NULL)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], col = \"grey\") +\n  geom_line(aes(Days, pred), show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#bootstrapped-prediction-intervals-observed-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Bootstrapped Prediction Intervals (observed data)",
    "text": "Bootstrapped Prediction Intervals (observed data)\nLet‚Äôs try and generate prediction intervals using lme4::bootMer() as suggested.\nFirst on the in-sample data.\n\n# predict function for bootstrapping\npredfn &lt;- function(.) {\n  predict(., newdata=new, re.form=NULL)\n}\n\n# summarise output of bootstrapping\nsumBoot &lt;- function(merBoot) {\n  return(\n    data.frame(fit = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.5, na.rm=TRUE))),\n               lwr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.025, na.rm=TRUE))),\n               upr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.975, na.rm=TRUE)))\n    )\n  )\n}\n\n# 'new' data\nnew &lt;- sleepstudy\n\nNotes:\n\nIn the predict() function we specify re.form=NULL which identifies which random effects to condition on. Here NULL includes all random effects. Obviously here you can compute individual predictions assuming you feed it with the correct grouping level in your data.\nIn the lme4::bootMer() function we set use.u=TRUE. This conditions on the random effects and only provides uncertainly estimates for the i.i.d. errors resulting from the fixed effects of the model.\n\n\nIf use.u is TRUE and type==‚Äúparametric‚Äù, only the i.i.d. errors are resampled, with the values of u staying fixed at their estimated values.\n\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=TRUE, type=\"parametric\")\n\n\nnew |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  ggplot(aes(Days, Reaction, col = Subject, fill = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#dealing-with-unobserved-data",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Dealing with unobserved data",
    "text": "Dealing with unobserved data\nHowever, this gets complicated if we want to make predictions for new subjects.\nWe can no longer condition on the random effects, as the new subject level will not have a fitted random intercept value. Instead we need to effectively make a population level prediction (i.e.¬†set the random effect to zero.). This makes sense as we don‚Äôt know what the random effect ought to be for a given, unobserved subject.\nBut we don‚Äôt want the prediction interval to just cover the uncertainty in the population level estimate. If we are interested in individual predictions, how can we incorporate the uncertainly of the random effects in the prediction intervals?\nLets generate a new, unobserved subject.\n\nnew_subject &lt;- tibble(\n  Days = 0:9,\n  Subject = factor(\"999\")\n  )\n\nWe provide a new predict function that doesn‚Äôt condition on the random effects by using re.form = ~0. This lets us input and obtain predictions for new subjects.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\n\nnew_subject |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\n\n\n\n\nHowever using predict just results in a completely deterministic prediction as shown above.\nAn alternative approach is to use lme4::simulate() which will simulate responses for subjects non-deterministically using the fitted model object.\nBelow we can see a comparison on both approaches.\n\npredfn &lt;- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\nsfun &lt;- function(.) {\n    simulate(., newdata=new_subject, re.form=NULL, allow.new.levels=TRUE)[[1]]\n}\n\n\nnew_subject |&gt; \n  bind_cols(simulated = sfun(fit)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  pivot_longer(cols = c(3, 4), names_to = \"type\", values_to = \"val\") |&gt; \n  ggplot(aes(Days, val, col = type)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\n\n\n\n\nWe can use this simulate() function in our bootstrapping to resample responses from the fitted model (rather than resampling deterministic population predictions).\nThis time we set use.u=FALSE to provide uncertainly estimates from both the model errors and the random effects.\n\nIf use.u is FALSE and type is ‚Äúparametric‚Äù, each simulation generates new values of both the ‚Äúspherical‚Äù random effects uu and the i.i.d. errors , using rnorm() with parameters corresponding to the fitted model x.\n\n\nboot &lt;- lme4::bootMer(fit, sfun, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))\n\n\n\n\n\n\n\n\nSo while we don‚Äôt have a conditional mode of the random effect (because its a new subject) we can derive a bootstrapped estimate of the prediction interval by resampling the random effects and model errors on simulated data values."
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#aside",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "Aside",
    "text": "Aside\nFor comparison, here is what the same prediction interval would look like if we just used an unconditional population prediction. While the overall gist is the same, despite also resampling both the random effects and the i.i.d. errors, the interval is narrower as it is resampling just the deterministic population predictions of the model.\n\nboot &lt;- lme4::bootMer(fit, predfn, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n\n\nnew_subject |&gt; \n  bind_cols(sumBoot(boot)) |&gt; \n  bind_cols(predicted = predfn(fit)) |&gt; \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))"
  },
  {
    "objectID": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "href": "posts/2023-02-06-prediction-intervals-for-linear-mixed-effects-models/index.html#references",
    "title": "Prediction Intervals for Linear Mixed Effects Models",
    "section": "References",
    "text": "References\nMost of the material and code is taken from a variety of sources below. In particular the lme4 github issue. Also, the merTools package has a nice vignette comparing these methods with their own solution.\nhttps://tmalsburg.github.io/predict-vs-simulate.html https://github.com/lme4/lme4/issues/388 https://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-confidence-or-prediction-intervals-on-predictions"
  },
  {
    "objectID": "posts/2024-10-11-eco/index.html",
    "href": "posts/2024-10-11-eco/index.html",
    "title": "Data Systems and Analytics for Vector-Borne Disease Threat Reduction",
    "section": "",
    "text": "Earlier this year I was fortunate enough to work with EcoHealth Alliance on a major project focused on two WHO priority zoonoses, Rift Valley fever virus and Crimean-Congo hemorrhagic fever virus. Over a 10 year period, research teams sampled over 800 people and livestock, sampled over 300 rodents, and conducted ecological site characterizations at 150 sites in each Tanzania and South Africa.\nTo integrate data from disparate datasets we developed a novel R pipeline to standardize data cleaning, quality control and integration following reproducible science principals. This was one of the first projects of this scale that I have seen using so many tools that align to current industry best practice:\n\nVersion controlled code in Github with strict code review\nFull CI/CD automation with Github Actions\n\n{targets} as a pipeline tool\n\nR package to store and document all functions\n\n{renv} for package dependency management\n\nProper encrypted environment files for all credentials\n\nA common deterrent I hear from beginners around using tools to aid reproducibility in R code often center around the barriers to entry. They are seen as adding complexity and overhead into the process. Which they do (on small projects), but as projects become more and more complex, the benefits quickly outweigh the costs.\nAn overview of this project was given as a poster at the recent 8th World One Health Congress in Cape Town\n\n\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html",
    "href": "posts/2023-09-17-geolocating/index.html",
    "title": "Geolocating Sydney‚Äôs weirdest property",
    "section": "",
    "text": "A defiant Aussie family has refused to sell their farm-land property despite the entire neighborhood being converted into a new housing estate.\nIs this real? Where is it? Could I geolocate it using just OSINT1 techniques?‚Ä¶ Yeah of course.\nI have a loose theory that no matter who or where you are, there is probably sufficient data for a sufficiently motivated person to find you."
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#the-challenge",
    "href": "posts/2023-09-17-geolocating/index.html#the-challenge",
    "title": "Geolocating Sydney‚Äôs weirdest property",
    "section": "The Challenge",
    "text": "The Challenge\nI saw this pop up on Twitter2 and other tabloid sites a while ago and thought it would be fun to try and geolocate it from the Twitter post alone.\n\n\nA family in Australia has remained defiant in selling their nearly 5-acre property in the last few years as developers have been forced to build around them. Most recently, they declined a whopping $50 million offer for their home. Slap bang in the middle of a new-build‚Ä¶ pic.twitter.com/pULUqpe1em\n\n‚Äî Historic Vids (@historyinmemes) June 22, 2023\n\n\nThe main clue we get is:\n\nAbout 40 minutes from Sydney‚Äôs central core, the property offers panoramic views of the Blue Mountains.\n\n(Actually the tweet and some articles do mention the suburb, but let‚Äôs ignore that so we don‚Äôt spoil the fun)"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#the-method",
    "href": "posts/2023-09-17-geolocating/index.html#the-method",
    "title": "Geolocating Sydney‚Äôs weirdest property",
    "section": "The Method",
    "text": "The Method\n\nIf the property is 40 minutes from Sydney and has views of the Blue Mountains, its likely to be somewhere West of the city.\n\n\n\nWe can use the {osrm} package in R to construct drive time isochrones. This leverages the Open Source Routing Machine, based on Open Street Map data to calculate polygons that represent a given drive time from a set of coordinates. If we set this as 35-45 min drive time from the center of Sydney, we should get a ‚Äòring‚Äô around Sydney containing the property location.\n\n\n\n\n\n\n\n\nBy looking at the final frame we can see some distinguishing features in the image:\n\n\nThere is a roundabout nearby\n\nThere is a ‚ÄòSecondary Road‚Äô (not a primary or trunk road, but more significant than roads found in villages etc)\nThere are several ‚Äòdead-end‚Äô roads. Defined as cul-de-sacs that are not turning-circle roundabouts, but have no other exit point.\n\n\nWe can now construct a query to the OpenStreetMap Overpass API which stores features about the metadata of the street network. Let‚Äôs look for roundabouts on secondary roads within 500m of a no-exit road.\n[out:json];\n\n(  \nway[junction=roundabout][highway=secondary]\n   ({{bbox}});\n  ) -&gt; .roundabout;\n\n(\nnode[noexit=yes]\n ({{bbox}});\n) -&gt; .culdesac;\n\n(\n way.roundabout(around.culdesac:500);\n);\n\nout body;\n&gt;;\nout skel qt;\n\nThe above query was run over all of Sydney, and exported as a geojson file, which I then intersected with our drive-time ring above.\n\n\n\n\n\n\n\n\n\nThis creates a shortlist of 101 candidate roundabouts. We now need to manually inspect each one to match it to the tweet. As these are unsorted, the average search time to find our roundabout of interest will be \\(n/2\\) which means I have have to manually check 50 images on average. To streamline this, I wrote a function to loop through all the candidate roundabouts, and automatically export a satellite image at roughly the level of zoom that would help me identify the right frame.\n\nVideo"
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#found-it",
    "href": "posts/2023-09-17-geolocating/index.html#found-it",
    "title": "Geolocating Sydney‚Äôs weirdest property",
    "section": "Found it!",
    "text": "Found it!\nAnd found it. Don‚Äôt think I need to share the exact address, not that it‚Äôs a secret or anything."
  },
  {
    "objectID": "posts/2023-09-17-geolocating/index.html#footnotes",
    "href": "posts/2023-09-17-geolocating/index.html#footnotes",
    "title": "Geolocating Sydney‚Äôs weirdest property",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOpen Source Intelligence‚Ü©Ô∏é\nhttps://x.com/historyinmemes/status/1671673688683413504?s=20‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-10-24-oceaniar/index.html",
    "href": "posts/2024-10-24-oceaniar/index.html",
    "title": "OceaniaR 2024",
    "section": "",
    "text": "The first ever OceaniaR meeting took place this week in Melbourne!\nThe event took place the day before the Monash WOMBAT 2024 conference and was designed as a hackathon/unconference style event with a focus on promoting projects and participation from R users in the Oceania region including Australia, New Zealand and the Pacific Islands.\nIt was such as nice day with R developers, researchers, scientists, statisticians of all career levels coming together to learn and make new connections. We were also lucky to have participants from Sydney, Brisbane, Perth, Canberra, New Zealand, Noumea, Vanuatu, Papua New Guinea, USA."
  },
  {
    "objectID": "posts/2024-10-24-oceaniar/index.html#projects",
    "href": "posts/2024-10-24-oceaniar/index.html#projects",
    "title": "OceaniaR 2024",
    "section": "Projects",
    "text": "Projects\n\nalt-text-for-data-plots\nAlt text provides a textual alternative to non-text content in HTML documents. It serves various purposes:\n\nIt is necessary for accessibility guidelines.\n\nensures your data visualisations communicate to everybody.\n\nAssistive technologies can convert alt text into other formats such as speech or Braille, providing a description for people using screen reading software.\n\nalt text is displayed in place of the figure if it fails to load, or is viewed in a text-based browser.\n\nalt text can assist both general and image-specific search engines.\n\n\n\nsymbolic\nsymbolic is an R package designed to provide symbolic algebra capabilities natively in R using R functions. It is highly experimental and dependent on external contributors to be a fully fledged Computer Algebra System (CAS). It will be developed to the extent needed for my upstream packages, specifically distributional which requires many symbolic algebra tools not yet readily available in R. Currently powered by yacas via Ryacas.\n\n\npopulaR\npopulaR provides a convenient interface to the UN‚Äôs World Population Prospects API. Several pre-baked datasets are included, as well as a convenient wrapper for more advanced API users\n\n\nozroaddeaths\nozroaddeaths is a package that pulls data from the Australian Road Deaths Database, run by the Bureau of Infrastructure, Transport and Regional Economics (BITRE). This provides basic details of road transport crash fatalities in Australia as reported by the police each month to the State and Territory road safety authorities.\n\n\nnzbabynames & ozbabynames\nThe ozbabynames package provides the dataset ozbabynames. This contains popular Australian baby names by sex, state and year.\n\n\nCase study: Unpaid Labour in Australia\nUnpaid labor includes essential tasks like housework, caregiving, volunteering. These tasks sustains households and communities but are often unrecognized. Women disproportionately bear the burden of unpaid labor, limiting their participation in the paid workforce and contributing to gender inequality. This imbalance affects women‚Äôs economic security and career opportunities, while the value of unpaid work remains largely invisible in traditional economic measures. Recognizing and addressing unpaid labor is key to achieving gender equity and a more inclusive economy.\n\n\nTools for detecting misidentified species in biodiversity databases\nBiodiversity databases such as the Global Biodiversity Information Facility (GBIF) or Atlas of Living Australia (ALA) contain large amounts of open data, but also face persistent challenges in detecting ‚Äòwrong‚Äô points; observations of plants and animals that appear to be in the wrong place, be allocated to the wrong species, or both. Detecting these records is notoriously difficult for those who lack expert knowledge of species biogeography.\n\n\nPackage for mapping Pacific countries and spatial objects\nA customised package for mapping and dealing with spatial data relating to pacific countries and regions could help standardise and speed up the production of insights such as choropleth maps etc.\n\n\nImproved Package for R SDMX Integration for Pacific Data\n{rsdmx} is an existing library exists to parse/read SDMX data and metadata in R. See here: https://github.com/opensdmx/rsdmx. This package is extensive but not tailored for use by Pacific Statistical Agencies and experts. Could a fork or domain specific implementation of this tool be achieved, specifically to improve usability for Pacific statistical data."
  },
  {
    "objectID": "posts/2024-10-24-oceaniar/index.html#photos",
    "href": "posts/2024-10-24-oceaniar/index.html#photos",
    "title": "OceaniaR 2024",
    "section": "Photos",
    "text": "Photos\n\n\n  \n\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "Real-time Computer Vision AI\n\n\nGetting started with YOLOv5 object detection\n\n\n\npython\n\n\ndeeplearning\n\n\n\n\n\n\n\n\n\nJun 18, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian numerical estimation of fat-tailed distributions\n\n\nCan you find the hidden shooter?\n\n\n\nanalysis\n\n\nR\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nMay 21, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nCan you trust what AI is telling you?\n\n\nA plea for reproducible workflows for GenAI analysis\n\n\n\nanalysis\n\n\nR\n\n\nAI\n\n\n\n\n\n\n\n\n\nMay 15, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing an Airbnb using mathematical analysis of review data\n\n\n‚òÖ‚òÖ‚òÖ‚òÖ‚òÖ\n\n\n\nanalysis\n\n\nR\n\n\nbayesian\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Analysis\n\n\nPearls and pitfalls for time-to-event modelling\n\n\n\nsurvival\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nHow AI Works\n\n\nFor the heavily time-constrained\n\n\n\nai\n\n\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nThe CEO‚Äôs Guide to Predicting the Future\n\n\nTime Series Analysis Made Simple\n\n\n\ntime series\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 8, 2025\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating R with Modern Tech Stacks\n\n\nA Practical Guide to Using R and Docker for Integrating Statistical Analysis into Modern Tech Stacks\n\n\n\ndocker\n\n\nR\n\n\n\n\n\n\n\n\n\nDec 1, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\n5 tips for dealing with IT\n\n\nGetting data science done with your IT team\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nOceaniaR 2024\n\n\nR Development and Open Source Software Hackathon - Melbourne 2024\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nData Systems and Analytics for Vector-Borne Disease Threat Reduction\n\n\nA real-world case study of reproducible workflows\n\n\n\nanalysis\n\n\nR\n\n\n\n\n\n\n\n\n\nOct 11, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows: Choosing the Right One\n\n\nDo a Repro-Retro\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nOct 8, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows: Inner Loop vs Outer Loop\n\n\nExperiment and Deploy\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nOct 4, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nData Science Workflows: CRISP-DM\n\n\n90‚Äôs nostaglia that holds up\n\n\n\nAnalysis\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nIs ChatGPT is a bullshit machine?\n\n\nAnd what you should do about it\n\n\n\nai\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nIs the share market just random noise?\n\n\nModelling market prices using a random walk\n\n\n\nR\n\n\ntime series\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nGeolocating Sydney‚Äôs weirdest property\n\n\nUsing Open Street Map and R to geolocate an image\n\n\n\nR\n\n\nOSM\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying Enterprise Scale AI & Machine Learning Infrastucture\n\n\n5 things you need to know before using Azure ML Studio\n\n\n\nanalysis\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nSep 11, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nThree Questions to Ask Your Data Scientist\n\n\nAre your predictive models doing what you think they are?\n\n\n\nanalysis\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nWhy your data science projects are failing\n\n\nand what you can‚Äôt do about it\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nMan vs Machine Learning\n\n\nI went head-to-head with Microsoft‚Äôs AutoML platform in a predictive modelling challenge.\n\n\n\nR\n\n\nazure\n\n\ntime series\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nLevel up your Forecasting\n\n\nQuantify uncertainty with distributional forecasts\n\n\n\nR\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Number Generator Testing\n\n\nA tale of an unemployed ice-cream man, secret societies and a $10 radio\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMay 19, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nDeploy Your Own R Data Science Lab in the Cloud\n\n\n\n\n\n\nR\n\n\nazure\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding your own R Data Science Lab in the browser\n\n\n\n\n\n\nR\n\n\ndocker\n\n\n\n\n\n\n\n\n\nMar 15, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nBeware of Boundaries in Binominal Proportion Confidence Intervals\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nWhen should you be using the Hypergeometric distribution in practice?\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction Intervals for Linear Mixed Effects Models\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nFeb 6, 2023\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Stopping Problems\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nWhat was that Bluey Episode?\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\n\n\n\n\n\n\nFeb 3, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Shiny in a Docker container\n\n\n\n\n\n\nR\n\n\nanalysis\n\n\ndocker\n\n\n\n\n\n\n\n\n\nJan 17, 2022\n\n\nDean Marchiori\n\n\n\n\n\n\n\n\n\n\n\n\nMapping NSW Fire Incidents in R\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nJan 17, 2021\n\n\nDean Marchiori\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html",
    "href": "posts/2023-05-18-RNG-Testing/index.html",
    "title": "Random Number Generator Testing",
    "section": "",
    "text": "We had really interesting discussion with a company recently who needed a random number generator certified.\nRandomness, like time or space, is one of these deep concepts that are super hard to reason about. Despite this, it‚Äôs fairly common to see random number generators in practice. A casino will use one in their gaming software to randomise outcomes; A lottery or competition website will use one to pick winners; Scientists use them to run simulations and cryptographic applications are powered by some form of randomness.\nFlash back to the 1980‚Äôs where down-on-his-luck unemployed ice cream man Michael Larsen cracked the (non-random) pattern in TV game show Press Your Luck and took them for over $100k. Sadly it didnt end well for Larsen with Ponzi schemes, radio station challenges awry and a break and enter. But it goes to show what can happen if you don‚Äôt take randomness seriously."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#aside-ok-but-what-is-random-and-who-is-ramsey",
    "href": "posts/2023-05-18-RNG-Testing/index.html#aside-ok-but-what-is-random-and-who-is-ramsey",
    "title": "Random Number Generator Testing",
    "section": "(Aside) Ok but what is random? And who is Ramsey?",
    "text": "(Aside) Ok but what is random? And who is Ramsey?\nRandomness is an actual or apparent lack of pattern, but it‚Äôs kind of hard to test and even its very nature is somewhat unclear. In 1903, a British mathematician called Frank Ramsey was born. Ramsey was a militant atheist, but interestingly his brother went on to become Archbishop of Canterbury. He went on to study mathematics and economics, becoming a student of famous economist John Maynard Keynes. Somehow he ended up also translating a German book of logical philosophy into English and joined a secret intellectual society just after the war. A minor discovery of his ended up blossoming into what is known as Ramsey theory, which is a theory in mathematical combinatorics showing that in any sufficiently large system, however disordered, there is always some order. This has had interesting (and conspiratorial) implications for whether there is even such a thing as ‚Äòrandom‚Äô. Oh and by the way, all this happened before he died at age 26 after complications from liver disease likely caused by swimming in a river."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#types-of-random-number-generators-rngs",
    "href": "posts/2023-05-18-RNG-Testing/index.html#types-of-random-number-generators-rngs",
    "title": "Random Number Generator Testing",
    "section": "Types of Random Number Generators (RNGs)",
    "text": "Types of Random Number Generators (RNGs)\nGenerally RNG‚Äôs can generate True random numbers or Pseudo random numbers. True RNGs generate random bits from natural stochastic sources like background radiation, quantum effects, atmospheric noise etc. Next time you are tempted to toss a coin, perhaps head over to random.org instead for some ‚Äòtrue‚Äô randomness.\nThere is a fun history lesson for how random.org got started with true RNG‚Äôs generated using random static from a cheap $10 radio, laden with a post-it note advising passers by not to fiddle with the knobs.\n\n\n\nEarly true RNG using atmospheric noise from a cheap radio\n\n\nPseudo-random numbers are generated using a ‚Äòseed‚Äô that deterministically produces numbers that look random, but can be entirely reproduced from the initial seed condition. This is often useful (and used by me all the time) when you need a random sample, but you need it to replicated exactly for scientific reproducibility purposes."
  },
  {
    "objectID": "posts/2023-05-18-RNG-Testing/index.html#statistical-tests",
    "href": "posts/2023-05-18-RNG-Testing/index.html#statistical-tests",
    "title": "Random Number Generator Testing",
    "section": "Statistical Tests",
    "text": "Statistical Tests\nGiven randomness itself is hard to test, there are a number of statistical test suites that perform a battery of diagnostics on a large sample of random numbers in order to test various aspects of randomness. One prominent test suite for cryptographic random bits is developed by NIST which uses 15 different statistical tests.\n\nThe Frequency (Monobit) Test\nFrequency Test within a Block\nThe Runs Test\nTests for the Longest-Run-of-Ones in a Block\nThe Binary Matrix Rank Test\nThe Discrete Fourier Transform (Spectral) Test\nThe Non-overlapping Template Matching Test\nThe Overlapping Template Matching Test\nMaurer‚Äôs ‚ÄúUniversal Statistical‚Äù Test\nThe Linear Complexity Test\nThe Serial Test\nThe Approximate Entropy Test\nThe Cumulative Sums (Cusums) Test\nThe Random Excursions Test\nThe Random Excursions Variant Test\n\nSo, like much of the mathematics behind every day scenarios there is a fascinating history and deep technical and philosophical implications. Given what is on the line for organisations relying on randomness, its useful to engage a specialist to help run and interpret these test suites.\nAnd remember, if you get it wrong, someone unemployed ice-cream man is just waiting to swoop in and take advantage."
  },
  {
    "objectID": "posts/2025-02-06-how-ai-works/index.html",
    "href": "posts/2025-02-06-how-ai-works/index.html",
    "title": "How AI Works",
    "section": "",
    "text": "Do you ever wake up at night, slightly disoriented, breathless, wondering how artificial neural nets are able to optimise weights without encountering discontinutities?\nDo you ever stare at your boss‚Äôs mouth while it‚Äôs moving but you can‚Äôt hear a word they are saying because you realise the words we use are just an artibrary token system that doesn‚Äôt make much sense until we use our little monkey mouths to make sounds.\nWell, regardless here are some enjoyable videos that nicely explain the underlying mathematical concepts and intuition behind common AI models like neural nets and LLM‚Äôs.\nNow you too can be an AI expert without the interminable tedium of doing a mathematics or computer science degree."
  },
  {
    "objectID": "posts/2025-02-06-how-ai-works/index.html#if-you-want-some-intuition-behind-the-atomic-building-blocks-of-ai",
    "href": "posts/2025-02-06-how-ai-works/index.html#if-you-want-some-intuition-behind-the-atomic-building-blocks-of-ai",
    "title": "How AI Works",
    "section": "If you want some intuition behind the ‚Äòatomic‚Äô building blocks of AI",
    "text": "If you want some intuition behind the ‚Äòatomic‚Äô building blocks of AI\nThis is a cute video that build the intuition behind neural nets and a little bit on underlying maths, but still mercifully short."
  },
  {
    "objectID": "posts/2025-02-06-how-ai-works/index.html#if-you-like-a-little-math-and-code",
    "href": "posts/2025-02-06-how-ai-works/index.html#if-you-like-a-little-math-and-code",
    "title": "How AI Works",
    "section": "If you like a little math and code",
    "text": "If you like a little math and code\nThis is a old-ish series on hand-rolling a neural net program in python, which is cool, but assumes a fair bit of basic maths knowledge. But bonus points for the episodes also being short."
  },
  {
    "objectID": "posts/2025-02-06-how-ai-works/index.html#if-you-are-super-keen-and-have-a-spare-3-hours",
    "href": "posts/2025-02-06-how-ai-works/index.html#if-you-are-super-keen-and-have-a-spare-3-hours",
    "title": "How AI Works",
    "section": "If you are super keen and have a spare 3 hours",
    "text": "If you are super keen and have a spare 3 hours\nThis is a slog, but it‚Äôs very well explained by a real expert, in very accessible language.\n\n\nEnjoy.\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2024-06-21-llm-bs/index.html",
    "href": "posts/2024-06-21-llm-bs/index.html",
    "title": "Is ChatGPT is a bullshit machine?",
    "section": "",
    "text": "A recent paper published in ‚ÄòEthics and Information Technology‚Äô titled ChatGPT is bullshit 1 is fast becoming one of my favorite papers. Not least because it uses the word bullshit 165 times.\nThe hype surrounding the use of Large Language Models (LLM‚Äôs) such as ChatGPT, Bard, Llama, Claude etc has been inescapable lately.\nI have become increasingly concerned with the general willingness to accept that these systems are generating knowledge, truth and reason.\nHere is the key message:\nChatGPT-like models are more kindly described as stochastic parrots3. As the authors of the above paper put it ‚ÄúThis means that their primary goal, insofar as they have one, is to produce human-like text. They do so by estimating the likelihood that a particular word will appear next, given the text that has come before.‚Äù\nThey are not searching for truth, they are not concerned with the truthyness of their statements. They just want to give a plausible looking response. Without any regard for if it‚Äôs right.\nWe often hear about these models ‚Äòhallucinating‚Äô as if its an occassional abberation from its otherwise sensical answers. But this is wrong. It‚Äôs a poor metaphor that attempts to anthropomorphize the technology. It‚Äôs something that I‚Äôve always hated about recent AI technology. Its need to be ‚Äòhuman-like‚Äô.\nThe reason so many are fooled by the randomness is that humans are, on average more likely to be right than wrong. So a model driven emulation will often be correct. These applications also act as ‚Äòagent-like‚Äô systems when in fact they don‚Äôt have agency or cognitive reasoning - but they are designed to be interacted with like they do.\nSo should you use this type of AI? Sure, but consider where it‚Äôs appropriate.\nThe benefits in generative and creative applications are impressive and exciting. Applications in pattern recognition, and information retrieval are exciting new ways to think about our relationship with computers.\nTo do nothing and be a luddite would be a mistake. We need more advanced use of algorithmic and model based decisions, but they need to be applied and built in consultation with experts.\nTo surrender your analytical and cognitive reasoning to AI doesn‚Äôt make you look hip and smart, it just puts the stochastic parrot on your shoulder.\nThe key here is to understand the fundamentals and take ownership and accountability for the outputs it produces."
  },
  {
    "objectID": "posts/2024-06-21-llm-bs/index.html#footnotes",
    "href": "posts/2024-06-21-llm-bs/index.html#footnotes",
    "title": "Is ChatGPT is a bullshit machine?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHicks, M.T., Humphries, J. & Slater, J. ChatGPT is bullshit. Ethics Inf Technol 26, 38 (2024). https://doi.org/10.1007/s10676-024-09775-5‚Ü©Ô∏é\nFrankfurt, H. (2005). On Bullshit, Princeton.‚Ü©Ô∏é\nBender, Emily M.; Gebru, Timnit; McMillan-Major, Angelina; Shmitchell, Shmargaret (2021-03-01). ‚ÄúOn the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú‚Äù. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. FAccT ‚Äô21. New York, NY, USA: Association for Computing Machinery. pp.¬†610‚Äì623. doi:10.1145/3442188.3445922.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html",
    "href": "posts/2024-01-04-randomshares/index.html",
    "title": "Is the share market just random noise?",
    "section": "",
    "text": "Earlier in my career, in the afternath of the GFC I worked as trader for an online share trading platform. Everyday I would get blasted with a firehose of emotion from investors. People day-trading on their accounts, needing help placing exotic trades or dealing during volatile markets.\nIt‚Äôs easy to get swept up in the excitement. Sweating on every tick of the market. Listening to the live market news and updates, trying to time it just right.\nI had a holiday booked with my partner, so I took a couple of weeks off and didn‚Äôt look at the markets. When I returned, it was like I never left. Everyone was still on the same hamster wheel. The market had moved, but it was kind of the same.\nHaving formal mathematics training lent me some perspective. It reminded me of the famous fractal pattern popularised by Benoit Mandelbrot where certain geometric shapes look similar to themselves as you continually zoom in (a process known as self-similarity1).\nIt got me thinking, was this all just random noise. Was this all just a giant illusion based on psychology, sales targets and the need to fill column inches?\nI recently completed Michael Kemp‚Äôs excellent book The Ulysses Contract: How to never worry about the share market again.2\nKemp breaks down common myths around ‚Äòauthority‚Äô in financial markets and the tendancy for humans to fall prey to cognitive biases and essentially forgetting history when it comes to the markets. I agreed with nearly every word in the book. Although admitedly, I did blush at how my attitudes differed when I was a freshly minted finance graduate.\nAuthor and interesting person Nassim Nicholas Taleb also writes about this in his 2001 classic Fooled by Randomness: The Hidden Role of Chance in Life and in the Markets.3. Despite his writing style being extrememly abrasive, the underlying points are compelling. Are we all blinded by randomness? Commentating, analysing and taking credit for what is essentially random noise happening to us?"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#a-random-market",
    "href": "posts/2024-01-04-randomshares/index.html#a-random-market",
    "title": "Is the share market just random noise?",
    "section": "A random market?",
    "text": "A random market?\nIt got me thinking, could the average punter tell the difference between the stock market and random noise?\nTo test this, I simulated a type of data called a ‚Äòrandom walk‚Äô. A random walk is a type of mathematical, time-series model. It describes a process of sequential observations (say, daily share prices). Each day‚Äôs share price is simulated by taking yesterday‚Äôs price and just adding randomly generated ‚Äònoise‚Äô. It doesn‚Äôt try to emulate the shape or style of share market. It‚Äôs really dumb. It just takes the price one day and either adds or subtracts a small random amount (of normally distributed noise).\nThe funny thing about this model is it often contains4:\n\nlong periods of apparent trends up or down\nsudden and unpredictable changes in direction.\n\nSound like the share market anyone??\nThis can be written as:\n\\[\ny_t = y_{t-1} + e_t\n\\] Really, this is just a special case of a class of time-series models called ARIMA models. In particular it is an Autoregressive model, that is, a model that is formed from linear combination of previous values.\n\\[\ny_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + e_t\n\\] Where we set just one term \\(\\phi_1 = 1\\), \\(c = 0\\) and specify \\(e_t \\sim N(0, s)\\)"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#the-test",
    "href": "posts/2024-01-04-randomshares/index.html#the-test",
    "title": "Is the share market just random noise?",
    "section": "The Test",
    "text": "The Test\nI have simulated 1000 values of a random walk (AR(1)) model and placed this next to 1000 recent observations of a randomly selected Australian stock‚Äôs daily closing price.\nCan you tell which is which?"
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#so-are-the-markets-actually-random",
    "href": "posts/2024-01-04-randomshares/index.html#so-are-the-markets-actually-random",
    "title": "Is the share market just random noise?",
    "section": "So are the markets actually random?",
    "text": "So are the markets actually random?\nI don‚Äôt really know. In the short term, it does look random. In the long term, the Australian share market has continued to exhibit long term growth. This doesn‚Äôt mean its dynamics aren‚Äôt driven my a random walk, after all these models can incorporate drift.\nThere is even a formal theory in finance called the Random Walk hypothesis5 in which some financial heavyweights have argued that stock prices evolve based on a random walk process and are thus - unpredictable.\nRandomness is very tricky - the implication here isn‚Äôt that the markets are meaningless or the underlying dynamics are random or that you cant make money.\nRather, we can think of stock prices being modelled well by a random walk process. So what you can do about a process that is modelled well by a random walk?\nYou can start by calling bullshit on attempts to explain or predict short term price fluctuations. Even if they sound confident and wear a suit."
  },
  {
    "objectID": "posts/2024-01-04-randomshares/index.html#footnotes",
    "href": "posts/2024-01-04-randomshares/index.html#footnotes",
    "title": "Is the share market just random noise?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Self-similarity‚Ü©Ô∏é\nhttps://www.goodreads.com/en/book/show/88826560‚Ü©Ô∏é\nhttps://www.goodreads.com/book/show/38315.Fooled_by_Randomness‚Ü©Ô∏é\nhttps://otexts.com/fpp3/stationarity.html#random-walk-model‚Ü©Ô∏é\nhttps://en.wikipedia.org/wiki/Random_walk_hypothesis‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "",
    "text": "In a previous post I linked to a project that makes it easy to deploy and extend an existing Rocker Project Docker image to quickly set up a fully featured RStudio Server environment locally on your machine.\nHere I‚Äôll cover some options to deploy this environment to the cloud so you can access it anywhere."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-1-deploy-to-a-virtual-machine",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 1: Deploy to a Virtual Machine",
    "text": "Option 1: Deploy to a Virtual Machine\nA common pattern is to create a Virtual Machine (VM) with a cloud service provider (such as AWS, Azure, GCP) and run your code there. I‚Äôll cover an example using Microsoft Azure.\n\nDeploy a VM with an Ubuntu operating system. Go ahead and choose the compute power you need.\n\n\n\nConfigure a custom network rule to allow traffic on port 8787 for RStudio\n\n\n3. Log into your new VM terminal using SSH\n\n\nInstall Docker Engine by following these steps\nClone and Deploy the docker container from Step 2 in my guide."
  },
  {
    "objectID": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "href": "posts/2023-03-17-deploy-your-own-rstudio-server-in-the-cloud/index.html#option-2-deploy-using-azure-app-service",
    "title": "Deploy Your Own R Data Science Lab in the Cloud",
    "section": "Option 2: Deploy using Azure App Service",
    "text": "Option 2: Deploy using Azure App Service\nThe above is fine, but arguably if you are setting up a VM from scratch for development purposes I‚Äôm not sure what benefit there is from using a docker container. You may as well just directly install what you want and consider the VM a ‚Äòcontainer‚Äô.\nHowever, if you plan to make this available to other users in your organisation, or to adapt this guide for Shiny App development you may be interested in other features such as TLS/SSL security, scale up, advanced networking, continuous integration, continuous deployment, staging/production deployment slots etc. This represents a shift from development sandpit to ‚Äòweb app‚Äô. For this case, Azure App Service may be a lower hassle option. This is Microsoft‚Äôs enterprise grade, web app deployment managed service.\nIn the Virtual Machine model you are setting up compute infrastructure, deploying and running containers directly - then fiddling with the infrastructure layer for everything else. In App Service you deploy your custom docker container (here containing RStudio Server) to Azure Container Registry (kind of like DockerHub). Azure App Services then builds and serves your app from there - without you having to stand up and manage an Infra layer directly.\n\n\nCreate Azure Container Registry (ACR) (or some other Docker repository) using this help guide\nRun and test your container locally\nDeploy your local container to ACR using this help guide\nCreate a new web app in Azure App Services using this help guide\nConfiguration:\n\n\nI didn‚Äôt have to fiddle with ports, presumably it reads the exposed ports in the docker file and does this magically.\n\nFor custom environment variables like the RStudio Server password, I had to manually add this in the config section.\n\n\nand it worked just fine:"
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html",
    "title": "Optimal Stopping Problems",
    "section": "",
    "text": "The August 2022 edition of Significance Magazine posts a challenge:\nThe questions are:\nThis is a well known problem in Optimal Stopping theory. When you look at the first few knights you have no information about how they rank relative to all 100. But by the time you have assessed most of the 100 knights you have information but you have no agency. The knights have all moved on and you either miss out or have to settle for whoever is left.\nThis problem comes up every time I look for a parking spot on a busy day. Do I take the first spot I find a long way from the beach? I could try to get a closer spot but may not find one and could miss out entirely. Most likely the earlier spots are now taken.\nWhen is the optimal time to stop looking and take the leap?\nThe strategy here is:\nWe assume we look at least one knight before stopping our search and assume that if we fail to find a superior knight after our looking period - we miss out! We obviously wont settle for less. (I guess I could force this to accept the last one in line - not a big deal)."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#simulation-in-r",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#simulation-in-r",
    "title": "Optimal Stopping Problems",
    "section": "Simulation in R",
    "text": "Simulation in R\nWe can simulate this problem by sampling 100 random knights and assigning them a random ranking of 1-100 (assume 100 is best).\n\nset.seed(111)\nsample(100)\n\n  [1]  78  84  83  47  25  59  69  35  72  26  49  45  74   8 100  96  24  48\n [19]  95   7  21  15   1   9  63  40  85  93  71  52  28  38  88  61  92  30\n [37]   5  53  37   6  36  41  70  42  18  27  29  23  32  89  86  57  16  90\n [55]  39   4  68  55  99  98  79  43  54  97  65  50  94  44  10  91  56  80\n [73]  19  73  33  11  12  67  81  62  76  60  77  34   2  20  13  51  14  64\n [91]  75  87  66  17  22   3  31  46  82  58\n\n\nNext we can look at the cumulative maximum ranking. This will track the score of the best knight seen so far. We also run, say, 10,000 simulations of this problem to analyse.\n\nlibrary(tidyverse)\n\nset.seed(111)\nn &lt;- 100\nsims &lt;- 1e4\n\nknight_sims &lt;- expand_grid(sim = 1:sims) |&gt; \n  mutate(rankings = map(sim, ~sample(1:n))) |&gt; \n  unnest(rankings) |&gt; \n  group_by(sim) |&gt; \n  mutate(stop_after_days = 1:n,\n         max_rank = cummax(rankings))\n\nknight_sims\n\n# A tibble: 1,000,000 √ó 4\n# Groups:   sim [10,000]\n     sim rankings stop_after_days max_rank\n   &lt;int&gt;    &lt;int&gt;           &lt;int&gt;    &lt;int&gt;\n 1     1       78               1       78\n 2     1       84               2       84\n 3     1       83               3       84\n 4     1       47               4       84\n 5     1       25               5       84\n 6     1       59               6       84\n 7     1       69               7       84\n 8     1       35               8       84\n 9     1       72               9       84\n10     1       26              10       84\n# ‚Ñπ 999,990 more rows\n\n\nWe can write a function that can map over each day and determine what rank knight we would end up with if we stopped searching after x days and committed to the next best knight.\n\noptimal_stop &lt;- function(samp, i) {\n  if (i == 0) {\n    samp[1]\n  }\n  \n  if (i &gt; length(samp) | i &lt; 0) {\n    stop(\"Invalid choice\")\n  }\n  \n  if (any(samp &gt; cummax(samp)[i]) == FALSE) {\n    # if you want to default to the last option\n    #samp[length(samp)]\n    \n    # if you would rather miss out if no superior choice\n    NA\n  } else{\n    idx &lt;- which.max(samp &gt; cummax(samp)[i])\n    samp[idx]\n  }\n}\n\nWe want to know how often we end up with the optimal (top ranked) knight depending on which day we stop looking."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#results",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#results",
    "title": "Optimal Stopping Problems",
    "section": "Results",
    "text": "Results\n\nresults &lt;- knight_sims |&gt;  \n  mutate(result = map_int(stop_after_days, ~optimal_stop(rankings, .x))) |&gt; \n  mutate(is_optimal = result == max(rankings, na.rm = TRUE)) |&gt; \n  group_by(stop_after_days) |&gt; \n  summarise(optimal_stops = sum(is_optimal, na.rm = TRUE),\n            n = n(),\n            optimal_stop_pct = optimal_stops / n,\n            var = sqrt(optimal_stop_pct * (1 - optimal_stop_pct) / n))\n\nresults\n\n# A tibble: 100 √ó 5\n   stop_after_days optimal_stops     n optimal_stop_pct     var\n             &lt;int&gt;         &lt;int&gt; &lt;int&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1               1           533 10000           0.0533 0.00225\n 2               2           846 10000           0.0846 0.00278\n 3               3          1116 10000           0.112  0.00315\n 4               4          1352 10000           0.135  0.00342\n 5               5          1572 10000           0.157  0.00364\n 6               6          1771 10000           0.177  0.00382\n 7               7          1941 10000           0.194  0.00396\n 8               8          2098 10000           0.210  0.00407\n 9               9          2225 10000           0.222  0.00416\n10              10          2360 10000           0.236  0.00425\n# ‚Ñπ 90 more rows\n\n\nWe can plot the success proportion of bagging the most optimal knight for each stopping day.\n\nggplot(results, aes(stop_after_days, optimal_stop_pct)) +\n  geom_line() +\n  geom_ribbon(aes(stop_after_days, ymax = optimal_stop_pct + 1.96 * var, \n                  ymin = optimal_stop_pct - 1.96 * var), \n              alpha = 0.22) +\n  theme_bw() +\n  geom_vline(xintercept = 37, lty = 2, col = 'red') +\n  geom_hline(yintercept = 0.368, lty = 2, col = 'blue') +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"What is the probability of picking the best knight?\",\n       subtitle = \"Based on stopping searching at x days and choosing the next best knight\",\n       x = \"Look for this many days\",\n       y = \"Probability of optimal choice\")\n\n\n\n\n\n\n\n\nHere is the number of knights we should observe without picking any\n\nwhich.max(results$optimal_stop_pct)\n\n[1] 37\n\n\nAnd here is the probability of choosing the best knight with this strategy\n\nmax(results$optimal_stop_pct)\n\n[1] 0.3755\n\n\nSo we should let the first 37 knights pass, just like Merlin said. If we do this, we will give ourselves roughly 37% probability of picking the most optimal knight, which is the best we can do."
  },
  {
    "objectID": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#mathematical-derivation",
    "href": "posts/2022-09-28-solving-the-optimal-stopping-problem-in-r/index.html#mathematical-derivation",
    "title": "Optimal Stopping Problems",
    "section": "Mathematical Derivation",
    "text": "Mathematical Derivation\nSo why 37 days and 37% probability?\nIf we assume we reject the first \\(r-1\\) knights and then choose the next best knight we get to probability\n\\[\nP(r) = \\sum_{j = r}^{n}P(\\text{jth knight is best and you select it})\n\\]\n\\[\nP(r) = \\sum_{j = r}^{n}(\\frac{1}{n})(\\frac{r-1}{j-1}) = (\\frac{r-1}{n})\\sum_{j=r}^{n}\\frac{1}{j-1}\n\\]\nWe want to know which \\(r\\) maximises the probability for large \\(n\\). As \\(n\\) approaches infinity we can let \\(x\\) be the limit of \\(r/n\\) and use \\(t\\) for \\(j/n\\) and \\(dt\\) for \\(1/n\\) we get the integral\n\\[\nP(x) = x \\int_{x}^{1}(\\frac{1}{t})dt = -x\\log(x)\n\\]\nIf we take the derivative with respect to x and set the equation to zero we can find the value of \\(x\\) which maximises this probability:\n\\[\n\\text{optimal x} = \\frac{1}{e} = 0.3678...\n\\]\nand gives the optimal stopping number (for any large n) of \\(\\frac{n}{e} \\approx 37\\)\n\n1/exp(1)\n\n[1] 0.3678794\n\n\n\n100/exp(1)\n\n[1] 36.78794"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html",
    "title": "Running Shiny in a Docker container",
    "section": "",
    "text": "Basic minimal example for running shiny in Docker. It is assumed you have Docker installed."
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#dockerfile",
    "title": "Running Shiny in a Docker container",
    "section": "Dockerfile",
    "text": "Dockerfile\nThis should be adapted as required.\n# Using rocker/rver::version, update version as appropriate\nFROM rocker/r-ver:3.5.0\n\n# install dependencies\nRUN apt-get update && apt-get install -y \\\n    sudo \\\n    gdebi-core \\\n    pandoc \\\n    pandoc-citeproc \\\n    libcurl4-gnutls-dev \\\n    libcairo2-dev \\\n    libxt-dev \\  \n    libxml2-dev \\  \n    libssl-dev \\  \n    wget\n\n\n# Download and install shiny server\nRUN wget --no-verbose https://download3.rstudio.org/ubuntu-14.04/x86_64/VERSION -O \"version.txt\" && \\\n    VERSION=$(cat version.txt)  && \\\n    wget --no-verbose \"https://download3.rstudio.org/ubuntu-14.04/x86_64/shiny-server-$VERSION-amd64.deb\" -O ss-latest.deb && \\\n    gdebi -n ss-latest.deb && \\\n    rm -f version.txt ss-latest.deb && \\\n    . /etc/environment && \\\n    R -e \"install.packages(c('shiny', 'rmarkdown'), repos='$MRAN')\" && \\\n    cp -R /usr/local/lib/R/site-library/shiny/examples/* /srv/shiny-server/\n\n# Copy configuration files into the Docker image\nCOPY shiny-server.conf  /etc/shiny-server/shiny-server.conf\nCOPY shiny-server.sh /usr/bin/shiny-server.sh\n\n# Copy shiny app to Docker image\nCOPY /myapp /srv/shiny-server/myapp\n\n# Expose desired port\nEXPOSE 80\n\nCMD [\"/usr/bin/shiny-server.sh\"]"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-images",
    "title": "Running Shiny in a Docker container",
    "section": "List Images",
    "text": "List Images\ndocker images"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#list-all-containers",
    "title": "Running Shiny in a Docker container",
    "section": "List All Containers",
    "text": "List All Containers\ndocker ps -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#remove-containers",
    "title": "Running Shiny in a Docker container",
    "section": "Remove Containers",
    "text": "Remove Containers\nFor individual containers add the container ID\n$ docker rm\nTo remove all exited containers :\n$ docker rm $(docker ps -a -q -f status=exited)"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#system-prune",
    "title": "Running Shiny in a Docker container",
    "section": "System Prune",
    "text": "System Prune\nRemove all unused containers, networks, images (both dangling and unreferenced), and optionally, volumes.\ndocker system prune -a"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#save-as-tar-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Save as tar-archive",
    "text": "Save as tar-archive\ndocker save -o ~/myapp.tar myapp"
  },
  {
    "objectID": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "href": "posts/2022-01-17-running-shiny-in-a-docker-container/index.html#load-and-run-archive",
    "title": "Running Shiny in a Docker container",
    "section": "Load and Run Archive",
    "text": "Load and Run Archive\ndocker load -i myapp.tar\ndocker run myapp"
  },
  {
    "objectID": "posts/2024-11-18-dealing-with-it/index.html",
    "href": "posts/2024-11-18-dealing-with-it/index.html",
    "title": "5 tips for dealing with IT",
    "section": "",
    "text": "No plan survives first contact with IT\nIn most organisations the single biggest challenge you will face as a data scientist is dealing with IT (Information Technology / Technical Support) to support and enable your data science workflows in Production.\nWhy?\nIt‚Äôs not really anyone‚Äôs fault, it‚Äôs just a lack of alignment.\nIT teams don‚Äôt care heaps about enabling 1% of users to run wild with Open Source technology or free software or weird programming languages.\nThey care about managing risk. They want to enable 99% of users to do their job safely and productively without interruption. They want you to succeed, but they deliberately want friction in the change management process to reduce the risk of breaking changes."
  },
  {
    "objectID": "posts/2024-11-18-dealing-with-it/index.html#tips-to-getting-stuff-done-with-it",
    "href": "posts/2024-11-18-dealing-with-it/index.html#tips-to-getting-stuff-done-with-it",
    "title": "5 tips for dealing with IT",
    "section": "Tips to getting stuff done with IT",
    "text": "Tips to getting stuff done with IT\n\nHave empathy: Understand what their role is. Learn how little you matter to them. Figure out what would make them look good. Find your common enemies and work together. Don‚Äôt get frustrated at them. It‚Äôs a feature not a bug. You just want different things. You have no idea the shit they have to deal with every day.\nPolitical alignment: You can go into battle with some poor IT manager with a million bigger fish to fry, or you can try to influence your own manager. They are actually motivated to ensure your success and will be able to influence IT in a more effective way on your behalf.\nFind supporters: Okay sometimes you just need a favour. Find out who likes you and if they are willing to give you a hand now and then. Reward generously with coffee and thanks. Swing by their desk, ask about cricket or what flavour of Linux they are running at home.\nFind (safe) workarounds: There are areas of grey where policy meets reality. Live in those areas. IT staff are smart technologists, they exploit this grey area too. Just do it ethically. If you org has actively blocked something and have no risk appetite to do something, then don‚Äôt bypass it. Otherwise, sometimes it‚Äôs better to beg for forgiveness than ask for permission.\nBuy commercially licensed software: If you go to your IT manager with arguments like: R/python is free, its open source, it wont cost anything, I can manage it myself, everyone cool on the Internet is using it, SAS/STATA are crap - then you wont get far. IT don‚Äôt care about free and open source. They want to know the software is supported so they can get help when inevitably users call them for help. They want to know the legal terms and conditions to keep legal happy. They want to ensure security vulnerabilities are patched (not by you). They want to know how it will integrate with the enterprise identity management solution. How will users be managed and authorised. They want to manage software in-line with the other billion apps they support and not by some exception because of some nerdy business user (you). Consider buying commercially licensed software like Posit Connect or Workbench. All of these problems go away and its just another piece of software. The argument then just devolves into who pays. And guess what? Everyone in a big organisation has it in their head that software licences are expensive so the sticker price won‚Äôt really surprise anyone.\n\nBonus tip: Share success: Don‚Äôt be a jerk and twist arms in other departments and get what you want then take all the credit yourself. Share success. Present to your IT team‚Äôs monthly meetings, include them in yours. Develop a ‚Äòpartnership‚Äô. They might have to be the ‚Äògatekeepers‚Äô but they work for the same company because they want to make an impact too. So recognise it and appreciate it. You can‚Äôt do impactful work without them.\n\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au"
  },
  {
    "objectID": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "href": "posts/2023-03-13-choosing-the-right-binomial-confidence-interval/index.html",
    "title": "Beware of Boundaries in Binominal Proportion Confidence Intervals",
    "section": "",
    "text": "Binomial proportion confidence intervals are often employed when attempting to perform tests for significance, or sample size calculations around sample measurements resulting from a Bernoulli process.\nThe typical choice when calculating binomial proportion confidence intervals is the asymptotic, or normally approximated ‚ÄòWald‚Äô interval where success probability is measured by:\n\\[\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\\]\nIn many settings, such as marketing analytics or manufacturing processes the sample proportion is close to 0 or 1. Evaluating asymptotic confidence intervals near these boundary conditions will lead to underestimation of the error, and in some cases producing an interval outside \\([0, 1]\\).\nFortunately other methods exist, such as Wilson‚Äôs score interval, exact methods and Bayesian approaches. The recommendation here is to examine the probability coverage and explore alternative methods for sample size and CI calculation, especially when the parameter is near the boundary conditions, or in cases of very small n.¬†\n\nlibrary(binom)\nlibrary(tidyverse)\n\nn &lt;- 50\np &lt;- c(0.01, 0.5, 0.99)\n\n\nx &lt;- purrr::map_df(p, .f =  ~binom.confint(x = n * .x, n = n, methods = 'all'))\n\n\nggplot(x, aes(colour = factor(x))) +\n  geom_point(aes(mean, method), show.legend = F) +\n  geom_errorbarh(aes(xmin = lower, xmax = upper, y = method), show.legend = F) +\n  geom_vline(xintercept =  c(0, 1), lty = 2, col = \"grey\") +\n  facet_wrap(~(x*2/100)) +\n  theme_bw() +\n  labs(title = \"A variety of binomial confidence interval methods for p = 0.01, 0.5 & 0.99\",\n       subtitle = \"Note unusual behaviour near 0.01 and 0.99\")\n\n\n\n\n\n\n\n\n\ncov &lt;- purrr::map_df(p, ~binom.coverage(.x, n, conf.level = 0.95, method = \"all\"))\n\n\nggplot(cov, aes(colour = factor(p))) +\n  geom_point(aes(coverage, method), show.legend = F) +\n  geom_vline(xintercept =  0.95, lty = 2) +\n  facet_wrap(~(p)) +\n  theme_bw() +\n  labs(title = \"Probability coverage for a variety of binomial confidence interval methods\",\n       subtitle = \"Reference line at 0.95 coverage\")\n\n\n\n\n\n\n\n\nA good discussion is contained in:\nWallis, Sean A. (2013). ‚ÄúBinomial confidence intervals and contingency tests: mathematical fundamentals and the evaluation of alternative methods‚Äù (PDF). Journal of Quantitative Linguistics. 20 (3): 178‚Äì208. doi:10.1080/09296174.2013.799918. S2CID 16741749.\nhttps://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval"
  },
  {
    "objectID": "posts/2024-12-02-r-docker/index.html",
    "href": "posts/2024-12-02-r-docker/index.html",
    "title": "Integrating R with Modern Tech Stacks",
    "section": "",
    "text": "There seems to be a subconscious deterrent from using specialised tools like R as part of a production or core tech stack, as if it‚Äôs somehow not compatible with other forms of technology.\nWhich is clealy BS. Over the past year a suprising number of my projects have involved ways to package and integrate complex R code into an existing application.\nThere are many ways to do this, but few are as convincing and uncontroversial as using Docker.\nThis guide will walk through how to use Docker to bridge the gap between R and the rest of the world, enabling you to leverage R‚Äôs power while maintaining the portability, scalability, and consistency required in modern environments.\n\nWhy Combine R and Docker?\nDocker addresses these challenges by containerizing applications along with their dependencies, ensuring that they run consistently across environments. For R, this means you can create a portable, self-contained environment that includes your scripts, libraries, and configurations‚Äîall packaged in a lightweight container.\n\n\nIntegrating R with Docker in 3 steps\nIf you need to, go ahead and install Docker first.\nYou can replicate the below steps by cloning the companion Github Repository.\n\n1. Set Up Your R Environment\nBefore diving into Docker, start by setting up your R project. Ensure that:\n\nYour scripts are modular and well-organized. Below we have a basic setup where the script or code you want to run is in the app.R file.\nDependencies are explicitly listed. I suggesting using {renv}, its good practice in general but it also makes installing the required packages a lot easier later on when we write our Dockerfile.\n\nExample or a bare-minimum directory structure:\nmy-r-project/  \n‚îÇ  \n‚îú‚îÄ‚îÄ R/              # other R scripts or function\n‚îú‚îÄ‚îÄ data/           # Data files (if applicable)  \n‚îú‚îÄ‚îÄ Dockerfile      # Docker configuration file  \n‚îú‚îÄ‚îÄ renv.lock       # Dependency file \n‚îú‚îÄ‚îÄ app.R           # Entry point  \n‚îî‚îÄ‚îÄ README.md       # Documentation  \n\n\n2. Write a Dockerfile for Your R Project\nThe Dockerfile defines the environment for your R application. Here‚Äôs a basic example:\n# Use the official R base image  \nFROM rocker/r-ver:4.3.1  \n\n# Install system dependencies (if needed)  \nRUN apt-get update && apt-get install -y \\  \n    libcurl4-openssl-dev \\  \n    libssl-dev \\  \n    libxml2-dev \\  \n    && rm -rf /var/lib/apt/lists/*  \n\n# Set the working directory  \nWORKDIR /usr/src/app  \n\n# Copy your project files  \nCOPY . .  \n\n# Install R dependencies  \nRUN Rscript -e \"install.packages('renv')\"\nRUN Rscript -e \"renv::restore()\"\n\n# Define the entry point  \nCMD [\"Rscript\", \"app.R\"]  \n\n\nKey Notes on This Dockerfile:\n\nBase Image: We use the rocker/r-ver image, which is optimized for R and maintained here.\n\nSystem Dependencies: Add system-level dependencies your R packages might require.\n\nInstall R packages: Notice how we are replying on renv‚Äôs automated package restoration?\n\n\n\n3. Build and Run the Docker Container\nBuild the container:\ndocker build -t my-r-project .  \nRun the container:\ndocker run my-r-project  \n\n\n\nAdvanced Use Cases with Docker and R\nIf you are doing serious work, you will likely want to move past cute ASCII art. In these cases you should invest a little more time into ensuring your R workflow is fit for purpose. Here are some resources to help go to the next level:\n\nIf you want an easy guide to writing better, production quality R code check out my e-book.\nIn particular, if you want to deploy a predictive model endpoint I cover this in the following case study.\nFor a simple example of deploying R as a shiny app using a base Docker image check this repo out.\nYou can even set up your own development environment using Docker. Just follow these steps.\n\n\n\n\nFinal Thoughts\nThis approach ultimately enhances collaboration between data science and engineering teams and empowers data scientists to use the tools they are most productive with, without sacrificing reliability and portability.\nFor teams looking to modernize their data science infrastructure, starting with Docker and R is a practical and transformative step.\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2025-05-15-trust-in-ai/index.html",
    "href": "posts/2025-05-15-trust-in-ai/index.html",
    "title": "Can you trust what AI is telling you?",
    "section": "",
    "text": "Can we trust the outputs that we are receiving from GenAI products?\nWell trust is built in part, on transparency.\nThe reason more traditional data science and analytics work found its way into mainstream decision making is because it was transparent and trusted. Most serious data scientists I speak to spend a lot of time worrying about reproducibility. From a scientific and computational perspective, this means their analysis be understood and reproduced by others and achieve the same results.\n\n\nWhy is this important?\nReproducibility enables us to understand, criticize and improve on the work of others. Without it, we are just making one flashy claim after another.\nWhile the promises of GenAI are many - how can it ever be used reliably in decision making if we cannot understand, reproduce, critique and improve on each other‚Äôs work?\nThis is a challenge for Generative AI as its underlying mechanics are non-deterministic. Meaning for the exact same inputs, it will return different unpredictable outputs.\n\nChat Completions are non-deterministic by default (which means model outputs may differ from request to request).\n- https://platform.openai.com/docs/advanced-usage\n\n\n\nHow can this be addressed?\nMy challenge for users of GenAI is to meet the same standards as a cooking recipe book: Write down the exact steps you followed so someone else can follow them and get the same results.\nPractically speaking:\n\nIf using a large language model (LLM), interact with it using code via an API. This will allow others to rerun it:\n\n\nWith the exact same prompt\nWith the exact same context (not whatever messed up history you have accumulated in your browser)\n\nWith the exact same model\n\n\n\nSet a seed. OpenAI (and possibly others) are introducing ‚Äòseeds‚Äô which allow you ‚Äúsome control towards deterministic outputs‚Äù.\nShare the code and the output (if you can) so others can replicate your work.\n\n\n\nDo seeds work?\nSort of.\nDisappointingly, this feature is only in Beta and apparently only supported for models: gpt-4-1106-preview and gpt-3.5-turbo-1106. It also doesn‚Äôt really work consistently, but I‚Äôve found it does help a lot.\n\n\nTesting reproducible LLM interactions\nBelow I will use the {ellmer} R package to generate an API call to an OpenAI model with a basic prompt:\n\n‚ÄúGenerate a short excerpt of news about a journey to Mars‚Äù\n\nI will do this twice without setting a seed value, and then I will repeat it with a seed value set and roughly compare the output.\n\nlibrary(ellmer)\n\nFirst I will create a function to send a basic prompt to an OpenAI LLM.\n\ngenerate_response &lt;- function(prompt,\n                              system_message,\n                              model,\n                              seed = NULL) {\n    print(\"LLM RESPONSE ==========================\")\n    chat &lt;- ellmer::chat_openai(\n        model = model,\n        system_prompt = system_message,\n        turns = NULL,\n        seed = seed,\n        echo = \"text\"\n    )\n    chat$chat(prompt)\n}\n\nI will run this twice with no seed value.\n\nnoseed &lt;- replicate(n = 2, {\n    generate_response(\n        system_message = \"You are a helpful assistant.\",\n        prompt = \"Generate a short excerpt of news about a journey to Mars\",\n        model = \"gpt-3.5-turbo-1106\",\n        seed = NULL\n    )\n})\n\n[1] \"LLM RESPONSE ==========================\"\n\"Earlier today, the Mars One mission successfully launched its first crewed spacecraft towards the red planet. The crew of six astronauts will \nembark on a historic journey, with the goal of establishing a permanent human settlement on Mars. The mission, expected to take approximately \nnine months, marks a significant milestone in space exploration and is sure to captivate the imagination of people around the world. Stay tuned \nfor updates on this exciting journey to the final frontier.\"\n[1] \"LLM RESPONSE ==========================\"\n\"NASA's Perseverance rover successfully landed on Mars today, marking a major milestone in the exploration of the red planet. The rover will \nconduct crucial experiments and collect samples to search for signs of ancient life and pave the way for future human missions. This historic \njourney to Mars offers hope for new discoveries and advancements in space exploration.\"\nUsing Jaccaard similarity this has a similarity value of:\n[1] 0.2588235\nNow we repeat with with a random seed value chosen:\n\nwithseed &lt;- replicate(n = 2, {\n    generate_response(\n        system_message = \"You are a helpful assistant.\",\n        prompt = \"Generate a short excerpt of news about a journey to Mars\",\n        model = \"gpt-3.5-turbo-1106\",\n        seed = 12345\n    )\n})\n\n[1] \"LLM RESPONSE ==========================\"\n\"NASA's Perseverance rover successfully landed on the surface of Mars today, marking a significant milestone in the mission to search for signs \nof ancient life on the red planet. The journey, which began seven months ago, culminated in a nail-biting descent and landing in the Jezero \nCrater. Scientists and engineers are now eagerly awaiting the first images and data from the mission, which aims to pave the way for future human\nexploration of Mars.\"\n[1] \"LLM RESPONSE ==========================\"\n\"NASA's Perseverance rover successfully landed on the surface of Mars today, marking a significant milestone in the mission to search for signs \nof ancient life on the red planet. The journey, which began seven months ago, culminated in a nail-biting descent and landing in the Jezero \nCrater. Scientists and engineers are now eagerly awaiting the first images and data from the mission, which aims to pave the way for future human\nexploration of Mars.\"\nAnd we get a Jaccard Index of:\n[1] 1\nThis minor step has allowed me to rerun the workflow and achieve the same response. Making these workflow changes I think will go a long way to making GenAI outputs more transperent and allow robust discussions around how they work and how they can be used to help decision making.\nNote: A more rigourous test with better similirty measurement (using embeddings) of this is done in this post: https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "href": "posts/2023-03-17-building-your-own-r-data-science-lab/index.html",
    "title": "Building your own R Data Science Lab in the browser",
    "section": "",
    "text": "Most R users probably just run RStudio Desktop from Posit on their local computers. This involves manually installing R, RStudio and all other packages.\nHowever it is often the case that users are operating in a restricted computing environment, such as in a corporate or government setting. Alternatively you may wish to create a custom development environment to test or replicate some other specific setup. This is a good case to move away from locally managed software to containerization, such as Docker.\nI have set up a Github repository that sets up a local data science development environment in the browser.\nIt builds a docker container including:\n\nUbuntu 20.04 LTS\nR version 4.2\nRStudio Server 2022.02.3+492\nAll tidyverse packages and devtools\ntex & publishing-related package\n\nThe image builds on the rocker/verse image from Rocker Project.\nSome other enhanced configuration options are included in the Dockerfile, such as preloading you RStudio preferences to get the same look and feel you have locally, the option to install other CRAN packages & mounting local volumes to persist your work locally.\nGo here for Step by step instructions:\nhttps://github.com/deanmarchiori/ds-env-setup"
  },
  {
    "objectID": "posts/2024-09-26-crispdm/index.html",
    "href": "posts/2024-09-26-crispdm/index.html",
    "title": "Data Science Workflows: CRISP-DM",
    "section": "",
    "text": "The cross-industry standard process for data mining (CRISP-DM) is a process model and framework for carrying out data mining projects.\n‚ÄòData mining‚Äô is quite a funny term and will instantly carbon-date you if you use it. It very much of a late 90‚Äôs era. If you were around then your WFH setup probably looked like this. So good.\n\n\n\nStill room for a CD burner\n\n\nNostalgia aside, it remains a popular choice for contemporary data analysis workflows. In my practice I use a version of it for almost every project. I‚Äôm not going to bother doing a deep dive (although here is a nice overview) into each step, as honestly I don‚Äôt think any framework should be rigidly adopted, but here is the thrust of it:\n\nYou start with Business Understanding and Data Understanding as a simultaneous, iterative step. Can‚Äôt do one without the other.\nEventually you want to do some Modeling which will require Data Preparation and feature engineering etc.\nOnce you have some results you want to Evaluate they solve the business problem\nEventually you want to Deploy something.\nThere is also a ‚Äòmeta‚Äô step of this whole thing not being just a one-shot framework and you may need to cycle through it a few times at different levels of detail.\n\n\n\n\nKenneth Jensen, CC BY-SA 3.0 https://creativecommons.org/licenses/by-sa/3.0, via Wikimedia Commons\n\n\nI like it, but want to focus on some of the more important insights I have found in practice:\n\nYou don‚Äôt have to (and shouldn‚Äôt) do the steps in a rigid order. It‚Äôs just a framework.\nDon‚Äôt skimp on Business Understanding and Data Understanding. I know you want to start modelling straight away, just slow down cowboy.\n\nEvaluation doesnt mean model-evaluation (like ROC, AIC etc). That is done in the ‚ÄòModelling‚Äô step. Here it means going back to the business owner or expert and stress-testing the results to see if it solves the problem and makes real-world sense.\nA key reason I use this is to show your stakeholder that you have a process to follow. This will give everyone confidence.\nIt‚Äôs a great tool to facilitate status and project updates. i.e.¬†We are here, and next we are moving in this direction.\n\nThat said, if you try to integrate this into waterfall or scrum project management frameworks you will find it a little unsatisfying, as it is recognised as not a real project management framework.\nFor smaller exploratory or discovery projects I tend to use these 6 phases as ‚Äòstages‚Äô of a project, which makes it easier to draw up a proposal and quote a job. In practice for larger projects you do need to be more iterative and cycle through 2 or three times, so be careful not to timebox each step unless you know you are just going around one time.\n\nThe deployment phase is a little tricky. It used to mean writing a paper or report. Now this is more of a full-blooded discipline in a tech sense. This is where is see the biggest modern drawback: Reconciling analysis work with product development and deployment.\n\nIn a future post I will discuss another framework on how to use CRISP-DM in the context of building and shipping something to production.\nSo should you use it? Yes. It‚Äôs great, and it‚Äôs changed my analysis projects profoundly. Rather than whipping off the curtain at the ‚Äòresults‚Äô presentation and hear everyone in the room gulp at the same time, you manage the flow of results throughout the process so there are no surprises and its truly more collaborative.\n\nWant to read more? Sign up to my mailing list here for weekly emails and no spam.\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au\n\n\n\n\nDon‚Äôt do this"
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html",
    "href": "posts/2025-01-24-survival/index.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "The use of classification algorithms to model binary response data are so ubiquitous there is a risk that in some settings this type of model is inappropriate. In cases when the response data occur over time and subjects may drop out (due to death, loss to follow up etc), traditional methods like logistic regression will not result in accurate estimations. Time to event models like survival analysis may provide a more appropriate way to address this type of data generating system.\nThe below are some brief notes and resources on typical workflows for time-to-event or survival modelling."
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#tldr",
    "href": "posts/2025-01-24-survival/index.html#tldr",
    "title": "Survival Analysis",
    "section": "",
    "text": "The use of classification algorithms to model binary response data are so ubiquitous there is a risk that in some settings this type of model is inappropriate. In cases when the response data occur over time and subjects may drop out (due to death, loss to follow up etc), traditional methods like logistic regression will not result in accurate estimations. Time to event models like survival analysis may provide a more appropriate way to address this type of data generating system.\nThe below are some brief notes and resources on typical workflows for time-to-event or survival modelling."
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#types-of-survival-models",
    "href": "posts/2025-01-24-survival/index.html#types-of-survival-models",
    "title": "Survival Analysis",
    "section": "Types of Survival Models",
    "text": "Types of Survival Models\nThere are several categories of survival models:\n1. Non-Parametric: Most commonly the Kaplan-Meier estimator where no assumptions are imposed on either the shape of the survival function nor on the form of the relationship between predictor variables and survival time.\n2. Semi-Parametric: The standard approach here is the Cox Proportional Hazards Model (PH Model). The hazard function gives the instantaneous potential of having an event at a time, given survival up to that time. No distributional assumptions are made about the survival times or baseline hazard function. This effectively results in a model that scales a ‚Äòbaseline‚Äô hazard function that is never actually estimated, hence the model being a ‚Äòproportional hazards‚Äô model and using only a partial-maximum likelihood estimation method.\n3. Parametric: Fully parametric models exist and rely on the analyst specifying the functional form of the survival curve (exponential, Weibull, log-normal etc.).\n4. Others: Other specialised methods exists such as AFT models exist and can be selected if there is significant justification or if the more traditional methods fail."
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#workflow-notes",
    "href": "posts/2025-01-24-survival/index.html#workflow-notes",
    "title": "Survival Analysis",
    "section": "Workflow Notes",
    "text": "Workflow Notes\n\nIf working with tabular binary outcome data for a specific event, start by defining:\n\nThe observed event time of the event: \\(T_i\\)\n\nThe censoring time (when the subject was last followed up or known to have dropped out, or the end of the observation period): \\(C_i\\)\n\nThe event time can be calculated as \\(Y_i = min(T_i, C_i)\\) with a binary event indicator (\\(\\delta_i\\)):\n\n\\[\n\\begin{equation}\n\\delta_i =\n   \\left\\{\\begin{array}{lr}\n       1, & T_i \\leq C_i \\\\\n       0 & T_i &gt; C_i\n    \\end{array}\\right.\n\\end{equation}\n\\]\n\nA recommended workflow is to start by using a non-parametric approach such as the Kaplan-Meier estimator as a descriptive tool.\nFor between group comparisons, a group-wise KM-curve can be estimated.\nFormal hypothesis testing between groups can be done using a Log-rank test as the typical standard.\nA Cox proportional hazards model is a safe choice to enable the inclusion of multiple covariates of interest. It will not measure the underlying baseline hazard but has good properties for interpreting and testing hazard ratios from the model‚Äôs coefficients (essentially an analogy to odds ratios in logistic regression).\nThe Cox PH model has a number of assumptions:\n\nProportional Hazards: Across time the relative hazard remains constant across covariate levels. This hypothesis can be tested using the scaled Schoenfeld residuals and should graphically be assessed to ensure there is no slope. A formal test can also be conducted using the cox.zph function in the {survival} package in R.\n\nNon-Informative Censoring: While this class of model is designed to handle censored data, the cause of censoring should be unrelated to the response and be entirely non-informative.\n\nResidual diagnostics can also be performed as the primary way to gauge influential observations, possible transformations and issues with non-proportional hazards. They can also be used to test for linearity and additivity in the covariates. There are several types of residuals available (Martingale, Deviance, Score etc.). See here for guidance.\n\nTime dependent covariates (covariates that change after baseline) can also be incorporated but require specialised data re-shaping. See this vignette for a detailed overview first.\nIf the diagnostic tests indicate a poor fit there are several remedies (see here), or an alternative model should be tried. If this is from class of parametric survival models, some knowledge and testing of the distribution will be required as an explicit input.\nResults are typically presented through interpretation of the regression coefficients. The raw coefficients are exponentiated to form a ‚Äòhazard ratio‚Äô which represents the percentage increase(decrease) over(under) the null value of 1 in the instantaneous hazard.\nSubject matter experts are more comfortable interpreting increasing values that are associated with poorer survival experience, rather than protective effects.\n\nBelow is a basic flow chart for a standard survival analysis:\n\n\n\n\n\nflowchart TD\n  A[(survival data)] --&gt; B[Kaplan-Meier Estimator]\n  B --&gt; C[Log Rank Test]  \n  C --&gt; D[Cox Proportional Hazards Model]\n  C -.-&gt; E[Fully parametric model]  \n  C -.-&gt; F[AFT, other exotic..]\n  D --&gt; G[Diagnostics]\n  G --&gt; D\n  \n  \n  style D fill:#f9f"
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#introductory-tutorials",
    "href": "posts/2025-01-24-survival/index.html#introductory-tutorials",
    "title": "Survival Analysis",
    "section": "Introductory tutorials",
    "text": "Introductory tutorials\nSurvival Analysis in R\nAn Introduction to Survival Analysis in R with Emily Zabor"
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#more-in-depth-tutorials",
    "href": "posts/2025-01-24-survival/index.html#more-in-depth-tutorials",
    "title": "Survival Analysis",
    "section": "More in-depth tutorials",
    "text": "More in-depth tutorials\nClark, T., Bradburn, M., Love, S. et al.¬†Survival Analysis Part I: Basic concepts and first analyses. Br J Cancer 89, 232‚Äì238 (2003). https://doi.org/10.1038/sj.bjc.6601118\nBradburn, M., Clark, T., Love, S. et al.¬†Survival Analysis Part II: Multivariate data analysis ‚Äì an introduction to concepts and methods. Br J Cancer 89, 431‚Äì436 (2003). https://doi.org/10.1038/sj.bjc.6601119\nBradburn, M., Clark, T., Love, S. et al.¬†Survival Analysis Part III: Multivariate data analysis ‚Äì choosing a model and assessing its adequacy and fit. Br J Cancer 89, 605‚Äì611 (2003). https://doi.org/10.1038/sj.bjc.6601120\nClark, T., Bradburn, M., Love, S. et al.¬†Survival Analysis Part IV: Further concepts and methods in survival analysis. Br J Cancer 89, 781‚Äì786 (2003). https://doi.org/10.1038/sj.bjc.6601117"
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#text-books",
    "href": "posts/2025-01-24-survival/index.html#text-books",
    "title": "Survival Analysis",
    "section": "Text books",
    "text": "Text books\nHarrell, F. E. (2001). Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis (Vol. 608). New York: springer.\nHosmer Jr, D. W., Lemeshow, S., & May, S. (2008). Applied survival analysis: regression modeling of time-to-event data (Vol. 618). John Wiley & Sons."
  },
  {
    "objectID": "posts/2025-01-24-survival/index.html#software",
    "href": "posts/2025-01-24-survival/index.html#software",
    "title": "Survival Analysis",
    "section": "Software",
    "text": "Software\nsurvival R package\nggsurvfit: Flexible Time-to-Event Figures\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2024-10-08-whichworkflow/index.html",
    "href": "posts/2024-10-08-whichworkflow/index.html",
    "title": "Data Science Workflows: Choosing the Right One",
    "section": "",
    "text": "In previous posts I have discussed frameworks for thinking about data science projects. When it comes to actually writing R code, there are a number of coding workflows you can adopt to get the work done. For example:\n\nMonolithic Notebook (RMarkdown, Quarto)\nDirectory structure with run.R control scripts (e.g.¬†ProjectTemplate)\nOpinionated pipeline tools ({targets}, {drake} etc)\n\nR Package\n\nSo which R workflow should you use?\nUnfortunately this is not a straightforward decision. For quick experimental code you are unlikely to create a new R package. For a complex production deployed model, you really don‚Äôt want all your code in one giant R script. It‚Äôs hard to know ahead of time what to do and where on this spectrum you will end up.\nIn my view, picking the correct workflow needs to align the project goals and scope. Often this choice can evolve throughout the project.\n\nAn evolution\nA concept or idea might be tested in a single R script, like how you would use the back of a napkin for an idea.\nNext you might break this down into chunks and add some prose, heading and plots so you can share and have other understand it.\nNext you might refactor the messy code into functions to better control the flow and the improve development practices. These functions can be documented and unit tested once you know you want to rely on them.\nTo orchestrate the running and dependency structure to avoid re-running slow and complex code you may use the {targets} package.\nFinally to re-use, share and improve on the functions you might spin these out into their own R package!\n\n\nREPRO-RETRO\nA few years back I gave a short talk at the rstudio::global conference. I talked about how you might want to weight and prioritise the elements of your analysis to make an appropriate choice.\nI framed this through the concept of a reproducibility-retrospective (repro-retro)\nEnjoy.\n\n\n\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au"
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html",
    "href": "posts/2023-05-29-forecasting/index.html",
    "title": "Level up your Forecasting",
    "section": "",
    "text": "This is a time series data set of half-hourly electricity demand for Victoria, Australia 1. You may not care about electricity forecasting, but there is probably some similar time series in your organisation you do care about.\nWhat is the forecast demand for the next week?\nWho knows? I certainly don‚Äôt know, despite doing forecasting for a job.\nIn fact, anyone who tells you they do know (with certainty) is wrong."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#but-i-need-to-make-a-forecast-so-what-do-i-do",
    "href": "posts/2023-05-29-forecasting/index.html#but-i-need-to-make-a-forecast-so-what-do-i-do",
    "title": "Level up your Forecasting",
    "section": "But I need to make a forecast, so what do I do?",
    "text": "But I need to make a forecast, so what do I do?\nOften you have to come up with something. Finance needs an estimate of sales or budgets, or your manager needs something to take to the board. So you have to churn out some numbers. But what numbers?\nHere are two extremes:\n\nYou could ignore all historical data and just use the most recent data point as the most reliable forecast.\n\nYou could ignore any specific value and just take the average of everything.\n\n\n\n\n\n\n\n\n\n\nBoth are in the ballpark but they aren‚Äôt very squiggly like the real data. They clearly aren‚Äôt capturing the seasonality.\nYou could be lazy (clever) and just use the same time period from last week as your current forecast. It actually looks really good. But it‚Äôs just one, fixed realization of what could happen. Will it play out exactly like your forecast? Exactly like last week? Almost certainly not.\n\n\n\n\n\n\n\n\n\nLet‚Äôs simulate another way history could play out. (By statistically resampling the residuals in the training data)\n\n\n\n\n\n\n\n\n\nAnd another\n\n\n\n\n\n\n\n\n\nAnd 20 more:\n\n\n\n\n\n\n\n\n\nSo instead of blindly relying on one (kind of okay looking, but totally unrealistic) forecast, we now have a ‚Äòfuzzy‚Äô region of plausible values."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#ok-but-why-bother",
    "href": "posts/2023-05-29-forecasting/index.html#ok-but-why-bother",
    "title": "Level up your Forecasting",
    "section": "Ok but why bother?",
    "text": "Ok but why bother?\nIf you don‚Äôt know with certainty what your forecast is going to be, then don‚Äôt just give one concrete number. It‚Äôs misleading.\n\nIts more important to know how uncertain your forecast is rather than what your forecast is.\n\nYou can still pull out a mean or point estimate but by delivering the whole story you are conveying not just what you think is likely to happen, but how certain you are about it.\nOften this can lead to more meaningful discussions. Perhaps it‚Äôs not the mean of your forecast distribution that you care about, its the extreme values. For example, If I was stress testing business cash flows and forecasting the cost of a maintenance activity, I‚Äôd be more interested in forecasting the 95% upper limit of forecast costs rather than the ‚Äòexpected‚Äô cost.\nIn practice, you often don‚Äôt have to do any mathematical simulations. Proper time series forecasts have methods to calculate prediction intervals out of the box (see below)."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#spoiler",
    "href": "posts/2023-05-29-forecasting/index.html#spoiler",
    "title": "Level up your Forecasting",
    "section": "Spoiler",
    "text": "Spoiler\nWant to know what actually happened? Here it is in red.\n\n\n\n\n\n\n\n\n\nA huge criticism of providing probabilistic forecasts is that is seems like you are ‚Äòhedging your bets‚Äô and being evasive. The reality is, in our electricity example, it was just very hot and demand surged (much higher than even our 95% prediction interval). So if it‚Äôs realistic and plausible to see forecast values in these ranges (or even more extreme) - Why wouldn‚Äôt you want to include that information in your forecast??"
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#aside",
    "href": "posts/2023-05-29-forecasting/index.html#aside",
    "title": "Level up your Forecasting",
    "section": "Aside",
    "text": "Aside\nIn practice a statistician will likely use a more sophisticated model than presented here. These models may take into account temperature and other factors but there will still be unexplained variance that will need to be quantified if you want a quality forecast produced.\nIf you or your organisation want to get serious about making proper forecasts and being proactive when making critical decisions - drop me a line, I can help."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#references",
    "href": "posts/2023-05-29-forecasting/index.html#references",
    "title": "Level up your Forecasting",
    "section": "References",
    "text": "References\nO‚ÄôHara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‚Äòtsibble‚Äô. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/."
  },
  {
    "objectID": "posts/2023-05-29-forecasting/index.html#footnotes",
    "href": "posts/2023-05-29-forecasting/index.html#footnotes",
    "title": "Level up your Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Australian Energy Market Operator. tsibbledata R package.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-02-03-what-was-that-bluey-episode/index.html",
    "href": "posts/2022-02-03-what-was-that-bluey-episode/index.html",
    "title": "What was that Bluey Episode?",
    "section": "",
    "text": "Does your child explain Bluey episodes to you but you have no idea what episode they are talking about and can‚Äôt handle flicking through all 131 of them?\nProblem solved.\nThis website lets you type in the vague descriptions of a small child and it will return a mathematically ranked list of closest matching Bluey episodes.\nhttps://deanmarchiori.shinyapps.io/blueysearch/"
  },
  {
    "objectID": "posts/2022-02-03-what-was-that-bluey-episode/index.html#development-notes",
    "href": "posts/2022-02-03-what-was-that-bluey-episode/index.html#development-notes",
    "title": "What was that Bluey Episode?",
    "section": "Development notes",
    "text": "Development notes\nThe website is a Shiny app (deployed to shinyapps.io), which contains all 130 episode titles, descriptions and thumbnails.\nAll of the episode descriptions were tokenized into ‚Äòterms‚Äô using the {tidytext} R package and formed into a Document-Term-Matrix. Rather than use the typical term counts, a binary indicator was used if a term appears in an episode. This was preferred as the user‚Äôs text input is unlikely to really mimic the detail of an episode description, which threw the similarity measure out a bit.\nOnce a user inputs text, the Shiny app dynamically forms a new term vector and compares it to the all-episode‚Äôs matrix using cosine distance. The episodes are then ranked based on smallest cosine distance and displayed to the users.\nFor the source code visit: https://github.com/deanmarchiori/bluey-search"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html",
    "title": "Man vs Machine Learning",
    "section": "",
    "text": "Over the past few years I have been doing more and more work in Microsoft Azure and the Azure Machine Learning Studio. One feature of the Azure ML studio is an automated machine learning feature. This is essentially a no-code UI that ‚Äòempowers professional and nonprofessional data scientists to build machine learning models rapidly‚Äô (emphasis mine).\nWhile many (including me) have leveled a fair amount of criticism towards such solutions, I thought it would be worth seeing what the fuss was about.\nCould I go head-to-head on the same predictive modelling challenge and compete with the might of Microsoft‚Äôs AutoML solution? Even worse, would I enjoy it? Even more worse, could I win??"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-1-azure-automl",
    "title": "Man vs Machine Learning",
    "section": "Method 1: Azure AutoML",
    "text": "Method 1: Azure AutoML\nThe process to set up a new AutoML job was very easy and assumes you are working under somewhat sanitized conditions (which I was in this case).\n\nOnce you kick it off, it chugged away for an hour and 33 minutes. To my horror, I realized it takes the ‚Äòkitchen sink‚Äô approach and fits a suite of 41 (!) different machine learning models at the training data. Hyperparameter tuning is done by constructing a validation set using K-Fold cross validation.\nVideo\nThe best performing model is then selected and then predictions are run on the test set. It‚Äôs a little concerning that Test set evaluation is only in ‚ÄòPreview‚Äô mode. It was also very confusing to dig out the results on the test set. Most of the metrics prominently displayed are overly confident in-sample accuracy results.\nThe winning model in my case was a ‚ÄòVoting Ensemble‚Äô of three models\n\nMaxAbsScaler, ExtremeRandomTrees\nStandardScalerWrapper, XGBoostRegressor\n\nStandardScalerWrapper, LightGBM\n\nOverall the process was very easy and user friendly. It look a long time to train, but I didn‚Äôt have to think about anything - at all (which is usually time consuming) so overall it was a quick solution. I trained the model on a Standard_DS11_v2 (2 cores, 14 GB RAM, 28 GB disk) compute instance which costs $0.2 per hour. So it cost money, but not much.\nPerformance evaluation to follow below‚Ä¶"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#method-2-manual-time-series-model-in-r",
    "title": "Man vs Machine Learning",
    "section": "Method 2: Manual Time Series Model in R",
    "text": "Method 2: Manual Time Series Model in R\nThe process for doing this myself involved much more thought and brain-effort. Here are some notes.\nThe data set is quite complicated as its sub-daily and has (probably) three seasonal periods (daily, weekly, yearly). There was also maybe some trend and outliers to deal with. The data set also contained covariates such as Temperature and Holiday indicators.\nDue to the seasonal complexity many traditional statistical methods were not appropriate like straight ARIMA (autoregressive integrated moving average) and ETS (exponential smoothing). While STL (Seasonal and Trend decomposition using Loess) can handle multiple seasonal periods I wanted a method to handle the covariates (like Temperature and Holidays). My next step was to think of Time Series Linear Regression models. However, accounting for yearly seasonality with 30min data meant fitting 17,520 (2 * 24 * 365) parameters just for this seasonal period. Which seemed excessive.\nFor longer, multiple-seasonal periods, using Fourier terms can be a good idea. Here a smaller number of terms in a fourier series can be estimated to approximate a more complex function. This type of Dynamic Harmonic Regression2 can also handle exogenous covariates and we can even fit the model with ARIMA errors to account for the short term dynamics of time series data.\nIn fact, this very approach was outlined in the excellent Forecasting: Principles and Practice3 using this very same example data set. I decided to borrow (steal) the ideas of creating a piece-wise linear trend for temperature. I also went a bit crazy with encoding specific holiday dummy variables and some other tweaks.\nOverall I found this method slow to fit, and not overly performant. I decided next to try fitting a Prophet4 model. Prophet is an open-source automated algorithm for time series forecasting developed by Facebook. It uses a Bayesian framework to fit complex, non-linear, piece-wise regression models. For complex time series data, it provides a decent, fast framework including exogenous variables, holiday and seasonal effects. I didn‚Äôt do any principled hyperparameter tuning, but I did fiddle around with the model a bit."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nSo who won?\nThe AutoML platform did :( , but only just. Below is the comparison of RMSE and MAPE. The AutoML is red, my predictions are in blue. I stuffed up over Christmas a bit, which admittedly is a tricky hold-out month for testing.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n213\n\n\nAzure AutoML\nMAPE\n3.56\n\n\nMe\nRMSE\n274\n\n\nMe\nMAPE\n4.96"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#discussion",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#discussion",
    "title": "Man vs Machine Learning",
    "section": "Discussion",
    "text": "Discussion\nSo overall it was pretty close, but in terms of pure predictive performance, the AutoML platform did pip me at the post. Admittedly, the solution I arrived at was probably more of an ML solution than a ‚Äòclassical‚Äô time series method given it is still an automated algorithm. If I had more time and patience I probably could have pursued a more complex regression model. In fact in Forecasting: Principles and Practice, the authors also cite the performance of a straight Dynamic Harmonic Regression is limited, however they go on to propose other innovative approaches56, including splitting the problem into separate models for each 30min period and using regression splines to better capture exogenous effects. So it can be done, but not without a huge amount of effort."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#automl-solution",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#automl-solution",
    "title": "Man vs Machine Learning",
    "section": "AutoML solution",
    "text": "AutoML solution\nThe AutoML platform again used a Voting Ensemble, churned out in 43 minutes, but this time using:\n\nProphetModel (it must have copied me from last round ;))\n\nExponential Smoothing"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#my-solution",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#my-solution",
    "title": "Man vs Machine Learning",
    "section": "My solution",
    "text": "My solution\nGiven the multiplicative process here, I modeled the log transformed data. (I did try a more generalized Box-Cox transformation, but got better performance with a straight natural log transform). I tried an ARIMA model, using model selection via the Hyndman-Khandakar algorithm8, which resulted in a ARIMA(2,0,1)(1,1,2)[12] w/ drift&gt;."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results-1",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#results-1",
    "title": "Man vs Machine Learning",
    "section": "Results",
    "text": "Results\nYay! I won this round. Quite easily.\n\n\n\nMethod\nMetric\nValue\n\n\n\n\nAzure AutoML\nRMSE\n2.43\n\n\nAzure AutoML\nMAPE\n9.22\n\n\nMe\nRMSE\n1.63\n\n\nMe\nMAPE\n7.23"
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#references",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#references",
    "title": "Man vs Machine Learning",
    "section": "References",
    "text": "References\nO‚ÄôHara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‚Äòtsibble‚Äô. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/.\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#other",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#other",
    "title": "Man vs Machine Learning",
    "section": "Other",
    "text": "Other\nThanks to the Tidyverts team https://tidyverts.org/. The new an improved time series stack in R makes all this so easy.\nNote: None of this was super-rigorous, and I certainly tilted the board in my favour here and there. It was just fun and a chance to play around with a tool that I have previously avoided for no real reason."
  },
  {
    "objectID": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#footnotes",
    "href": "posts/2023-06-02-man-vs-ml-20230619T002502Z-001/2023-06-02-man-vs-ml/index.html#footnotes",
    "title": "Man vs Machine Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSource: Australian Energy Market Operator; tsibbledata R package‚Ü©Ô∏é\nYoung, P. C., Pedregal, D. J., & Tych, W. (1999). Dynamic harmonic regression. Journal of Forecasting, 18, 369‚Äì394. https://onlinelibrary.wiley.com/doi/10.1002/(SICI)1099-131X(199911)18:6%3C369::AID-FOR748%3E3.0.CO;2-K‚Ü©Ô∏é\nHyndman, R.J., & Athanasopoulos, G. (2021) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on 2023-06-05.‚Ü©Ô∏é\nTaylor SJ, Letham B. 2017. Forecasting at scale. PeerJ Preprints 5:e3190v2 https://doi.org/10.7287/peerj.preprints.3190v2‚Ü©Ô∏é\nFan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-parametric additive model. IEEE Transactions on Power Systems, 27(1), 134‚Äì141. https://ieeexplore.ieee.org/document/5985500‚Ü©Ô∏é\nHyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak electricity demand. IEEE Transactions on Power Systems, 25(2), 1142‚Äì1153. https://ieeexplore.ieee.org/document/5345698‚Ü©Ô∏é\nSource: Medicare Australia; tsibbledata R package‚Ü©Ô∏é\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The forecast package for R. Journal of Statistical Software, 27(1), 1‚Äì22. https://doi.org/10.18637/jss.v027.i03‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-01-08-time-series/index.html",
    "href": "posts/2025-01-08-time-series/index.html",
    "title": "The CEO‚Äôs Guide to Predicting the Future",
    "section": "",
    "text": "I had an interesting discussion with a client recently about the value proposition for collecting large amounts of time series data. I thought it might be worthwhile recording my thoughts here in case of interest to others."
  },
  {
    "objectID": "posts/2025-01-08-time-series/index.html#references",
    "href": "posts/2025-01-08-time-series/index.html#references",
    "title": "The CEO‚Äôs Guide to Predicting the Future",
    "section": "References",
    "text": "References\nO‚ÄôHara-Wild M, Hyndman R, Wang E, Godahewa R (2022). tsibbledata: Diverse Datasets for ‚Äòtsibble‚Äô. https://tsibbledata.tidyverts.org/, https://github.com/tidyverts/tsibbledata/.\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "",
    "text": "We have a manufacturing process in the day job that is subject to sample auditing.\nThere are \\(N\\) widgets produced and we need to audit \\(n\\) of them. Some sort of rejection threshold is needed on that sample to decide if the whole batch of widgets has met a specified quality level.\nTypically, a binomial distribution would be appropriate for measuring the probability of \\(k\\) successes (in this case defects found) in \\(n\\) independent trials with probability \\(p\\).\n\\[\nPr(X=k) = {n \\choose k} p^k(1-p)^{n-k}\n\\]\nThe word independent is doing a lot of work here as it implies that we are sampling with replacement in order to maintain a fixed probability parameter \\(p\\).\nIn cases where you are taking draws from a population without replacement (such as when you do destructive inspections on a widget) the underlying population changes with each draw and so does the probability \\(p\\).\nIn this case, modelling the process using a hypergeometric distribution may be a more appropriate choice.\n\\[\nPr(X=k) = \\frac{{K \\choose k}{N-K \\choose n-k}}{{N \\choose n}}\n\\]\nIt similarly describes the probability of \\(k\\) successes in \\(n\\) draws without replacement. However, instead of specifying a parameter \\(p\\), we provide the population size \\(N\\), which contains \\(K\\) success states in the population."
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#example",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Example",
    "text": "Example\nLet‚Äôs say we have 2000 widgets manufactured and we want to sample 50 (ignore why 50, that is a whole separate question). We have an assumed quality level of 10% defective units (which we define as ‚Äòsuccess‚Äô for complicated reasons).\nQ: Based on a sample of 50 widgets how many defective units would be considered unlikely (95% CI) to occur randomly given our assumed quality level, and therefore result in us rejecting the entire batch?\nWe can compare the binomial probability mass function with the hypergeometric and observe they are essentially the same.\n\nlibrary(tidyverse)\n\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\n\n\n\n\nHowever, if we had a smaller population of say 100 or 70 widgets, how would this compare?\n\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       hypergeom_100 = dhyper(x, m = 10, n = 90, k = 50),\n       hypergeom_070 = dhyper(x, m = 7, n = 63, k = 50)\n       ) |&gt; \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |&gt; \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n\n\n\n\n\n\n\n\nWe can see these curves are markedly different. And indeed the 95% confidence intervals obtained are narrower for the hypergeometric case.\n\nqbinom(p = c(0.025, 0.975), size = 50, prob = 0.1)\n\n[1] 1 9\n\nqhyper(p = c(0.025, 0.975), m = 10, n = 90, k = 50)\n\n[1] 2 8\n\n\nWe can see from a random draw of 1 million samples from each PMF that they both have the same expected values, but the variance is smaller in the hypergeometric case.\n\nX &lt;- rbinom(n = 1e6, size = 50, prob = 0.1)\nY &lt;- rhyper(nn = 1e6, m = 10, n = 90, k = 50)\n\nmean(X)\n\n[1] 4.998998\n\nvar(X)\n\n[1] 4.497051\n\nmean(Y)\n\n[1] 4.997974\n\nvar(Y)\n\n[1] 2.274506"
  },
  {
    "objectID": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "href": "posts/2023-02-20-when-should-you-be-using-the-hypergeometric-distribution-in-practice/index.html#does-it-matter-which-one-you-use",
    "title": "When should you be using the Hypergeometric distribution in practice?",
    "section": "Does it matter which one you use?",
    "text": "Does it matter which one you use?\nAs a consequence of removing samples in each draw we influence the probability of a subsequent success. If our \\(N\\) and \\(K\\) is very large relative to our sample \\(n\\) this wont make much of an impact, but it can be impactful for smaller populations, or relatively larger samples.\nFrom our example above, failing to use a hypergeometric distribution to model this process for smaller populations will result in wider, more conservative acceptance regions which can increase consumer risk in a manufacturing process.\nTypical guidance on when to use each distribution is given in manufacturing standards such as AS 1199.1-2003: Sampling Procedures for Inspection by Attributes and typically involves how you structure your sampling scheme."
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "",
    "text": "This feed contains a list of current incidents from the NSW RFS, and includes location data and Major Fire Update summary information where available. Click through from the feed to the NSW RFS website for full details of the update.\nGeoJSON is a lightweight data standard that has emerged to support the sharing of information with location or geospatial data. It is widely supported by modern applications and mobile devices.\nSee here: https://www.rfs.nsw.gov.au/news-and-media/stay-up-to-date/feeds for attribution and guidelines. Please read these important guidelines before using this data."
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#current-incidents-feed-geojson",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#current-incidents-feed-geojson",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "",
    "text": "This feed contains a list of current incidents from the NSW RFS, and includes location data and Major Fire Update summary information where available. Click through from the feed to the NSW RFS website for full details of the update.\nGeoJSON is a lightweight data standard that has emerged to support the sharing of information with location or geospatial data. It is widely supported by modern applications and mobile devices.\nSee here: https://www.rfs.nsw.gov.au/news-and-media/stay-up-to-date/feeds for attribution and guidelines. Please read these important guidelines before using this data."
  },
  {
    "objectID": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#code",
    "href": "posts/2021-01-17-mapping-nsw-current-incidents-in-r/index.html#code",
    "title": "Mapping NSW Fire Incidents in R",
    "section": "Code",
    "text": "Code\nLoad packages\n\nlibrary(sf)\nlibrary(mapview)\nlibrary(tidyverse)\n\nPull incidents\n\nurl &lt;- \"http://www.rfs.nsw.gov.au/feeds/majorIncidents.json\"\nfires &lt;- st_read(url)\n\nReading layer `majorIncidents' from data source \n  `http://www.rfs.nsw.gov.au/feeds/majorIncidents.json' using driver `GeoJSON'\nSimple feature collection with 30 features and 7 fields\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 145.1853 ymin: -37.10646 xmax: 153.4195 ymax: -28.33643\nGeodetic CRS:  WGS 84\n\n\nOptional step to get points only\n\n# points only\nfire_pt &lt;- fires %&gt;% \n  st_cast(\"POINT\") \n\nOptional Step to get Polygons only. Note the hack to aply a zero distance buffer.\n\n#' Polygons only\nfire_poly &lt;- fires %&gt;% \n  st_buffer(dist = 0) %&gt;% \n  st_union(by_feature = TRUE)\n\nMapping data interactively\n\nmapview(fire_poly, layer.name = \"RFS Current Incident Polygons\", zcol = \"category\") +\n  mapview(fire_pt, layer.name = \"RFS Current Incident Locations\", zcol = \"category\")\n\n\n\n\nscreenshot of leafet interactive map of fire incidents"
  },
  {
    "objectID": "posts/2025-03-05-star-ratings/index.html",
    "href": "posts/2025-03-05-star-ratings/index.html",
    "title": "Choosing an Airbnb using mathematical analysis of review data",
    "section": "",
    "text": "Have you ever looked for a hotel, Airbnb, restaurant and checked out the reviews?\nAlways go for the highest rating right?\nSure..although that 5 star apartment only has 3 reviews‚Ä¶Ooo this one has more, oh but its only 4.72 stars‚Ä¶hmmm. Shit. What do I do?\nWe know higher reviews are better than lower reviews. But we also know more reviews are more reliable than fewer reviews."
  },
  {
    "objectID": "posts/2025-03-05-star-ratings/index.html#questions",
    "href": "posts/2025-03-05-star-ratings/index.html#questions",
    "title": "Choosing an Airbnb using mathematical analysis of review data",
    "section": "Questions‚Ä¶",
    "text": "Questions‚Ä¶\n\nCan we somehow weight the review with how many reviews it has received?\n\nCan we also include our own prior views on an item and have this influence our interpretation of the reviews?\n\nIs there a way to quantify the likelihood of a bad experience despite a strong review?\n\nDespite sounding wishy-washy, fortunately all of this can be resolved using common-or-garden variety techniques in a branch of statistics called Bayesian Analysis."
  },
  {
    "objectID": "posts/2025-03-05-star-ratings/index.html#example",
    "href": "posts/2025-03-05-star-ratings/index.html#example",
    "title": "Choosing an Airbnb using mathematical analysis of review data",
    "section": "Example",
    "text": "Example\nLet‚Äôs say we are looking at a restaurant with 3 reviews of 4, 5, 5 on a 5-star scale.\nReviews are typically (but not always) just the arithmetic mean of all reviews received.\nWe have a average rating of 4.67 ‚òÖ (3).\nWe can add these up and view them as ‚Äòtotal stars received‚Äô out of ‚Äòpossible available stars‚Äô.\nSo we have 14 stars out of a possible 15. We can convert this to a proportion and call this our ‚Äòsuccess probability‚Äô (14/15 = 0.9333).\nWe can now model this type of process using a mathematical distribution called a Binomial distribution.\n\\[\nY_i \\sim Binomial(n_i, \\theta_i)\n\\] Where \\(Y_i\\) is the number of stars for something, \\(n_i\\) is the total number of stars possible and \\(\\theta_i\\) is success probability.\nThe binomial process gives us a way to model or simulate how many stars would be most likely awarded out of 5 for an restaurant that usually scores (14/15 = 0.9333).\nThe reason we need a complicated mathematical way of doing this is because the expected result changes based on how much data we collect (just like how coin flips aren‚Äôt always 50/50 until we do a large number of flips).\n\n\n\n\n\n\nNote\n\n\n\nActually, the support for a Binomial distribution is (0..n) but star ratings are usually only from 1 - 5 (with no zero-star reviews) so we need to shift our ‚Äòstar count‚Äô to be the number of ‚Äòstars above the minimum amount (1)‚Äô. So instead of our ratings accumulating 14 out of 15 stars, we actually model it as 11 ‚Äòexcess‚Äô stars out of a possible 12. Another way of thinking about it is if we have 3 x one-star reviews. This is the lowest possible result but would look like we have a score of 3 out of 15 (3/15 = 0.2). So we need a way of making this be zero.\n\\[\nY'_i = \\Sigma_{reviews} stars - n_i \\\\  \n\\]\n\\[\nY'_i \\sim Binomial(4n_i, \\theta_i)\n\\] Where \\(n_i\\) is the number of reviews.\n\n\nSo far we are assuming our success rate is fixed at 11 out of 12 (11/12 = 0.9166). But what if the next rating for this restaurant was a one-star. Now we have 4, 5, 5, 1. This gives us 11/16 excess stars = 0.6875. This is quite a drop. Obviously we only have a few ratings so our overall success will swing around wildly as new reviews come in.\nThe key underpinning of Bayesian analysis is to not just plug in a single metric here, but to thoughtfully use a range of plausible options. Without further explanation (it‚Äôs boring), we are going to use another mathematical distribution called a Beta distribution to simulate a range of possible ‚Äòsuccess probabilities‚Äô. This distribution is bounded between zero and one and using the two hyper parameters we can adjust it‚Äôs shape to look like a plausible range of success probabilities.\n\\[\n\\theta_i \\sim Beta(\\alpha, \\beta)\n\\]\n\n\n\n\n\nAn example probability distribution that approximates a range of values around 0.7 with a slight negative skew\n\n\n\n\nIf we simulate lots of draws from this distribution we can then feed them into our binomial model and simulate ratings data under a range of input conditions. Why is this good?\n\nAs we accumulate more ratings information, the spread of this curve gets narrower and we gain more confidence in what we think the ‚Äòtrue‚Äô star success proportion is.\n\nWe can start the process by making up a Beta distribution that reflects our best-guess in lieu of actual ratings. This might be totally uninformative if we have no clue how good a product is, or we might have strong view that it‚Äôs going to be highly rated based on word of mouth or previous products.\n\nThe above point (2) is why we might insist on buying a new product and ignore its poor initial ratings if we think it might be good anyway."
  },
  {
    "objectID": "posts/2025-03-05-star-ratings/index.html#choosing-an-airbnb",
    "href": "posts/2025-03-05-star-ratings/index.html#choosing-an-airbnb",
    "title": "Choosing an Airbnb using mathematical analysis of review data",
    "section": "Choosing an Airbnb",
    "text": "Choosing an Airbnb\nSo back to the Airbnb example from the picture above.\nFirstly, the Monticiano apartment is just rated better than the DEZA apartment. No amount of mathematical trickery can reverse that. There are a few things we can do to try and calculate how much overlap there is in the possible ratings outcomes. But I‚Äôm going to ask a different question:\n\nI don‚Äôt care if it‚Äôs 5 star or 4.72 stars. I just really don‚Äôt want a bad experience (say, 3 rating or less). Given the current ratings (and the inherent uncertainly in the ratings), what is the probability I will have a 3 star or worse stay?\n\nI have written some basic code to calculate that for me below.\n\n# What is the simulated probability that I will have a 3 star night (or worse).\n\nbad_experience_prob &lt;- function(a = 1, b = 1, rating, n_reviews, rating_threshold, n_sim = 10000) {\n    # adjusted star ratings\n    n_stars_gt_min &lt;- rating * n_reviews - n_reviews\n    n_total &lt;- n_reviews * 4\n\n    # update prior\n    alpha &lt;- n_stars_gt_min + a\n    beta &lt;- (n_total - n_stars_gt_min) + b\n\n    # posterior predictions\n    beta_samples &lt;- rbeta(n = n_sim, shape1 = alpha, shape2 = beta)\n    posterior_samples &lt;- rbinom(n = n_sim, size = 4, prob = beta_samples) + 1\n\n    prob_bad &lt;- mean(posterior_samples &lt;= rating_threshold)\n\n    return(prob_bad)\n}\n\n\nThe Monticiano apartment with ‚òÖ 5.0 (3):\n\n\nResults:\n\nBased on 10,000 simulated ratings.\n\nWith a ‚òÖ 5.0 (3) rating, assuming no prior view on the quality, the chance of a three star rating or worse is:\n4.53%\n\n\n\n\nThe DEZA cosy place in Monti with ‚òÖ 4.72 (171):\n\n\nResults:\n\nBased on 10,000 simulated ratings.\n\nWith a ‚òÖ 4.72 (171) rating, assuming no prior view on the quality, the chance of a three star rating or worse is:\n2.62%\n\n\nSo despite having a higher rating, the first apartment has a 4% chance of a bad experience, double that of the other apartment.\nWhy? Well despite having a higher mean success rate, just the 3 ratings introduces lots of uncertainly which helpfully propagates to our simulations.\n\n\n\n\n\nComparing the posterior distribution of both apartments\n\n\n\n\nThis all begs the question (at least for me) if we can visualise this probability surface for all combinations of ratings and review strength.\nBelow we can see that as the average ratings get below 3, its obviously expected to increase the odds of a bad (&lt; 3 star) experience. There is an interesting lip when we look at the curve where review numbers are very small. Let‚Äôs zoom in.\n\n\n\n\n\n\nInteractive Charts\n\n\n\nThe below charts are interactive. Use your mouse to pan and zoom.\n\n\n\n\n\n\nAnything can happen we we only have one or two reviews, but things stabilise very quickly as we approach 10, 20 and 30 reviews.\nWhen we focus on just 4.5 - 5 star rated products, we can locate situations where say a 4.8 star review with 5 reviews would yield the same risk of a bad experience as a 4.6 with 20 reviews etc.\n\n\n\n\n\n\n\n\n\n\nKey Takeaway\n\n\n\nGood decision making probably requires more than just acting on a single metric alone, and some context around the uncertainty of that metric can help with making stronger, empirical decisions.\nIf you are working with review data, you need to have the tools to model and analyse more than just biggest number wins."
  },
  {
    "objectID": "posts/2025-03-05-star-ratings/index.html#further-thoughts",
    "href": "posts/2025-03-05-star-ratings/index.html#further-thoughts",
    "title": "Choosing an Airbnb using mathematical analysis of review data",
    "section": "Further thoughts",
    "text": "Further thoughts\nThere are a few different directions you could keep taking this. There are certainly different modelling approaches.I didn‚Äôt explain much about the selection of a prior distribution, which is very important in Bayesian analysis. Arguably the prior for consumer ratings wouldn‚Äôt share the nice normal convergence properties of a Beta distribution. It‚Äôs probably more bimodal with a tendency to see spikes at 1 star and 5 star. This would require mores sophisticated computational methods that I can‚Äôt be bothered with right now.\nFor more info on the maths behind this type of work you can check out this good accessible and free resource: Probability and Bayesian Modeling\n\nLooking for a data science consultant? Feel free to get in touch here"
  },
  {
    "objectID": "posts/2024-10-04-innerloop/index.html",
    "href": "posts/2024-10-04-innerloop/index.html",
    "title": "Data Science Workflows: Inner Loop vs Outer Loop",
    "section": "",
    "text": "In a previous post I discussed the charming nostalgia of CRISP-DM as a data analysis workflow choice.\nA modern limitation of this model is how it handles the Deployment step. In the olden days this was usually some form of static report to a client. It‚Äôs a loaded topic now with deployment usually having something to do with giving money to AWS/MSFT or a battle to the death with your IT department.\nIn a practical sense this usually means rigging up an endpoint for your analysis like an API, shiny app, dashboard or similar. Managing this process is rapidly becoming it‚Äôs own sub-speciality, fashionably called MLOps.\nA useful way to think about this is in terms of Inner Loop vs Outer Loop thinking. I first observed this (for better or worse) in Microsoft‚Äôs Azure Machine Learning Classical ML Architecture.\n\n\n\nSource: Microsoft MLOps Accelerator. MIT Licence\n\n\nThe Inner Loop (Item 3 in the above diagram) is everything in CRISP-DM except ‚ÄòDeployment‚Äô. It covers the whole data identification, EDA and modelling processes. For lack of a better term I usually call this ‚Äòexperimental‚Äô code. It‚Äôs usually circular, iterative and experimental in nature.\nAt some point you will be happy with an output and want to push it out into the world. This is the Outer Loop (Item 5 in the diagram). It‚Äôs its own process and involves more engineer-y tasks like provisioning infrastructure, testing & deployment (usually using some CI/CD pipeline) versioning your model and monitoring for drift, quality etc.\nI see two big issues with this in practice:\n\nAnalysts do the inner loop, declare victory and have no awareness or interest managing the outer loop. This means the analyst needs to re-run the model on their laptop every month and email the results to Kevin, or some such process. Heaven forbid they live in an area of dense, fast moving buses.\nThe analysts want to more formally deploy their models but don‚Äôt have the skills, support or permission. In the end, everything gets shoehorned into a PowerBI dashboard or some half-way solution.\n\nMy proposed solutions to this in ascending order of complexity are:\n\nStart implementing more robust controls over how you save, version, monitor and document the outputs of your analysis - including models.\n\n\n\n\n\n\n\nMLOps in R\n\n\n\nIf you are an R programmer, the tidymodels team at Posit have recently plugged this gaping hole with the {vetiver} package. https://vetiver.posit.co/\n\n\n\nGet some really light-weight deployment solution to get off the ground like shinyapps.io or a development/sandpit Virtual Machine (in a sensible network location) to throw stuff up on manually and build some goodwill with IT.\nMove to a more commercially palatable ‚Äòwalled-garden‚Äô solution like Posit Connect. This strikes a nice balance between being licensed, managed software and giving analysts autonomy in what and how they deploy.\nUtilize your existing cloud providers ML Ops solutions to automate the build and deployment from your version control system - just be careful, this is a slippery slope in terms of complexity and you don‚Äôt need to go all-in. This typically keeps IT happy in a heavily regulated environment due to the native integration of network/cloud security protocols.\n\n\nLooking for a data science consultant? Feel free to get in touch at wavedatalabs.com.au"
  },
  {
    "objectID": "consulting/2024-01-01-service-5/index.html#footnotes",
    "href": "consulting/2024-01-01-service-5/index.html#footnotes",
    "title": "Customer segmentation and recommender system build for e-commerce company",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nName and industry changed/withheld to respect client privacy.‚Ü©Ô∏é\nExact methods used are confidential and cannot be disclosed‚Ü©Ô∏é"
  },
  {
    "objectID": "consulting/index.html",
    "href": "consulting/index.html",
    "title": "Dean Marchiori",
    "section": "",
    "text": "I‚Äôm a specialist independent data science consultant helping science & technology teams innovate innovate faster with data and maths. Not hype. I work with clients on complex problems across applied mathematics, statistical modelling, data analysis & machine learning.\nI value long term trust and flexibility over billable hours. Unlike every other technical consultant, I offer a low-cost subscription based model for my fees.\n\n\n Statistical Modelling and Analysis\n Expert R Programming\n Machine Learning\n\n Responsible AI Advisory\n Career Mentoring & Coaching\n Data Science Training\n\n\n\n\n\n\n Get Started"
  },
  {
    "objectID": "consulting/index.html#data-science-statistical-consulting",
    "href": "consulting/index.html#data-science-statistical-consulting",
    "title": "Dean Marchiori",
    "section": "",
    "text": "I‚Äôm a specialist independent data science consultant helping science & technology teams innovate innovate faster with data and maths. Not hype. I work with clients on complex problems across applied mathematics, statistical modelling, data analysis & machine learning.\nI value long term trust and flexibility over billable hours. Unlike every other technical consultant, I offer a low-cost subscription based model for my fees.\n\n\n Statistical Modelling and Analysis\n Expert R Programming\n Machine Learning\n\n Responsible AI Advisory\n Career Mentoring & Coaching\n Data Science Training\n\n\n\n\n\n\n Get Started"
  },
  {
    "objectID": "consulting/index.html#trusted-by",
    "href": "consulting/index.html#trusted-by",
    "title": "Dean Marchiori",
    "section": "Trusted by",
    "text": "Trusted by\n\n\n  \n    \n      \n    \n    \n      \n        \"Dean and Wave Data Labs brought a rare combination of high-level technical skill, with seamless collaboration and flexibility. They were able to quickly identify and scope critical improvements to our data model and machine learning algorithms. As well as producing high-quality technical outputs, Wave Data Labs worked in a way that uplifted the capability of our internal team, delivering lasting improvements to our ways of working.\"\n      \n      Tom O‚ÄôConnor, CEO AptoNow\n    \n  \n\n\n  \n    \n      \n    \n    \n      \n        \"You were able to work toward our goals, making suggestions to improve the workflow and then execute those suggestions in the code. You were friendly, thoughtful and hard working. We're really happy with the package you delivered. Thanks!\"\n      \n      Dr Melinda Rostal, Principal Scientist, EcoHealth Alliance\n    \n  \n\n\n  \n    \n      \n    \n    \n      \n        \"Dean's ability to unpack problems, package, engage, present, influence, validate, mentor and resolve is fantastic and highly sought after.\"\n      \n      Paul Oyston, Group Head of IT Data Analytics, IRT Group"
  },
  {
    "objectID": "consulting/index.html#how-i-have-helped-other-clients",
    "href": "consulting/index.html#how-i-have-helped-other-clients",
    "title": "Dean Marchiori",
    "section": "How I have helped other clients",
    "text": "How I have helped other clients"
  },
  {
    "objectID": "consulting/2025-02-17-service-apto/index.html",
    "href": "consulting/2025-02-17-service-apto/index.html",
    "title": "Case Study: Innovating Faster with Data Science",
    "section": "",
    "text": "AptoNow is an innovative technology business with a mission to be the world‚Äôs leading co-pilot for higher education institutions. To support and accelerate their product development priorities, AptoNow engaged Wave Data Labs, a data science consulting practice that helps science and technology teams innovate faster and scale their data science capability.\nIn just 3 months this engagement helped the team at AptoNow:"
  },
  {
    "objectID": "consulting/2025-02-17-service-apto/index.html#aptonows-challenges-and-key-objectives",
    "href": "consulting/2025-02-17-service-apto/index.html#aptonows-challenges-and-key-objectives",
    "title": "Case Study: Innovating Faster with Data Science",
    "section": "AptoNow‚Äôs challenges and key objectives",
    "text": "AptoNow‚Äôs challenges and key objectives\nAptoNow empowers their clients to deliver exceptional student experiences through advanced data and AI-assisted technology to solve complex challenges around timetable optimisation, student engagement, space optimisation and auto-scheduling. The team were at an inflection point where they wanted to continue scaling both their client servicing projects and the continued development of their core data products. In the face of a growing and complex code-base, the AptoNow team wanted to maintain client focus and needed technical solutions that would help them realise immediate value without the cost, overhead and risk of hiring additional full-time data scientists until they were ready.\nAptoNow‚Äôs goals with this engagement were to:\n\nFind a technical expert in mathematical modelling, R programming and their preferred tech stack.\n\nFind a consultant who understands not only the technical but also the commercial and business objectives.\n\nEnable value quickly and affordably so the core team could focus working with their clients to ensure they were always solving the right problems.\n\n\n‚ÄúDean and Wave Data Labs brought a rare combination of high-level technical skill, with seamless collaboration and flexibility‚Äù says AptoNow CEO, Tom O‚ÄôConnor."
  },
  {
    "objectID": "consulting/2025-02-17-service-apto/index.html#wave-data-labs-approach-and-methodology",
    "href": "consulting/2025-02-17-service-apto/index.html#wave-data-labs-approach-and-methodology",
    "title": "Case Study: Innovating Faster with Data Science",
    "section": "Wave Data Lab‚Äôs approach and methodology",
    "text": "Wave Data Lab‚Äôs approach and methodology\nThe engagement with Dean and Wave Data Labs offered a low risk, high reward approach with a two-stage implementation:\nStage 1: One day on-site rapid review and diagnostic report: This phase focused on three technical elements: Speed, Scalability and Stability. The outputs were 7 clear, technical recommendations that were achievable across three ‚Äòhorizons‚Äô that aligned to the value proposition AptoNow had committed to delivering to their clients.\nStep 2: Hands-on delivery of high-value improvements: Following this Wave Data Labs partnered closely with the engineering and product team at AptoNow to do hands-on work to implement these ideas and make them a reality. From a technical perspective this covered:\n\nReproducibility and code dependency management\n\nImplementing data pipeline tools\n\nAlgorithm review and re-factoring tech debt\n\nWorking flexibly and pivoting to solve critical client needs when required."
  },
  {
    "objectID": "consulting/2025-02-17-service-apto/index.html#results-and-impact",
    "href": "consulting/2025-02-17-service-apto/index.html#results-and-impact",
    "title": "Case Study: Innovating Faster with Data Science",
    "section": "Results and impact",
    "text": "Results and impact\nOver three months, Wave Data Labs was able to partner with AptoNow to deliver several tangible benefits:\n\nThe team is now able to review and test new client features in minutes rather than hours.\n\nAt all times there is complete assurance and stability of the core product performance.\n\nThe code base and its orchestration are simpler and in line with industry best practice, setting the team up for scaling and hiring additional developers to support their clients growth.\n\n\n  \n    \n      \n    \n    \n      \n        \"Dean and Wave Data Labs brought a rare combination of high-level technical skill, with seamless collaboration and flexibility. They were able to quickly identify and scope critical improvements to our data model and machine learning algorithms. As well as producing high-quality technical outputs, Wave Data Labs worked in a way that uplifted the capability of our internal team, delivering lasting improvements to our ways of working.\"\n      \n      Tom O‚ÄôConnor, CEO AptoNow"
  },
  {
    "objectID": "consulting/2024-01-01-service-1/index.html",
    "href": "consulting/2024-01-01-service-1/index.html",
    "title": "Australian Bushfire Risk Analysis & Mapping",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nFull source code can be viewed on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-1/index.html#project",
    "href": "consulting/2024-01-01-service-1/index.html#project",
    "title": "Australian Bushfire Risk Analysis & Mapping",
    "section": "",
    "text": "This project was completed with 360info. 360info is an independent nonprofit public information service with headquarters in Melbourne hosted at Monash University. Monash is Australia‚Äôs largest and most globally connected university with campuses around the world.\nFull source code can be viewed on Github"
  },
  {
    "objectID": "consulting/2024-01-01-service-1/index.html#outcomes",
    "href": "consulting/2024-01-01-service-1/index.html#outcomes",
    "title": "Australian Bushfire Risk Analysis & Mapping",
    "section": "Outcomes",
    "text": "Outcomes\n360info has collated data from New South Wales (below), and all other states and territories which shows which suburbs are most prone to bushfire. This interactive analysis was the first of its kind to highlight bushfire planning risk for the entire Australian mainland.\nTo reference this analysis see here."
  }
]