{
  "hash": "39550723d51af9886610d2748861c51d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Survival Analysis\"  \nsubtitle: \"Pearls and pitfalls for time-to-event modelling\"\ndraft: false\nauthor: \"Dean Marchiori\"\ndate: \"2025-02-20\"\ncategories: [survival, analysis]\nimage: \"featured.png\"\neditor_options: \n  chunk_output_type: console\nexecute: \n  echo: false\n  warning: false \n  message: false\n---  \n\n::: {.cell}\n\n:::\n\n\n\n\n## tl;dr  \n\nThe use of classification algorithms to model binary response data are so ubiquitous there is a risk that in some settings this type of model is inappropriate. In cases when the response data occur over time and subjects may drop out (due to death, loss to follow up etc), traditional methods like logistic regression will not result in accurate estimations. Time to event models like survival analysis may provide a more appropriate way to address this type of data generating system.\n\nThe below are some brief notes and resources on typical workflows for time-to-event or survival \nmodelling. \n\n## Types of Survival Models\n\nThere are several categories of survival models:\n\n**1. Non-Parametric:** Most commonly the *Kaplan-Meier estimator* where no assumptions are imposed on either the shape of the survival function nor on the form of the relationship between predictor variables and survival time.     \n\n**2. Semi-Parametric:** The standard approach here is the *Cox Proportional Hazards Model (PH Model)*. The hazard function gives the instantaneous potential of having an event at a time, given survival up to that time. No distributional assumptions are made about the survival times or baseline hazard function. This effectively results in a model that scales a 'baseline' hazard function that is never actually estimated, hence the model being a 'proportional hazards' model and using only a partial-maximum likelihood estimation method.  \n\n**3. Parametric:** Fully parametric models exist and rely on the analyst specifying the functional form of the survival curve (exponential, Weibull, log-normal etc.).  \n\n**4. Others:** Other specialised methods exists such as AFT models exist and can be selected\nif there is significant justification or if the more traditional methods fail.  \n\n\n## Workflow Notes \n\n- If working with tabular binary outcome data for a specific event, start by defining:\n  - The observed event time of the event: $T_i$  \n  - The censoring time (when the subject was last followed up or known to have dropped out, or the end of the observation period): $C_i$  \n  \n- The event time can be calculated as $Y_i = min(T_i, C_i)$ with a binary event indicator ($\\delta_i$): \n\n$$\n\\begin{equation}\n \\delta_i =\n   \\left\\{\\begin{array}{lr}\n       1, & T_i \\leq C_i \\\\\n       0 & T_i > C_i\n    \\end{array}\\right.\n \\end{equation}\n$$\n\n- A recommended workflow is to start by using a non-parametric approach such as the\nKaplan-Meier estimator as a descriptive tool. \n\n- For between group comparisons, a group-wise KM-curve can be estimated. \n\n- Formal hypothesis testing between groups can be done using a *Log-rank test* as the typical standard.  \n\n- A *Cox proportional hazards model* is a safe choice to enable the inclusion\nof multiple covariates of interest. It will not measure the underlying baseline hazard but has good\nproperties for interpreting and testing hazard ratios from the model's coefficients (essentially\nan analogy to odds ratios in logistic regression).  \n\n- The Cox PH model has a number of assumptions: \n  - **Proportional Hazards:** Across time the relative hazard remains constant across covariate levels. This\nhypothesis can be tested using the scaled Schoenfeld residuals and should graphically be assessed to ensure there is no slope. A formal\ntest can also be conducted using the `cox.zph` function in the `{survival}` package in R.  \n  - **Non-Informative Censoring:**  While this class of model is designed to handle censored data, the cause \nof censoring should be unrelated to the response and be entirely non-informative.    \n  - **Residual diagnostics** can also be performed as the primary way to gauge influential observations, \n possible transformations and issues with non-proportional hazards. They can also be used to test for linearity and additivity in the covariates. There are several types of residuals\n available (Martingale, Deviance, Score etc.). See [here](https://doi.org/10.1038/sj.bjc.6601120) for guidance. \n\n- Time dependent covariates (covariates that change after baseline) can also be incorporated but \nrequire specialised data re-shaping. See this [vignette](https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf) for a detailed overview first. \n\n- If the diagnostic tests indicate a poor fit there are several remedies (see [here](https://doi.org/10.1038/sj.bjc.6601120)), or an alternative model should be tried. If\nthis is from class of parametric survival models, some knowledge and testing of the \ndistribution will be required as an explicit input.    \n\n- Results are typically presented through interpretation of the regression coefficients. The raw coefficients\nare exponentiated to form a 'hazard ratio' which represents the percentage increase(decrease) over(under) the null value of 1 in \nthe instantaneous hazard.   \n\n- Subject matter experts are more comfortable interpreting increasing values that are associated with poorer survival experience, rather than protective effects. \n\n\nBelow is a basic flow chart for a standard survival analysis: \n\n\n\n\n```{mermaid}\nflowchart TD\n  A[(survival data)] --> B[Kaplan-Meier Estimator]\n  B --> C[Log Rank Test]  \n  C --> D[Cox Proportional Hazards Model]\n  C -.-> E[Fully parametric model]  \n  C -.-> F[AFT, other exotic..]\n  D --> G[Diagnostics]\n  G --> D\n  \n  \n  style D fill:#f9f\n```\n\n\n\n\n\n## Introductory tutorials  \n\n[Survival Analysis in R](https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html)  \n[An Introduction to Survival Analysis in R with Emily Zabor](https://www.youtube.com/watch?v=VenrZGkDJcU)  \n\n## More in-depth tutorials   \n\nClark, T., Bradburn, M., Love, S. et al. Survival Analysis Part I: Basic concepts and first analyses. Br J Cancer 89, 232–238 (2003). [https://doi.org/10.1038/sj.bjc.6601118](https://doi.org/10.1038/sj.bjc.6601118)\n\nBradburn, M., Clark, T., Love, S. et al. Survival Analysis Part II: Multivariate data analysis – an introduction to concepts and methods. Br J Cancer 89, 431–436 (2003). [https://doi.org/10.1038/sj.bjc.6601119](https://doi.org/10.1038/sj.bjc.6601119)\n\nBradburn, M., Clark, T., Love, S. et al. Survival Analysis Part III: Multivariate data analysis – choosing a model and assessing its adequacy and fit. Br J Cancer 89, 605–611 (2003). [https://doi.org/10.1038/sj.bjc.6601120](https://doi.org/10.1038/sj.bjc.6601120)\n\nClark, T., Bradburn, M., Love, S. et al. Survival Analysis Part IV: Further concepts and methods in survival analysis. Br J Cancer 89, 781–786 (2003). [https://doi.org/10.1038/sj.bjc.6601117](https://doi.org/10.1038/sj.bjc.6601117  )\n\n\n## Text books  \n\nHarrell, F. E. (2001). Regression modeling strategies: with applications to linear models, logistic regression, and survival analysis (Vol. 608). New York: springer.\n\nHosmer Jr, D. W., Lemeshow, S., & May, S. (2008). Applied survival analysis: regression modeling of time-to-event data (Vol. 618). John Wiley & Sons.\n\n## Software  \n\n[survival R package](https://cran.r-project.org/web/packages/survival/index.html)\n\n[ggsurvfit: Flexible Time-to-Event Figures](https://cran.r-project.org/web/packages/ggsurvfit/index.html)\n\n> **Looking for a data science consultant? Feel free to [get in touch here](https://www.deanmarchiori.com)**",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}