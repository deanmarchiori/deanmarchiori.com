{
  "hash": "21995859a4c248d456ba4e74494771c3",
  "result": {
    "markdown": "---\ntitle: Prediction Intervals for Linear Mixed Effects Models\nauthor: Dean Marchiori\ndate: '2023-02-06'\ncategories: [R, analysis]\nimage: \"featured.png\"\n---\n\n\nA recent project with repeated measures data involved fitting a random\nintercept term, and eventually making predictions for new groups not in the training\nsample. Importantly there was a need for individual predictions rather than\npopulation mean level predictions. Now, you obviously cannot include\nthe random effect for a level that is not in your data, so the idea was to make\na population level prediction with an adequate prediction interval that reflected \nthe variation from both the fixed and random effects. This is complicated. \n\nIn the help page for `lme4::predict.merMod()` is the following note:\n\n> + There is no option for computing standard errors of predictions because it is difficult to define an efficient method that incorporates uncertainty in the variance parameters; we recommend bootMer for this task.\n  \nThere are some useful resources out there but it took a while to track down, so \nthis post may serve as a good reference in the future.  \n\nLet's go through an example using the famous `sleepstudy` data showing the \naverage reaction time per day (in milliseconds) for subjects in a sleep deprivation study.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lme4)\nlibrary(tidyverse)\n\ndata(\"sleepstudy\")\n```\n:::\n\n\n## Linear Model  \n\nWe would like to model the relationship between `Reaction` and `Days`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nFitting a basic linear model:  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_lm <- lm(Reaction ~ Days, data = sleepstudy)\n\nsummary(fit_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Reaction ~ Days, data = sleepstudy)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-110.848  -27.483    1.546   26.142  139.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  251.405      6.610  38.033  < 2e-16 ***\nDays          10.467      1.238   8.454 9.89e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 47.71 on 178 degrees of freedom\nMultiple R-squared:  0.2865,\tAdjusted R-squared:  0.2825 \nF-statistic: 71.46 on 1 and 178 DF,  p-value: 9.894e-15\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(Days, Reaction)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nBut this ignores the fact these data are not independent. We have multiple observation\nper subject. Some look like a good fit, others not. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sleepstudy, aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  facet_wrap(~Subject) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## Linear Mixed Effects Model  \n\nLet's add a random intercept term for `Subject`. For simplicity we will leave out any other\nrandom effects. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lme4::lmer(Reaction ~ Days + (1|Subject), data = sleepstudy)\n\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (1 | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1786.5\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.2257 -0.5529  0.0109  0.5188  4.2506 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Subject  (Intercept) 1378.2   37.12   \n Residual              960.5   30.99   \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 251.4051     9.7467   25.79\nDays         10.4673     0.8042   13.02\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.371\n```\n:::\n:::\n\n\nNew fitted lines can be drawn, showing the adjusted intercept for each subject \n(original regression line kept for reference). \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsleepstudy |> \n  mutate(pred = predict(fit, re.form = NULL)) |> \n  ggplot(aes(Days, Reaction, col = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1], col = \"grey\") +\n  geom_line(aes(Days, pred), show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## Bootstrapped Prediction Intervals (observed data)  \n\nLet's try and generate prediction intervals using `lme4::bootMer()` as suggested.  \n\nFirst on the in-sample data.  \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# predict function for bootstrapping\npredfn <- function(.) {\n  predict(., newdata=new, re.form=NULL)\n}\n\n# summarise output of bootstrapping\nsumBoot <- function(merBoot) {\n  return(\n    data.frame(fit = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.5, na.rm=TRUE))),\n               lwr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.025, na.rm=TRUE))),\n               upr = apply(merBoot$t, 2, function(x) as.numeric(quantile(x, probs=.975, na.rm=TRUE)))\n    )\n  )\n}\n\n# 'new' data\nnew <- sleepstudy\n```\n:::\n\n\nNotes:\n\n1. In the `predict()` function we specify `re.form=NULL` which identifies which random effects\nto condition on. Here `NULL` includes all random effects. Obviously here you can compute\nindividual predictions assuming you feed it with the correct grouping level in your data.   \n\n2. In the `lme4::bootMer()` function we set `use.u=TRUE`. This conditions on the \nrandom effects and only provides uncertainly estimates for the i.i.d. errors resulting\nfrom the fixed effects of the model.  \n\n> If use.u is TRUE and type==\"parametric\", only the i.i.d. errors are resampled, with the values of u staying fixed at their estimated values.     \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot <- lme4::bootMer(fit, predfn, nsim=250, use.u=TRUE, type=\"parametric\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew |> \n  bind_cols(sumBoot(boot)) |> \n  ggplot(aes(Days, Reaction, col = Subject, fill = Subject)) +\n  geom_point(show.legend = FALSE) +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  facet_wrap(~Subject) +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n## Dealing with unobserved data  \n\nHowever, this gets complicated if we want to make predictions for *new* subjects.  \n\nWe can no longer condition on the random effects, as the new subject level will\nnot have a fitted random intercept value. Instead we need to effectively make\na population level prediction (i.e. set the random effect to zero.). This makes\nsense as we don't know what the random effect ought to be for a given, unobserved subject. \n\nBut we don't want the prediction interval to just cover the uncertainty in the population level estimate. \nIf we are interested in individual predictions, how can we incorporate \nthe uncertainly of the random effects in the prediction intervals?\n\nLets generate a new, unobserved subject. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_subject <- tibble(\n  Days = 0:9,\n  Subject = factor(\"999\")\n  )\n```\n:::\n\n\nWe provide a new predict function that doesn't condition on the random effects \nby using `re.form = ~0`. This lets us input and obtain predictions for new subjects.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredfn <- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_subject |> \n  bind_cols(predicted = predfn(fit)) |> \n  ggplot(aes(Days, predicted, col = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nHowever using `predict` just results in a completely deterministic prediction as shown above. \n\nAn alternative approach is to use `lme4::simulate()` which will simulate\nresponses for subjects non-deterministically using the fitted model object. \n\nBelow we can see a comparison on both approaches. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredfn <- function(.) {\n  predict(., newdata=new_subject, re.form=~0, allow.new.levels=TRUE)\n}\n\nsfun <- function(.) {\n    simulate(., newdata=new_subject, re.form=NULL, allow.new.levels=TRUE)[[1]]\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_subject |> \n  bind_cols(simulated = sfun(fit)) |> \n  bind_cols(predicted = predfn(fit)) |> \n  pivot_longer(cols = c(3, 4), names_to = \"type\", values_to = \"val\") |> \n  ggplot(aes(Days, val, col = type)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  theme_bw() +\n  ylim(c(150, 450))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can use this `simulate()` function in our bootstrapping to resample responses from \nthe fitted model (rather than resampling deterministic population predictions). \n\nThis time we set `use.u=FALSE` to provide uncertainly estimates from both the \nmodel errors and the random effects. \n\n> If use.u is FALSE and type is \"parametric\", each simulation generates new values \nof both the “spherical” random effects uu and the i.i.d. errors \\epsilonϵ, \nusing rnorm() with parameters corresponding to the fitted model x.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot <- lme4::bootMer(fit, sfun, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_subject |> \n  bind_cols(sumBoot(boot)) |> \n  bind_cols(predicted = predfn(fit)) |> \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nSo while we don't have a conditional mode of the random effect (because its a \nnew subject) we can derive a bootstrapped estimate of the prediction interval by \nresampling the random effects and model errors on simulated data values. \n\n## Aside  \n\nFor comparison, here is what the same prediction interval would look like if \nwe just used an unconditional population prediction. While the overall gist \nis the same, despite also resampling both the random effects and the i.i.d. errors, \nthe interval is narrower as it is resampling just the deterministic population predictions of the model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot <- lme4::bootMer(fit, predfn, nsim=250, use.u=FALSE, type=\"parametric\", seed = 100)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_subject |> \n  bind_cols(sumBoot(boot)) |> \n  bind_cols(predicted = predfn(fit)) |> \n  ggplot(aes(Days, predicted, col = Subject, fill = Subject)) +\n  geom_point() +\n  geom_abline(slope = fit_lm$coefficients[2], intercept = fit_lm$coefficients[1]) +\n  geom_line(aes(Days, fit), show.legend = FALSE) +\n  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.3, show.legend = FALSE) +\n  theme_bw() +\n  ylim(c(150, 450))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n## References  \n\nMost of the material and code is taken from a variety of sources below. In particular\nthe lme4 github issue. Also, the `merTools` package has a nice vignette comparing\nthese methods with their own solution. \n\nhttps://tmalsburg.github.io/predict-vs-simulate.html\nhttps://github.com/lme4/lme4/issues/388\nhttps://cran.r-project.org/web/packages/merTools/vignettes/Using_predictInterval.html\nhttp://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#predictions-andor-confidence-or-prediction-intervals-on-predictions",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}