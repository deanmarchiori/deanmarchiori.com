{
  "hash": "133a838ff7351ddae4125ed835ae2782",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Level up your Forecasting\"  \nsubtitle: \"Quantify uncertainty with distributional forecasts\"\nauthor: \"Dean Marchiori\"\ndate: \"2023-05-29\"\ncategories: [R, statistics, time series]\nimage: \"featured.png\"\neditor_options: \n  chunk_output_type: console\nexecute: \n  echo: false\n  warning: false \n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\n\n\nThis is a time series data set of half-hourly electricity demand for Victoria, Australia [^1]. You \nmay not care about electricity forecasting, but there is probably some similar time series\nin your organisation you do care about.  \n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n**What is the forecast demand for the next week?** \n\nWho knows? I certainly don't know, despite doing forecasting for a job.  \n\nIn fact, anyone who tells you they do know (with certainty) is wrong. \n\n\n## But I need to make a forecast, so what do I do?  \n\nOften you have to come up with something. Finance needs an estimate of sales or budgets, or\nyour manager needs something to take to the board. So you have to churn out some numbers. But what numbers?   \n\nHere are two extremes: \n\n1. You could ignore all historical data and just use the most recent data point as the most reliable forecast.  \n2. You could ignore any specific value and just take the average of everything. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nBoth are in the ballpark but they aren't very squiggly like the real data. They clearly\naren't capturing the *seasonality*. \n\nYou could be lazy (clever) and just use the same time period from *last* week as your current\nforecast. It actually looks really good. But it's just one, fixed realization of what could happen. Will it play out exactly \nlike your forecast? *Exactly* like last week? Almost certainly not.  \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\nLet's simulate another way history could play out. (By statistically resampling \nthe residuals in the training data)\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nAnd another\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nAnd 20 more: \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\nSo instead of blindly relying on one (kind of okay looking, but totally unrealistic) forecast, \nwe now have a 'fuzzy' region of plausible values. \n\n## Ok but why bother?  \n\nIf you don't know with certainty what your forecast is going to be, \nthen don't just give one concrete number. It's misleading.\n\n> Its more important to know how uncertain your forecast is rather than what your forecast is. \n\nYou can still pull out a mean or point estimate but by delivering the whole story you are\nconveying not just what you think is likely to happen, but how certain you are about it.   \n\nOften this can lead to more meaningful discussions. Perhaps it's not the mean of your\nforecast distribution that you care about, its the extreme values.\nFor example, If I was stress testing business cash flows and forecasting the cost\nof a maintenance activity, I'd be more interested in forecasting the 95% upper limit of forecast \ncosts rather than the 'expected' cost.\n\nIn practice, you often don't have to do any mathematical simulations. Proper time\nseries forecasts have methods to calculate prediction intervals out of the box (see below). \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n## Spoiler \n\nWant to know what actually happened? Here it is in red. \n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\nA huge criticism of providing probabilistic forecasts is that is seems like you\nare 'hedging your bets' and being evasive. The reality is, in our electricity\nexample, it was just very hot and demand surged (much higher than even our 95% prediction interval). \nSo if it's realistic and plausible to see forecast values in these ranges (or even more extreme) - Why \nwouldn't you want to include that information in your forecast??\n\n## Aside  \n\nIn practice a statistician will likely use a more sophisticated model than presented here. These models may take into account temperature and other factors but there will still be unexplained variance that\nwill need to be quantified if you want a quality forecast produced. \n\n**If you or your organisation want to get serious about making proper forecasts and \nbeing proactive when making critical decisions - [drop me a line](https://www.wavedatalabs.com.au/contact/), I can help.**  \n\n## References \n\n[^1]: Source: Australian Energy Market Operator. *tsibbledata* R package. \n\nO'Hara-Wild M, Hyndman R, Wang E, Godahewa R (2022). _tsibbledata:\n  Diverse Datasets for 'tsibble'_. https://tsibbledata.tidyverts.org/,\n  https://github.com/tidyverts/tsibbledata/.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}