{
  "hash": "e61f9b56a67db750f3892cc48a22a882",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Real-time Computer Vision AI\"  \nsubtitle: \"Getting started with YOLOv5 object detection\"\nauthor: \"Dean Marchiori\"\ndate: \"2025-06-18\"\ncategories: [python, deeplearning]\nimage: \"img/featured.png\"\neditor_options: \n  chunk_output_type: console\nexecute: \n  echo: false\n  warning: false \n  message: false\n---  \n\n::: {.cell}\n\n:::\n\n\n\n> This post just serves as a basic notes write up of getting started with [YOLO](https://en.wikipedia.org/wiki/You_Only_Look_Once) for object detection.    \n\nYOLO (You Only Look Once)[^1] is a real-time computer vision object detection system that uses a single neural network to predict bounding boxes and class probabilities directly from full images in one evaluation.\n\nThere are many versions of YOLO from v1 up to (at time of writing) v11. I have chosen to use [YOLOv5](https://github.com/ultralytics/yolov5)[^2] which is maintained by Ultralytics. The v5 model is popular as it's highly performant, very easy to use and is built on the popular [PyTorch](https://pytorch.org/) framework. \n\n## Setup  \n\nI am using a laptop with the following specs: \n\n**Memory**: 96GB  \n**Processor**: Intel Core i9-14900HX x 32  \n**Graphics**: NVIDIA GeForce RTX 4050 GPU   \n**OS**:  Ubuntu 24.04.2 LTS  \n\n\n\n\n\n### Dependencies  \n\n- Python >= 3.80   [Get Started](https://www.python.org/)\n- PyTorch >= 1.8   [Get Started](https://pytorch.org/get-started/locally/)\n\n### Install  \n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n# Clone the YOLOv5 repository\ngit clone https://github.com/ultralytics/yolov5\n\n# Navigate to the cloned directory\ncd yolov5\n\n# Install required packages\npip install -r requirements.txt\n```\n:::\n\n\n\n## Prediction \n\nHere we will demonstrate prediction (inference) only using a pre-trained model. \n\nThe model in this example is [yolov5su.pt](https://docs.ultralytics.com/models/yolov5/#performance-metrics) which is a pre-trained PyTorch model for Object Detection tasks. This was trained on the [COCO (Common Objects in Context) dataset](https://docs.ultralytics.com/datasets/detect/coco/). COCO is a large-scale object detection, segmentation, and captioning dataset. It is designed to encourage research on a wide variety of object categories and is commonly used for benchmarking computer vision models.  \n\n### Image Object Detection \n\nHere is an example of object detection on a static `jpg` image.  \n\n\n\n::: {.cell python.reticulate='false'}\n\n```{.python .cell-code}\nfrom ultralytics import YOLO\n\n# load pre-trained model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# download sample image or provide file path to your own\nimg = \"https://ultralytics.com/images/zidane.jpg\" \n\n# Perform inference\nresults = model(img, save=True)\n```\n:::\n\n\n\n![](img/zidane.jpg)\n\n\n### Video\n\nYOLOv5 can also perform inference on video files. Here is a short clip I shot on my phone. \n\n\n\n::: {.cell python.reticulate='false'}\n\n```{.python .cell-code}\nfrom ultralytics import YOLO\n\n# Load a pre-trained YOLOv5 model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# Define path to video file\nsource = \"input/street.mp4\"\n\n# Run inference on the source\nresults = model(source, save=True) \n```\n:::\n\n\n\n![](img/street.webm)\n\n### Webcam streaming  \n\nA subtle variation is to use your webcam as a source and perform live object-detection. Under default settings this will buffer the entire stream into RAM then write it to an .avi file. This can be adjusted to purely streaming the current framer in a live viewer window, which is fun. \n\n\n\n::: {.cell python.reticulate='false'}\n\n```{.python .cell-code}\nimport cv2\nfrom ultralytics import YOLO\n\n# Load the YOLO model\nmodel = YOLO(\"yolov5/yolov5su.pt\")\n\n# Open the video file. Not the path as '0' to indicate the webcam source\ncap = cv2.VideoCapture(0)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n    if success:\n        # Run YOLO inference on the frame\n        results = model(frame)\n\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n\n        # Display the annotated frame\n        cv2.imshow(\"YOLO Inference\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n```\n:::\n\n\n\n![](img/webcam.webm)\n\n\n\n\n[^1]: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)  \n\n[^2]: Jocher, G. (2020). Ultralytics YOLOv5 (Version 7.0) [Computer software]. AGPL-3.0 License. https://github.com/ultralytics/yolov5. https://doi.org/10.5281/zenodo.3908559  \n\n\n> **Looking for a data science consultant? Feel free to [get in touch here](https://www.deanmarchiori.com)**\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}