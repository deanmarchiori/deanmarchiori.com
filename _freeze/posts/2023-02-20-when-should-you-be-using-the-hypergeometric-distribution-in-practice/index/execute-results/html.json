{
  "hash": "28e8be487c9bcfe21a36202b0c2157af",
  "result": {
    "markdown": "---\ntitle: When should you be using the Hypergeometric distribution in practice?\nauthor: Dean Marchiori\ndate: '2023-02-20'\ncategories: [R, analysis]\nimage: \"featured.png\"\n---\n\n\n\n\n\n\nWe have a manufacturing process in the day job that is subject to sample\nauditing. \n\nThere are $N$ widgets produced and we need to audit $n$ of them. Some sort of\nrejection threshold is needed on that sample to decide if the whole batch of widgets\nhas met a specified quality level. \n\nTypically, a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) would be appropriate for measuring the probability of $k$ successes (in this case defects found) in $n$ independent trials with probability $p$.   \n\n$$\nPr(X=k) = {n \\choose k} p^k(1-p)^{n-k}\n$$\n\nThe word *independent* is doing a lot of work here as it implies that we are sampling\n*with* replacement in order to maintain a fixed probability parameter $p$.  \n\nIn cases where you are taking draws from a population *without* replacement (such\nas when you do destructive inspections on a widget) the underlying population\nchanges with each draw and so does the probability $p$.   \n\nIn this case, modelling the process using a [hypergeometric distribution](https://en.wikipedia.org/wiki/Hypergeometric_distribution) may be a more appropriate choice.  \n\n$$\nPr(X=k) = \\frac{{K \\choose k}{N-K \\choose n-k}}{{N \\choose n}}\n$$\n\nIt similarly describes the probability of $k$ successes in $n$ draws without replacement. However, instead of specifying a parameter $p$, we provide the population size $N$, which contains $K$ success states in the population. \n\n## Example  \n\nLet's say we have 2000 widgets manufactured and we want to sample 50 (ignore why 50, \nthat is a whole separate question). We have an assumed quality level of 10% defective units \n(which we define as 'success' for complicated reasons).\n\nQ: Based on a sample of 50 widgets how many defective units would be considered unlikely (95% CI) to occur randomly  given our assumed quality level, and therefore result in us rejecting the \nentire batch? \n\nWe can compare the binomial probability mass function with the hypergeometric \nand observe they are essentially the same. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       ) |> \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |> \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nHowever, if we had a smaller population of say 100 or 70 widgets, how would this compare? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(\n       x =  seq.int(0, 50, by = 1),\n       binomial = dbinom(x, size = 50, prob = 0.1),\n       hypergeom_2000 = dhyper(x, m = 200, n = 1800, k = 50),\n       hypergeom_100 = dhyper(x, m = 10, n = 90, k = 50),\n       hypergeom_070 = dhyper(x, m = 7, n = 63, k = 50)\n       ) |> \n  pivot_longer(cols = -1, names_to = 'distribution', values_to = 'density') |> \n  ggplot(aes(x, density, col = distribution)) +\n  geom_line() +\n  geom_point() +\n  xlim(c(0, 20)) +\n  theme_bw() +\n  labs(x = \"Observed defective units in sample\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nWe can see these curves are markedly different. And indeed the 95% confidence\nintervals obtained are narrower for the hypergeometric case. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbinom(p = c(0.025, 0.975), size = 50, prob = 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 9\n```\n:::\n\n```{.r .cell-code}\nqhyper(p = c(0.025, 0.975), m = 10, n = 90, k = 50)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2 8\n```\n:::\n:::\n\nWe can see from a random draw of 1 million samples from each PMF that they both \nhave the same expected values, but the variance is smaller in the hypergeometric\ncase.   \n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- rbinom(n = 1e6, size = 50, prob = 0.1)\nY <- rhyper(nn = 1e6, m = 10, n = 90, k = 50)\n\nmean(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.003297\n```\n:::\n\n```{.r .cell-code}\nvar(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.503315\n```\n:::\n\n```{.r .cell-code}\nmean(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.000162\n```\n:::\n\n```{.r .cell-code}\nvar(Y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.27195\n```\n:::\n:::\n\n\n\n## Does it matter which one you use?  \n\nAs a consequence of removing samples in each draw we influence the probability\nof a subsequent success. If our $N$ and $K$ is very large relative to our sample \n$n$ this wont make much of an impact, but it can be impactful for smaller\npopulations, or relatively larger samples. \n\nFrom our example above, failing to use a hypergeometric distribution to model \nthis process for smaller populations will result in wider, more conservative\nacceptance regions which can increase consumer risk in a manufacturing process. \n\nTypical guidance on when to use each distribution is given in manufacturing \nstandards such as *AS 1199.1-2003: Sampling Procedures for Inspection by Attributes* \nand typically involves how you structure your sampling scheme. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}